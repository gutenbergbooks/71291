<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>Physical significance of entropy | Project Gutenberg</title>

<link href="images/cover.jpg" rel="icon" type="image/x-cover">

<style>

body {
    font-family: "Times New Roman", Times, serif;
    margin-left: 10%;
    margin-right: 10%;
}

h1 {
    text-align: center;
    clear: both;
}
h2 {
    text-align: center;
    font-weight: bold;
    margin-top: 1em;
    margin-bottom: 1em;
    }

h3,h4,h5,h6 {
    text-align: center;
    clear: both;
}

p {
    margin-top: .51em;
    text-align: justify;
    margin-bottom: .49em;
    text-indent:4%;
}

.nind {text-indent:0%;}

.hanging {padding-left: 2em;
}

.hanging2 {padding-left: 2em;
         text-indent: -2em;
          }

hr {
    width: 33%;
    margin-top: 2em;
    margin-bottom: 2em;
    margin-left: auto;
    margin-right: auto;
    clear: both;
}

hr.r5  {width: 5%; margin-top: 1em; margin-bottom: 1em;}

table { margin-left: auto; margin-right: auto;}

.tdl {text-align: left;}
.tdr {text-align: right;}
.tdc {text-align: center;}
.tdc_top    {text-align: center; vertical-align: top;}
.tdl_ws1    {text-align: left; vertical-align: top; padding-left: 1em;}

.fontsize_80 { font-size:  80%; }
.fontsize_90 { font-size:  90%; }
.fontsize_100 { font-size:  100%; }
.fontsize_120 { font-size:  120%; }
.smcap    {font-variant: small-caps;}
.center   {text-align: center; text-indent: 0em;}

.space-above1 { margin-top: 1em; }
.space-above2 { margin-top: 2em; }
.space-below1 { margin-bottom: 1em; }
.space-below2 { margin-bottom: 2em; }

td.hanging2 {
  vertical-align: middle;
  text-align: justify;
  text-indent: -1em;
  padding-left: 1em;
  padding-right: .25em;
  padding-bottom: .25em;
  padding-top: .25em;
}

div.chapter {
    page-break-before: always;
    margin-top: 4em
    }

.align-center {
display: block;
text-align: center;
margin-top: 1em;
margin-bottom: 1em;
}


.footnote         {margin-left: 10%; margin-right: 10%; font-size: 0.9em;}

.footnote .label  {position: absolute; right: 84%; text-align: right;}

.fnanchor {
    vertical-align: super;
    font-size: .8em;
    text-decoration:
    none;
}

.figcenter   {
    margin: 3% auto 3% auto;
    clear: both;
    text-align: center;
    text-indent: 0%
}

.pagenum {
    position: absolute;
    left: 92%;
    font-size: small;
    text-align: right;
    font-style: normal;
    font-weight: normal;
    font-variant: normal;
    text-indent: 0;
}

    </style>
</head>

<body>
<p style='text-align:center; font-size:1.2em; font-weight:bold'>The Project Gutenberg eBook of Physical significance of entropy or of the second law, by John Frederick Klein</p>
<div style='display:block; margin:1em 0'>
This eBook is for the use of anyone anywhere in the United States and
most other parts of the world at no cost and with almost no restrictions
whatsoever. You may copy it, give it away or re-use it under the terms
of the Project Gutenberg License included with this eBook or online
at <a href="https://www.gutenberg.org">www.gutenberg.org</a>. If you
are not located in the United States, you will have to check the laws of the
country where you are located before using this eBook.
</div>

<p style='display:block; margin-top:1em; margin-bottom:1em; margin-left:2em; text-indent:-2em'>Title: Physical significance of entropy or of the second law</p>
<p style='display:block; margin-top:1em; margin-bottom:0; margin-left:2em; text-indent:-2em'>Author: John Frederick Klein</p>
<p style='display:block; text-indent:0; margin:1em 0'>Release Date: July 28, 2023 [eBook #71291]</p>
<p style='display:block; text-indent:0; margin:1em 0'>Language: English</p>
  <p style='display:block; margin-top:1em; margin-bottom:0; margin-left:2em; text-indent:-2em; text-align:left'>Credits: MWS, Laura and the Online Distributed Proofreading Team at https://www.pgdp.net (This file was produced from images generously made available by The Internet Archive/Canadian Libraries)</p>
<div style='margin-top:2em; margin-bottom:4em'>*** START OF THE PROJECT GUTENBERG EBOOK PHYSICAL SIGNIFICANCE OF ENTROPY OR OF THE SECOND LAW ***</div>

<div class="figcenter" style="width: 500px;">
<img src="images/cover.jpg" width="500" alt="cover">
</div>

<div class="chapter">
<h1>PHYSICAL SIGNIFICANCE<br>
OF ENTROPY<br>
<small>
OR OF THE SECOND LAW</small></h1>

<p class="center space-above2 space-below2 fontsize_80">BY</p>

<div style='text-align:center; font-size:1.2em;'>J. F. KLEIN</div>

<p class="center space-above2 space-below1 smcap">
Professor of Mechanical Engineering,<br>
Lehigh University</p>


<p class="center space-above2 space-below2 fontsize_80">NEW YORK</p>

<p class="center space-above2 space-below2 fontsize_100">D. VAN NOSTRAND COMPANY<br>
23 MURRAY AND 27 WARREN STREETS<br>
1910</p>
</div>

<div class="chapter">
<p class="center space-above2 space-below2">COPYRIGHT, 1910,<br>
BY<br>
JOSEPH FREDERICK KLEIN</p>

<p class="center space-above2 space-below2">THE SCIENTIFIC PRESS<br>
ROBERT DRUMMOND AND COMPANY<br>
BROOKLYN, N. Y.</p>
</div>

<div class="chapter">
<h2>PREFACE</h2>

<p>
In this little book the author has in the main sought to present the
interpretation reached by BOLTZMANN and by PLANCK. The writer has drawn
most heavily upon PLANCK, for he is at once the clearest expositor of
BOLTZMANN and an original and important contributor. Now these two
investigators reach the result that entropy of any physical state is the
logarithm of the probability of the state, and this probability is
identical with the number of "complexions" of the state. This number is
the measure of the permutability of certain elements of the state and in
this sense entropy is the "measure of the disorder of the motions of a
system of mass points." To realize more fully the ultimate nature of
entropy, the writer has, in the light of these definitions, interpreted
some well-known and much-discussed thermodynamic occurrences and
statements. A brief outline of the general procedure followed will be
found on p. 3, while a fuller synopsis is of course given in the
accompanying table of contents.
</p>
<p style="text-align:right">
J. F. Klein.
</p>
<p>
Lehigh University, October, 1910.
</p>
<span class="pagenum" id="Page_iii">[Pg iii]</span>

</div>

<div class="chapter">
<h2>TABLE OF CONTENTS</h2>

<hr class="r5">

<table class="fontsize_100">
<tbody>
<tr>
<td class="tdl">&nbsp;</td>
<td class="tdc">&nbsp;</td>
<td class="tdr fontsize_80">PAGE</td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdc space-above2 space-below2">INTRODUCTION</td>
<td class="tdr">&nbsp;</td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="hanging2"><span class="smcap">Purpose, acknowledgments, the two methods of approach and outline of
treatment</span></td>
<td class="tdr"><a href="#Page_1">&nbsp;1</a></td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdc_top space-above2 space-below2 fontsize_120">PART I</td>
<td class="tdr">&nbsp;</td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="hanging2 fontsize_80">THE DEFINITIONS, GENERAL PRELIMINARIES, DEVELOPMENT, CURRENT
AND PRECISE STATEMENTS OF THE MATTERS CONSIDERED</td>
<td class="tdr">&nbsp;</td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdc_top space-above2 space-below2">SECTION A</td>
<td class="tdr">&nbsp;</td>
</tr>
<tr>
<td class="tdc">&nbsp;</td>
<td class="tdc space-above1 space-below1">(1) <i>The "state" of a body and its "change of state"</i></td>
<td class="tdr"><a href="#Page_5">&nbsp;5</a></td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="hanging2"><span class="smcap">The two points of view; the microscopic and the macroscopic observer;
the micro-state and macro-state or aggregate</span></td>
<td class="tdr"><a href="#Page_5">&nbsp;5</a></td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="hanging2"><span class="smcap">The selected and the rejected micro-states; the use of the hypothesis of
"elementary chaos"</span></td>
<td class="tdr"><a href="#Page_7">&nbsp;7</a></td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="hanging2"><span class="smcap">PLANCK'S fuller description of what constitutes the state of a physical
system</span></td>
<td class="tdr"><a href="#Page_10">10</a></td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdc hanging2 space-above2 space-below1">(2) <i>Further elucidation of the essential prerequisite, "elementary chaos."
Sundry aspects of haphazard</i></td>
<td class="tdr"><a href="#Page_11">11</a></td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="hanging2"><span class="smcap">BOLTZMANN'S service to science in this field and his view of what constitute
the necessary features of haphazard</span></td>
<td class="tdr"><a href="#Page_12">12</a></td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="hanging2"><span class="smcap">BURBURY'S simplification of haphazard necessary and his example of
"elementary chaos"</span></td>
<td class="tdr"><a href="#Page_15">15</a></td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="hanging2"><span class="smcap">Haphazard as expressed by a system possessing an extraordinary number
of degrees of freedom</span></td>
<td class="tdr"><a href="#Page_17">17</a></td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdc hanging2 space-above2 space-below1">(3) <i>Settled and unsettled states; distinction between final stage of "elementary
chaos" and its preceding stages</i></td>
<td class="tdr"><a href="#Page_18">18</a></td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="hanging2"><span class="smcap">Each stage has sufficient haphazard; examples and characteristics of the
settled and unsettled stages of "elementary chaos"; all micro-states
not equally likely; the assumed state of "chaos" does not
eliminate adequate haphazard; two anticipatory remarks</span></td>
<td class="tdr"><a href="#Page_19">19</a></td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdc_top space-above2 space-below2 fontsize_80">SECTION B</td>
<td class="tdr">&nbsp;</td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdc space-above2 space-below1 fontsize_80">CONCERNING THE APPLICATION OF THE CALCULUS OF PROBABILITIES</td>
<td class="tdr">&nbsp;</td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdc hanging2 space-above2 space-below1">(1) <i>The probability concept, its usefulness in the past, its present
necessity, and its universality</i></td>
<td class="tdr"><a href="#Page_22">22</a></td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="hanging2"><span class="smcap">Popular objection to its use; Boltzmann's justification of this concept;
its usefulness in the past and in other fields; some of its good points;
the haphazard features necessary for its use</span></td>
<td class="tdr"><a href="#Page_23">23</a></td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdc hanging2 space-above2 space-below2">(2) <i>What is meant by probability of a state? Example</i></td>
<td class="tdr"><a href="#Page_27">27</a></td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdc_top space-above2 space-below2 fontsize_80">SECTION C</td>
<td class="tdr">&nbsp;</td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdc hanging2 space-above2 space-below1">(1) <i>The existence, definition, measure, properties, relations and scope
of irreversibility and reversibility</i></td>
<td class="tdr"><a href="#Page_29">29</a></td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="hanging2"><span class="smcap">Inference from experience; inference from the H-theorem or calculus of
probabilities; definitions of irreversible and reversible processes;
examples of each</span></td>
<td class="tdr"><a href="#Page_30">30</a></td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdc space-above2 space-below1">(2) <i>Character of process decided by limiting states</i></td>
<td class="tdr"><a href="#Page_32">32</a></td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdl_ws1"><span class="smcap">Nature's preference for a state; measure of this preference</span></td>
<td class="tdr"><a href="#Page_33">33</a></td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdl_ws1"><span class="smcap">Entropy both the criterion and the measure of irreversibility</span></td>
<td class="tdr"><a href="#Page_33">33</a></td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdc space-above2 space-below1">(3) <i>All the irreversible processes stand or fall together</i></td>
<td class="tdr"><a href="#Page_34">34</a></td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdc space-above2 space-below1">(4) <i>Convenience of the fiction, the reversible processes</i></td>
<td class="tdr"><a href="#Page_35">35</a></td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="hanging2"><span class="smcap">Entropy the only universal measure of irreversibility. Outcome of the
whole study of irreversibility</span></td>
<td class="tdr"><a href="#Page_36">36</a></td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdc_top space-above2 space-below2 fontsize_80">SECTION D</td>
<td class="tdr">&nbsp;</td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdc hanging2 space-above2 space-below1">(1) <i>The gradual development of the idea that entropy depends on probability
or number of complexions</i></td>
<td class="tdr"><a href="#Page_37">37</a></td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="hanging2"><span class="smcap">Why it is difficult to conceive of entropy. Origin and first definition
due to CLAUSIUS; some formulas for it available from the start. Its
statistical character early appreciated; lack of precise physical meaning;
its dependence on probability; number of complexions a synonym
for probability</span></td>
<td class="tdr"><a href="#Page_37">37</a></td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdc hanging2 space-above2 space-below1">(2) PLANCK'S <i>formula for the relation between entropy and the number
of complexions</i></td>
<td class="tdr"><a href="#Page_40">40</a></td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdl_ws1"><span class="smcap">Certain features of entropy</span></td>
<td class="tdr"><a href="#Page_41">41</a></td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdc_top space-above2 space-below2 fontsize_80">SECTION E</td>
<td class="tdr">&nbsp;</td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdc hanging2 space-above2 space-below1"><i>Equivalents of change of entropy in more or less general physical terms
or aspects</i></td>
<td class="tdr"><a href="#Page_41">41</a></td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="hanging2"><span class="smcap">Not surprising that its many forms should have been a reproach to the
second law</span></td>
<td class="tdr"><a href="#Page_41">41</a></td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="hanging2"><span class="smcap">General principles for comparing these aspects. Various aspects of
growth of entropy from the experiential and from the atomic point
of view</span></td>
<td class="tdr"><a href="#Page_42">42</a></td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdc_top space-above2 space-below2 fontsize_80">SECTION F</td>
<td class="tdr">&nbsp;</td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdc space-above2 space-below1"><i>More precise and specific statements of the second law</i></td>
<td class="tdr"><a href="#Page_44">44</a></td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdl_ws1"><span class="smcap">General arrangement and the principles for comparison</span></td>
<td class="tdr"><a href="#Page_44">44</a></td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdl_ws1"><span class="smcap">Ten different statements of the law and comments thereon</span></td>
<td class="tdr"><a href="#Page_44">44</a></td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdc_top space-above2 space-below2 fontsize_120">PART II</td>
<td class="tdr">&nbsp;</td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdc space-above2 space-below1 fontsize_80">ANALYTICAL EXPRESSIONS FOR A FEW PRIMARY RELATIONS</td>
<td class="tdr">&nbsp;</td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdl_ws1"><span class="smcap">Procedure followed</span></td>
<td class="tdr"><a href="#Page_48">48</a></td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdc_top space-above2 space-below2 fontsize_80">SECTION A</td>
<td class="tdr">&nbsp;</td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdc space-above2 space-below1"><i>Maxwell's law of distribution of molecular velocities</i></td>
<td class="tdr"><a href="#Page_48">48</a></td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdl_ws1"><span class="smcap">Outline of proof, illustration, and consequences of this law</span></td>
<td class="tdr"><a href="#Page_48">48</a></td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdc_top space-above2 space-below2 fontsize_80">SECTION B</td>
<td class="tdr">&nbsp;</td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdc space-above2 space-below1"><i>Simple analytical expression for dependence of entropy on probability</i></td>
<td class="tdr"><a href="#Page_53">53</a></td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="hanging2"><span class="smcap">PLANCK'S derivation; illustration, limitations, consequences, features and
comments</span></td>
<td class="tdr"><a href="#Page_53">53</a></td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdc_top space-above2 space-below2 fontsize_80">SECTION C</td>
<td class="tdr">&nbsp;</td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdc hanging2 space-above2 space-below1"><i>Determination of a precise, numerical expression for the entropy of
any physical configuration</i></td>
<td class="tdr"><a href="#Page_56">56</a></td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdl_ws1"><span class="smcap">BOLTZMANN'S pioneer work, PLANCK'S exposition, and the six main steps</span></td>
<td class="tdr"><a href="#Page_56">56</a></td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdc space-above2 space-below1"><i>Step a</i></td>
<td class="tdr">&nbsp;</td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="hanging2"><span class="smcap">Determination of the general expression for the
<img style="vertical-align: -2.148ex; width: 24.977ex; height: 5.428ex;" src="images/1.svg" alt=" " data-tex="\left\{\begin{aligned}\text{probability or number}\\ \text{of complexions}\end{aligned}\right\}">
of a given configuration of a known aggregate state</span></td>
<td class="tdr"><a href="#Page_57">57</a></td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdc space-above2 space-below1"><i>Step b</i></td>
<td class="tdr">&nbsp;</td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="hanging2"><span class="smcap">Determination of the general expression for the entropy <img style="vertical-align: -0.05ex; width: 1.459ex; height: 1.645ex;" src="images/2.svg" alt=" " data-tex="S"> of a given configuration
of a known aggregate state</span></td>
<td class="tdr"><a href="#Page_63">63</a></td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdc space-above2 space-below1"><i>Step c</i></td>
<td class="tdr">&nbsp;</td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="hanging2"><span class="smcap">Special case of (<i>b</i>), namely, expression for the entropy <img style="vertical-align: -0.05ex; width: 1.459ex; height: 1.645ex;" src="images/2.svg" alt=" " data-tex="S"> of the state of
thermal equilibrium of a monatomic gas</span></td>
<td class="tdr"><a href="#Page_63">63</a></td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdc space-above2 space-below1"><i>Step d</i></td>
<td class="tdr">&nbsp;</td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="hanging2"><span class="smcap">Confirmation, by equating this value of <img style="vertical-align: -0.05ex; width: 1.459ex; height: 1.645ex;" src="images/2.svg" alt=" " data-tex="S"> with that found thermodynamically
and then deriving known results</span></td>
<td class="tdr"><a href="#Page_64">64</a></td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdc space-above2 space-below1"><i>Step e</i></td>
<td class="tdr">&nbsp;</td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="hanging2"><span class="smcap">PLANCK'S conversion of the expressions of (<i>b</i>) and (<i>c</i>) into more precise
ones by finding numerical value of <img style="vertical-align: -0.025ex; width: 1.179ex; height: 1.595ex;" src="images/3.svg" alt=" " data-tex="k"></span></td>
<td class="tdr"><a href="#Page_66">66</a></td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdc space-above2 space-below1"><i>Step f</i></td>
<td class="tdr">&nbsp;</td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="hanging2"><span class="smcap">Determination of the dimensions of the universal constant <img style="vertical-align: -0.025ex; width: 1.179ex; height: 1.595ex;" src="images/3.svg" alt=" " data-tex="k"> and therefore
also of entropy in general</span></td>
<td class="tdr"><a href="#Page_67">67</a></td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdc_top space-above2 space-below2 fontsize_120">PART III</td>
<td class="tdr">&nbsp;</td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdc space-above2 space-below1 fontsize_80">THE PHYSICAL INTERPRETATIONS</td>
<td class="tdr">&nbsp;</td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdc_top space-above2 space-below2 fontsize_80">SECTION A</td>
<td class="tdr">&nbsp;</td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdc space-above2 space-below1"><i>Of the simple reversible operations in thermodynamics</i></td>
<td class="tdr">&nbsp;</td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdl_ws1"><span class="smcap">Isometric, isobaric, isothermal, and isentropic change</span></td>
<td class="tdr"><a href="#Page_69">69</a></td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdc_top space-above2 space-below2 fontsize_80">SECTION B</td>
<td class="tdr">&nbsp;</td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdc space-above2 space-below1"><i>Of the fundamentally irreversible processes</i></td>
<td class="tdr">&nbsp;</td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="hanging2"><span class="smcap">Heat conduction, work into heat of friction, expansion without work, and
diffusion of gases</span></td>
<td class="tdr"><a href="#Page_72">72</a></td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdc_top space-above2 space-below2 fontsize_80">SECTION C</td>
<td class="tdr">&nbsp;</td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdc space-above2 space-below1"><i>Of negative change of entropy</i></td>
<td class="tdr">&nbsp;</td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdl_ws1"><span class="smcap">Some of its physical features and necessary accompaniments</span></td>
<td class="tdr"><a href="#Page_78">78</a></td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdc_top space-above2 space-below2 fontsize_80">SECTION D</td>
<td class="tdr">&nbsp;</td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="hanging2"><i>Physical significance of the equivalents for growth of entropy given on
pp. 42-43</i></td>
<td class="tdr"><a href="#Page_80">80</a></td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdc_top space-above2 space-below2 fontsize_80">SECTION E</td>
<td class="tdr">&nbsp;</td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="hanging2"><i>Physical significance of the more specific statements of second law given
on pp. 44-47</i></td>
<td class="tdr"><a href="#Page_81">81</a></td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdc_top space-above2 space-below2 fontsize_120">PART IV</td>
<td class="tdr">&nbsp;</td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdc space-above2 space-below2 fontsize_80">SUMMARY OF THE CONNECTION BETWEEN PROBABILITY, IRREVERSIBILITY,
ENTROPY, AND THE SECOND LAW</td>
<td class="tdr">&nbsp;</td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdc_top space-above2 space-below2 fontsize_80">SECTION A</td>
<td class="tdr">&nbsp;</td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="hanging2 space-above2 space-below1">(1) <i>Prerequisites and conditions necessary for the application of the
theory of probabilities</i></td>
<td class="tdr">&nbsp;</td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="hanging2"><span class="smcap">(<i>a</i>) Atomic theory; (<i>b</i>) like particles; (<i>c</i>) very numerous particles; (<i>d</i>)
"elementary chaos"</span></td>
<td class="tdr"><a href="#Page_83">83</a></td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdc space-above2 space-below1">(2) <i>Differences in the states of "elementary chaos"</i></td>
<td class="tdr"><a href="#Page_85">85</a></td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdc space-above2 space-below1">(3) <i>Number of complexions, or probability of a chaotic state</i></td>
<td class="tdr"><a href="#Page_86">86</a></td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdc_top space-above2 space-below2 fontsize_80">SECTION B</td>
<td class="tdr">&nbsp;</td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdc"><i>Irreversibility</i></td>
<td class="tdr"><a href="#Page_86">86</a></td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdc_top space-above2 space-below2 fontsize_80">SECTION C</td>
<td class="tdr">&nbsp;</td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdc"><i>Entropy</i></td>
<td class="tdr"><a href="#Page_87">87</a></td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdc_top space-above2 space-below2 fontsize_80">SECTION D</td>
<td class="tdr">&nbsp;</td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdc space-above2 space-below1"><i>The Second Law</i></td>
<td class="tdr">&nbsp;</td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdl_ws1"><span class="smcap">Its basis and best statements; it has no <i>independent</i> significance</span></td>
<td class="tdr"><a href="#Page_88">88</a></td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdc_top space-above2 space-below2 fontsize_120">PART V</td>
<td class="tdr">&nbsp;</td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdc space-above2 space-below1 fontsize_80">REACH OR SCOPE OF THE SECOND LAW</td>
<td class="tdr">&nbsp;</td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdc_top space-above2 space-below2 fontsize_80">SECTION A</td>
<td class="tdr">&nbsp;</td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdc space-above2 space-below1"><i>Its extension to all bodies</i></td>
<td class="tdr">&nbsp;</td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdl_ws1"><span class="smcap">PLANCK'S presentation; fifteen steps in the proof</span></td>
<td class="tdr"><a href="#Page_91">91</a></td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdc_top space-above2 space-below2 fontsize_80">SECTION B</td>
<td class="tdr">&nbsp;</td>
</tr>
<tr>
<td class="tdr">&nbsp;</td>
<td class="tdc"><i>General conclusion as to entropy changes</i></td>
<td class="tdr"><a href="#Page_98">98</a></td>
</tr>
</tbody>
</table>

</div>

<div class="chapter">
<h2>THE PHYSICAL SIGNIFICANCE OF ENTROPY<br>
AND OF THE SECOND LAW</h2>

<p class="center">
[There is no difference between change of Entropy and Second Law, when each
is fully defined.]
</p>

<hr class="r5">

<h2><a id="INTRODUCTION">INTRODUCTION</a>
<br><br>
PURPOSE, ACKNOWLEDGMENTS, THE TWO METHODS OF APPROACH<br>
AND OUTLINE OF TREATMENT</h2>

<p>
THIS article is intended for those students of engineering who
already have some elementary knowledge of thermodynamics.
It is intended to clear up a difficulty that has beset every earnest
beginner of this subject. The difficulty is not one of application
to engineering problems, although here too there have been
widespread misconceptions,<a id="FNanchor_1_1"></a><a href="#Footnote_1_1" class="fnanchor">[1]</a> for the expressions developed by
CLAUSIUS are simple, have long been known and much used by
engineers and physicists. The difficulty is rather as to the
ultimate physical meaning of entropy. This term has long been
known as a sort of property of the state of the body, has long been
surmised to be of essentially a statistical nature, but with it all
there was a sense that it was a sort of mathematical fiction,
that it was somehow unreal and elusive, so it is no wonder
that in certain engineering quarters it was dubbed the "ghostly
quantity."
</p>
<p>
Now this instinct of the true engineer to understand things
<span class="pagenum" id="Page_1">[Pg 1]</span>
down to the bottom is worthy of all encouragement and respect.
For this reason and because the matter is of prime importance
to the technical world, the final meaning of entropy (i.e., of the
Second Law) must be clarified and realized. Indeed, we may well
go beyond this somewhat narrow view and say that this is well
worth doing because <i>change of entropy constitutes the driving motive
in all natural events</i>; it has therefore a reach and a universality
which even transcends that of the First Law, or Principle of the
Conservation of Energy.
</p>
<p>
In striving to present the physical meaning of entropy and of the Second
Law, the writer cannot lay claim to any originality; he has simply tried
here to put in logical order the somewhat scattered propositions of the
leading investigators of this subject and in such a way that the
difficulties of apprehension might be minimized; in other words, to
present the solutions of his own difficulties, in the hope that the
solutions may be helpful to other students of engineering and
thermodynamics. In overcoming these difficulties, the writer owes
everything to the books and papers by PLANCK and BOLTZMANN,
pre-eminently to PLANCK, who has so clearly and appreciatively
interpreted the life work of BOLTZMANN.<a id="FNanchor_2_1"></a><a href="#Footnote_2_1" class="fnanchor">[2]</a>
The writer furthermore wishes to say that he has not hesitated here to
quote <i>verbatim</i> from both these investigators and not always so
that their own statements can be distinguished from his own. If any part
of this presentation is particularly clear and exact the reader will be
safe in crediting it to one or the other of these two investigators and
expositors, although it would not be right to consider them responsible
for everything contained in this little book.
</p>
<p>
In considering the proper approach to the matter in hand we
must remember that<a id="FNanchor_3_1"></a><a href="#Footnote_3_1" class="fnanchor">[3]</a> "in physical science there are two more or
<span class="pagenum" id="Page_2">[Pg 2]</span>
less distinct modes of attack, namely, (<i>a</i>) a mode of attack in
which the effort is made to develop conceptions of the physical
processes of nature, and (<i>b</i>) a mode of attack in which the
attempt is made to correlate phenomena on the basis of sensible things,
things that can be seen and measured. In the theory of heat the first
mode is represented by the application of the atomic theory to the study
of heat phenomena, and the second mode is represented by what is called
thermodynamics." In solving the special problem before us, as to the
physical meaning of entropy and of the Second Law, our main dependence
must be on the first mode of attack.
</p>
<p>
The second mode will furnish checks and confirmations of the results
developed by the first, or we may say that the combination of the two
modes will give the well-established characteristic equations and
relations of bodies and their physical elements.
</p>
<p>
The whole discussion will now be taken up in a non-mathematical way,
without the full proof required by a complete presentation, and about in
this order:
</p>
<p>
(<i>a</i>) The definitions, general preliminaries and current statements
of the matters considered.
</p>
<p>
(<i>b</i>) More or less precise statement of the primary relations
and theorems.
</p>
<p>
(<i>c</i>) The physical interpretations.
</p>
<p>
(<i>d</i>) Summary of the connection between probability, irreversibility,
entropy and the Second Law.
</p>
<p>
(<i>e</i>) Reach or scope of the Second Law.
</p>
<p>
On account of the difficulty which every student experience in
realizing the physical nature of entropy we will in the main confine
our attention here to gases and indeed to their simplest case,
the monatomic gas, and will as usual assume that the dimensions
of an atom or particle are very small in comparison with the
average distance between two adjacent particles, that for the atoms
approaching collision the distance within which they exert a significant
influence on each other is very small as compared with the
<span class="pagenum" id="Page_3">[Pg 3]</span>
mean distance between adjacent atoms, and that between collisions
the mean length of the particle's path is great in comparison with
the average distance between the particles. Later on we will indicate
in a very general and brief way how the entropy idea may
be extended to other states of aggregation and to other than
purely thermodynamic phenomena. Mostly, therefore, we will
only consider states and processes in which heat phenomena and
mechanical occurrences take place.
</p>

<div class="footnote">

<p class="nind">
<a id="Footnote_1_1"></a><a href="#FNanchor_1_1"><span class="label">[1]</span></a>See Entropy, by JAMES SWINBURNE; this author has called attention to
necessary corrections and duly emphasized the engineering aspect.</p></div>

<div class="footnote">

<p class="nind">
<a id="Footnote_2_1"></a><a href="#FNanchor_2_1"><span class="label">[2]</span></a>BOLTZMANN, Gas Theorie; PLANCK, Thermodynamik, Theorie der Wärmestrahlung,
and Acht Vorlesungen über Theoretische Physik.</p></div>

<div class="footnote">

<p class="nind">
<a id="Footnote_3_1"></a><a href="#FNanchor_3_1"><span class="label">[3]</span></a>Professor W. S. FRANKLIN, The Second Law of Thermodynamics: its basis
in Intuition and Common Sense. <i>Pop. Science Monthly</i>, March, 1910.</p></div>

<p><span class="pagenum" id="Page_4">[Pg 4]</span></p>

</div>

<div class="chapter">
<h2><a id="PART_I">PART I</a>
<br><br>
DEFINITIONS, GENERAL PRELIMINARIES, DEVELOPMENT, CURRENT
AND PRECISE STATEMENTS OF THE MATTERS CONSIDERED</h2>

<h2><a id="SECTION_A">SECTION A</a></h2>

<p class="center">
(1) <i>The "State" of a Body and its "Change of State"</i>
</p>

<p>
As we will make constant use of the terms contained in this
heading and as they here represent fundamentally important
conceptions, we will seek to make them clear by presenting them
in the various forms into which they have been cast by the different
investigators, even at the risk of being considered prolix.
</p>
<p>
In the Introduction to this article we called attention to the two
distinct modes of attacking any physical problem. Now the conception
"state of a body" varies with the chosen mode of attack. Of course as
both modes are legitimate and lead to correct results, these differences
in the conception of "state" can be reconciled and a broader definition
reached. We can illustrate these different methods of approach, as
PLANCK has done, by assuming two different observers of the state of the
body, one called the microscopic-observer and the other the
macroscopic-observer. The former possesses senses so acute and powers so
great that he can recognize each individual atom and can measure its
motion. For this observer each atom will move exactly according to the
elementary laws prescribed for it by General Dynamics. These laws, so
far as we know them, also at once permit of exactly the opposite course
of each event. Consequently there can be here no question of
probability, of entropy or of its growth. On the other hand, the
"macro-observer," (who perceives the atomic host, say as a homogeneous
gas, and consequently applies to its mechanical and thermal
<span class="pagenum" id="Page_5">[Pg 5]</span>
events the laws of thermodynamics) will regard the process as a whole
to be an irreversible one in accordance with the Second Law.... Now a
particular change of state cannot at the same time be both reversible
and irreversible. But the one observer has a different idea of "change
of state" from the other; the micro-observer's conception of "change of
state" is different from that of the macro-observer. What then is
"change of state?" The state of a physical system can probably not be
rigorously defined, otherwise than the conception, as a whole, of all
those physical magnitudes whose instantaneous values, under given
external conditions, also uniquely determine the sequence of these
changing values.
</p>
<p>
BOLTZMANN'S statement is much more clear, namely, "The state of a body
is determined, (<i>a</i>) by the law of distribution of the particles in
space and (<i>b</i>) by the law of distribution of the velocities of the
particles; in other words, a body's condition is determined (<i>a</i>)
by the number of particles which lie in each elementary realm of the
space and (<i>b</i>) by a statement of the number of particles which
belong to each elementary velocity group. These elementary realms are
all equal and so are the elementary velocity groups equal among
themselves. But it is furthermore assumed that each elementary realm and
each elementary velocity group contains very many particles."
</p>
<p>
Now if we ask the aforesaid two observers what they understand
by the state of the atomic host or gas under consideration,
they will give entirely different answers. The micro-observer
will mention those magnitudes which determine the location and
the velocity condition of all the individual atoms. This would
mean in the simplest case, in which the atoms are regarded as
material points, that there would be six times as many magnitudes
as atoms present, namely, for each atom there would be three
co-ordinates of location and three of velocity components; in
the case of composite molecules there would be many more such
<span class="pagenum" id="Page_6">[Pg 6]</span>
magnitudes. For the micro-observer, the state and the sequence
of the event would not be determined until all these many magnitudes
had been separately given. The state thus defined we
will call the "micro-state." The macroscopic-observer on the
other hand gets along with much fewer data; he will say that the
state of the contemplated homogeneous gas is already determined
by the density, the visible velocity and the temperature at each
place of the gas and he will expect, when these magnitudes are
given, that the course of the physical events will be completely
determined, namely, will occur in obedience to the two laws of
thermodynamics and therefore be bound to show an increase in
entropy. The state thus defined we will call the "macro-state."
The difference in the two observers is that one sees only the atomic
events and the other the occurrences in the aggregate. The former
would have the absolute mechanical idea of state and the latter
the statistical idea. Before attempting to reconcile their apparently
conflicting conclusions, we will here call attention to some
necessary relations between the micro-state and the macro-state.
In the first place we must remember that <i>all a priori</i> possible
micro-states are not realized in nature; they are conceivable
but never attain fruition. How shall we select what may be
called these <i>natural</i> micro-states? The principles of general
dynamics furnish no guide for such selection and so recourse
may be had to any dynamic hypothesis whose selection will be
fully justified by experience.
</p>
<p>
Now PLANCK says: "In order to traverse this path of investigation,
we must evidently first of all keep in mind all the conceivable
positions and velocities of the individual atoms, which
are compatible with particular values of the density, the velocity
and the temperature of the gas, or, in other words, we must consider
all the micro-states which belong to a particular macro-state
and must examine all the different events which follow
from the different micro-states according to the fixed laws of
dynamics. Now up to this time, the closer calculation and combination
<span class="pagenum" id="Page_7">[Pg 7]</span>
of these minute elements has always given the important
result that the vast majority of these micro-states belong to one
and the same macro-state or aggregate, and that only comparatively
few of the said micro-states furnish an anomalous result,
and these few are characterized by very special and far-reaching
conditions existing between the locations and the velocities of
adjacent atoms. And, furthermore, it has appeared that the
almost invariably resulting macro-event is just the very one
perceived by the macroscopic observer, the one in which all
the measurable mean values have a unique sequence, and consequently
and in particular satisfies the second law of thermodynamics."
</p>
<p>
"Herewith is revealed the bridge of reconciliation between
the two observers. The micro-observer needs only to take up
in his theory the physical hypothesis, that all such particular
cases (which premise very special, far-reaching conditions between
the states of adjacent and interacting atoms) do not occur in
Nature; or in other words, the micro-states are in 'elementary
disorder' (<i>elementar ungeordnet</i>). This secures the unique
(unambiguous) character of the macroscopic event and makes
sure that the Principle of the Growth of Entropy will be satisfied
in every direction."
</p>
<p>
Before elaborating all that is implied in this hypothesis of
"elementary disorder" we will again point out that for each
macro-state (even with settled values of density and temperature)
there may be many micro-states which satisfy it in the aggregate.
</p>
<p>
According to PLANCK, "it is easy to see that the macro-observer
deals with mean values; for what he calls density, visible velocity,
temperature of the gas, are for the micro-observer certain <i>averages</i>,
statistical data, which have been suitably obtained from the
spatial arrangement and the velocities of the atoms. But with
these averages the micro-observer at first can do nothing even
if they are known for a certain time, for thereby the sequence
of events is by no means settled; on the contrary, he can easily
<span class="pagenum" id="Page_8">[Pg 8]</span>
with said given averages ascertain a whole host of different values
for the location and velocities of the individual atoms, all of which
correspond to said given averages, and yet some of these lead to
wholly different sequences of events even in their mean values,"
events which do not at all accord with experience. It is evident,
if any progress is to be made, that the micro-observer must in
some suitable way limit the manifold character of the multifarious
micro-states. This he accomplishes by the hypothesis of "elementary
disorder" about to be more fully defined.
</p>
<p>
In passing we may here note for future use, that what has just been said
concerning macro-states (aggregates) with "settled" mean velocity,
density and temperature, applies also to states unsettled in the
aggregate, so far as concerns the manifold character of the conceivable
constituent micro-states and the differences in the mean character of
their sequences. Even after the above limiting hypothesis removes all
illegitimate micro-states, an enormously greater number of legitimate
ones will be left to constitute the number of complexions properly
belonging to the state contemplated. We may also add that it seems quite
evident that the numbers representing these complexions will be
different in the settled and unsettled states even if the latter should
ultimately possess the mean velocity, density and temperature of the
former.
</p>
<p>
On the other hand, we also point out that for one and the same
set of external conditions the macro-state may itself vary very
greatly. When it has a settled density and temperature, it is
said to be in a stationary state, to be in thermal equilibrium and,
anticipating, we may add that it is then has maximum entropy,
in short we may say it is in a "normal" condition. But the
external conditions remaining the same, before attaining to
said "normal" ultimate state, it may pass through a whole
series of so-called "abnormal" states after it leaves its initial
condition. While it is in any one of these "abnormal" states,
it may be said to be in a more or less turbulent condition;
<span class="pagenum" id="Page_9">[Pg 9]</span>
it may then possess whirls and eddies; it may have different
densities and temperatures in its different parts and then it will
be difficult or impossible to measure these external physical
features of its state as a whole. All this implies ever-varying
atomic locations and velocities, but does not indicate any such
special far-reaching regularities between adjacent and interacting
particles as would vitiate at any stage our hypothesis of
"elementary disorder" (<i>elementar ungeordnet</i>) or "molecular
chaos."
</p>
<p>
Before going into more detail concerning this particular chaotic
condition of the particles we will give PLANCK'S somewhat fuller
statement of what constitutes the "state" of a physical system at a
particular time and under given external conditions. It is, "the
conception as a whole of all those mutually independent magnitudes which
determine the sequence of events occurring in the system so far as they
are accessible to measurement; the knowledge of the state is therefore
equivalent to a knowledge of the <i>initial conditions</i>. For example,
in a gas composed of invariable molecules the state is determined by the
law of their space and velocity distribution, i.e., by the statement of
the number of molecules, of their co-ordinates and velocity components
which lie within each single small region. The number of molecules in
any one of these different regions is in general entirely independent of
the number in any other region, for the state need not be a stationary
one nor one of equilibrium; these numbers should therefore all be
separately known if the state of the gas is to be considered as given in
the absolute mechanical sense. On the other hand, for the
characterization of the state in the statistical sense, it is not
necessary to go into closer detail concerning the molecules present in
each elementary space; for here the necessary supplement is supplied by
the hypothesis of molecular chaos, "which in spite of its mechanically
indeterminate character guarantees the unambiguous sequence of the
physical events."
<span class="pagenum" id="Page_10">[Pg 10]</span>
</p>

<p><br><br></p>

<p class="center">
(2) <i>Further Elucidation of this Essential Condition of "Elementary
Chaos." Sundry Aspects of Haphazard</i></p>

<p>
To gain as complete an understanding as possible of this fundamental
idea we will now give the views of the several investigators as to the
physical features of this chaotic state. We have seen how PLANCK, the
chief expositor of BOLTZMANN, boldly excludes from consideration all
cases leading to anomalous results, because of the very special
conditions existing between the molecular data, by assuming that these
cases do not occur in Nature. PLANCK reminds the physicists who object
to the hypothesis of elementary disorder because they feel it is
unnecessary or even unjustifiable, that the hypothesis is already much
used in Physics, that tacitly or otherwise it underlies every
computation of the constants attached to friction, diffusion and the
conduction of heat. On the other hand he reminds others, those inclined
to regard the hypothesis of "elementary disorder" as axiomatic, of the
theorem of H. POINCARÉ, which excludes this hypothesis for <i>all</i>
times from a space surrounded with absolutely smooth walls. PLANCK says
that the only escape from the portentous sweep of this proposition is
that absolutely smooth walls do not exist in Nature.
</p>
<p>
The foregoing thought PLANCK has also put in a slightly different way.
Appreciating that all mechanically possible simultaneous arrangements
and velocities of molecules <i>are not realized</i> in Nature, the
concept of "elementary disorder" implies one limitation of the
conceivable molecular states, namely that, between the numerous elements
of a physical system there exist <i>no other relations</i> than those
conditioned by the existing measurable mean values of the physical
features of the system in question.
</p>
<p>
Another, briefer but equivalent, definition is that: "In Nature
all states and processes which contain numerous independent
(<i>unkontrollierbar</i>) constituents are in 'elementary disorder'
(<i>elementar ungeordnet</i>)." The constituents are molecular elements
<span class="pagenum" id="Page_11">[Pg 11]</span>
in mechanics and in thermodynamics and the energy elements in radiation.
</p>
<p>
The German word "<i>unkontrollierbar</i>"<a id="FNanchor_4_1"></a><a href="#Footnote_4_1" class="fnanchor">[4]</a> here used may also
with some justice be translated as, unconditioned, undetermined,
unmeasurable, unregulated, uncorrelated, ungovernable or haphazard.
But whichever term is best, PLANCK, mechanically speaking, meant by it,
the confused, unregulated and whirring intermingling of very many atoms.
</p>
<p>
Either of these two equivalent definitions implies that such elementary
disorder or chaos is a condition of sufficiently complete haphazard to
warrant the application of the Theory of Probabilities to the unique
(unambiguous) determination of the measurable physical features of the
process viewed as a whole.
</p>
<p>
The foregoing ideas more or less tacitly underlie the whole of
BOLTZMANN'S great pioneer work in this vast field. He it was who clearly
showed that the Second Law could be derived from mechanical principles:
that entropy was a property of every state, turbulent or otherwise; that
the entropy idea would be emancipated from all thought of human,
experimental, skill, and who thereby raised the Second Law to the
position of a real principle. He did all this by a general basing of the
idea of entropy on the idea of probability. Consequently we find much
attention paid in all his work to haphazard molecular conditions. He
first used the terms "molekular-geordnet" (molecularly ordered, or
arranged), and "molekular ungeordnet" (molecularly disordered or
disarranged), which latter phrase we must regard as synonymous with the
term "<i>elementar ungeordnet</i>" (elementary disorder or chaos) with
which we have already become acquainted in PLANCK'S presentation. We
will, therefore, confine ourselves here to BOLTZMANN'S illustrations of
these terms, for his work does not, in these particulars, contain any
<span class="pagenum" id="Page_12">[Pg 12]</span>
sharp definitions. Indeed he may have feared over-precision and may
have trusted to the use he made of the terms at different times
to convey their meaning.
</p>
<p>
Concerning some of the characteristics of BOLTZMANN'S haphazard
motion we take the following from Vol. I of his "Vorlesungen
über Gas Theorie."
</p>
<p>
If in a finite part of a gas the variables determining the motion of the
molecules have different mean values from those in another finite part
of the gas (for example if the mean density or mean velocity of a gas in
one-half of a vessel is different from those in the other half), or more
generally, if any finite part of a gas behaves differently from another
finite part of a gas, then such a distribution is said to be
"molar-geordnet" (in molar order). But when the total number of
molecules in every unit of volume exists under the same conditions and
possesses the same number of each kind of molecules throughout the
changes contemplated, then the same number of molecules will leave a
unit volume and will enter it so that the total number ever present
remains the same; under such conditions we call the distribution
"molar-ungeordnet" (in molar disorder) and that finite distribution is
one of the characteristics of the haphazard state to which the Theory of
Probabilities is applicable. [As another illustration of the excluded
molar-geordnet states we may instance the case when all motions are
parallel to one plane.]
</p>
<p>
But although in passing from one finite part to another of a gas no
regularities (of average character) can be discerned, yet infinitesimal
parts (say of two or more molecules) may exhibit certain regularities,
and then the distribution would be "molekular-geordnet"
(molecularly-ordered) although as a whole the gas is "molar-ungeordnet."
For example (to take one of the infinite number of possible cases)
suppose that the two nearest molecules always approached each other
along their line of centers, or if a molecule moving with a particularly
slow speed always had ten (10) slow neighbors, then the distribution
<span class="pagenum" id="Page_13">[Pg 13]</span>
would be "molekular-geordnet." But then the locality of one molecule
would have some influence on the locality of another molecule and then
in the Theory of Probabilities the presence of one molecule in one place
would not be independent of the presence of some other molecule in some
other place. Such dependence is not permissible by the Theory of
Probabilities. Before, however, we can further describe what is here
perhaps the most important term (molekular-ungeordnet), we must point
out that BOLTZMANN considers the number of molecules <img style="vertical-align: -0.025ex; width: 1.986ex; height: 1.025ex;" src="images/4.svg" alt=" " data-tex="m"> of one kind
whose component velocities along the co-ordinate axes are confined
between the limits,
<span class="align-center"><img style="vertical-align: -0.566ex; width: 45.706ex; height: 2.262ex;" src="images/5.svg" alt=" " data-tex="
\xi\, \text{and}\, \xi + d\xi,\quad
\eta\, \text{and}\, \eta + d\eta,\quad
\zeta\, \text{and}\, d\zeta,
\qquad\text{(1)}
"></span>
and also the number of molecules <img style="vertical-align: -0.339ex; width: 2.974ex; height: 1.339ex;" src="images/6.svg" alt=" " data-tex="m_{1}"> of another kind whose
component velocities similarly lie between the limit
<span class="align-center"><img style="vertical-align: -0.566ex; width: 47.37ex; height: 2.262ex;" src="images/7.svg" alt=" " data-tex="
\xi_{1} + \xi_{1} + \xi{_1},\quad
\eta_{1} + \eta_{1} + d\eta_{1},\quad
\zeta_{1} + d\zeta_{1},
\qquad\text{(2)}
"></span>
then, considering the chances that a molecule <img style="vertical-align: -0.025ex; width: 1.986ex; height: 1.025ex;" src="images/4.svg" alt=" " data-tex="m"> shall have
velocities between the limits specified in (1) and molecule <img style="vertical-align: -0.339ex; width: 2.974ex; height: 1.339ex;" src="images/6.svg" alt=" " data-tex="m_{1}">
have velocities between limits (2), BOLTZMANN intimates that these
chances are independent of the relative position of the molecules.
Where there is such complete independence, or absence of all
minute regularities, the distribution, according to BOLTZMANN,
is "molekular-ungeordnet" (molecularly-disordered).
</p>
<p>
BOLTZMANN furthermore informs us that, as soon as in a gas, the mean
length of path is great in comparison with the mean distance between two
adjacent molecules, the neighboring molecules will quickly become
different from what they formerly were. Therefore it is exceedingly
probable that a "molekular-geordnete" (but molar-ungeordnete)
distribution would shortly pass into a "molekular-ungeordnete"
distribution.
</p>
<p>
Furthermore, from the constitution of a gas results that the
place where a molecule collided is entirely independent of the
spot where its preceding collision took place. Of course, this
<span class="pagenum" id="Page_14">[Pg 14]</span>
independence could be maintained for an indefinite time only
by an <i>infinite</i> number of molecules.
</p>
<p>
The place of collision of a pair of molecules must in our Theory of
Probabilities be independent of the locality from which either molecule
started.
</p>
<p>
From all the preceding we must infer what measure of haphazard BOLTZMANN
considers necessary for the legitimate use of the Theory of
Probabilities.
</p>
<p>
BOLTZMANN in proving his H-Theorem,<a id="FNanchor_5_1"></a><a href="#Footnote_5_1" class="fnanchor">[5]</a> which establishes the
one-sidedness of all <i>natural</i> events, makes the explicit assumption
that the motion at the start is both "molar- und molekular-ungeordnet"
and remains so. Later on, he assumes the same things but adds that if they
are not so at the start they will soon become so; therefore said assumption
does not preclude the consideration by Probability methods of the general
case or the passage from "ordnete" to "ungeordnete" conditions which
characterizes all natural events.
</p>
<p>
In fact these very definitions show solicitude for securing the
uninterrupted operation of the laws of probability. BOLTZMANN intimates
his approval of S. H. BURBURY'S statement of the condition of
independence underlying his work.
</p>
<p>
Here S. H. BURBURY<a id="FNanchor_6_1"></a><a href="#Footnote_6_1" class="fnanchor">[6]</a> simplifies the matter by assuming that
any unit of volume of space contains a uniform mixture of differently
speeded molecules and then says:
</p>
<p>
"Let <img style="vertical-align: -0.05ex; width: 1.74ex; height: 1.595ex;" src="images/8.svg" alt=" " data-tex="V"> be the velocity of the center of gravity of <i>any</i> pair of
molecules and <img style="vertical-align: -0.048ex; width: 1.717ex; height: 1.593ex;" src="images/9.svg" alt=" " data-tex="R"> their relative velocity. Then the following condition
(here called <img style="vertical-align: 0; width: 1.697ex; height: 1.62ex;" src="images/10.svg" alt=" " data-tex="A">) holds: For any given <i>direction</i> of <img style="vertical-align: -0.048ex; width: 1.717ex; height: 1.593ex;" src="images/9.svg" alt=" " data-tex="R">
<i>before</i> collision, all directions of <img style="vertical-align: -0.048ex; width: 1.717ex; height: 1.593ex;" src="images/9.svg" alt=" " data-tex="R"> <i>after</i> collision are
equally probable. Then BOLTZMANN'S H-theorem proves that if condition
<img style="vertical-align: 0; width: 1.697ex; height: 1.62ex;" src="images/10.svg" alt=" " data-tex="A"> be satisfied, then if all directions of the relative velocity
<img style="vertical-align: -0.048ex; width: 1.717ex; height: 1.593ex;" src="images/9.svg" alt=" " data-tex="R"> for given <img style="vertical-align: -0.05ex; width: 1.74ex; height: 1.595ex;" src="images/8.svg" alt=" " data-tex="V"> are not equally likely, the effect of collisions
<span class="pagenum" id="Page_15">[Pg 15]</span>
is to make <img style="vertical-align: 0; width: 2.009ex; height: 1.545ex;" src="images/11.svg" alt=" " data-tex="H"> diminish." [In essence BURBURY'S condition <img style="vertical-align: 0; width: 1.697ex; height: 1.62ex;" src="images/10.svg" alt=" " data-tex="A"> says
no more than that Theory of Probabilities is applicable for finding
number of collisions.] Furthermore, "any actual material system receives
disturbances from without, the effect of which coming at haphazard
without regard to state of system for the time being is, <i>pro
tanto</i>, to renew or maintain the independence of the molecular
motions, that very distribution of co-ordinates (of collision) which is
required to make <img style="vertical-align: 0; width: 2.009ex; height: 1.545ex;" src="images/11.svg" alt=" " data-tex="H"> diminish. So there is a general tendency for
<img style="vertical-align: 0; width: 2.009ex; height: 1.545ex;" src="images/11.svg" alt=" " data-tex="H"> to diminish, though it may conceivably increase in particular
cases. Just as in matters political, change for the better is possible,
but the tendency is for all change to be from bad to worse." Here
BURBURY states what is practically true in all actual cases and thus
furnishes an additional reason, if that were needed, for the legitimacy
of the Probability method pursued by Boltzmann, and, another explanation
of why the results obtained are in such perfect accord with experience.
</p>
<p>
As BURBURY'S remarks with respect to the nature of "elementary chaos"
under consideration are always illuminating, we will, at the risk of
repeating something already said, quote the following:
</p>
<p>
"The chance that the spheres <i>approaching</i> collision shall have
velocities within assigned limits is independent of their relative
position, and of the positions and velocities of all other spheres, and
also independent of the past history of the system except so far as this
has altered the distribution of the velocities <i>inter se</i>. In the
following example this independence is satisfied for the <i>initial
state</i> and, for the assumed method of distribution, has no past
history.
</p>
<p>
"<i>Example</i>. A great number of equal elastic spheres, each of
unit mass and diameter <img style="vertical-align: -0.023ex; width: 1.197ex; height: 1.02ex;" src="images/12.svg" alt=" " data-tex="a">, are at an initial instant set in motion
within a field <img style="vertical-align: -0.05ex; width: 1.459ex; height: 1.645ex;" src="images/2.svg" alt=" " data-tex="S"> of no force and bounded by elastic walls. The
initial motion is formed as follows: (1) One person assigns component
velocities <img style="vertical-align: -0.439ex; width: 6.023ex; height: 1.441ex;" src="images/13.svg" alt=" " data-tex="u, v, w"> to each sphere according to <i>any law</i> subject
to the conditions that <img style="vertical-align: -0.186ex; width: 19.094ex; height: 1.731ex;" src="images/14.svg" alt=" " data-tex="\Sigma u = \Sigma v = \Sigma w = 0"> and that
<span class="pagenum" id="Page_16">[Pg 16]</span>
<img style="vertical-align: -0.566ex; width: 20.113ex; height: 2.452ex;" src="images/15.svg" alt=" " data-tex="\Sigma (u^{2}  +v^{2} + w^{2}) = a"> given constant. (2) Another person,
in complete ignorance of the velocities so assigned, scatters the spheres
at haphazard throughout <img style="vertical-align: -0.05ex; width: 1.459ex; height: 1.645ex;" src="images/2.svg" alt=" " data-tex="S">. And they start from the initial
positions so assigned by (2) with the velocities assigned to them
respectively by (1)."
</p>
<p>
The system thus synthetically constructed would without doubt, at the
start be "molekular-ungeordnet"&mdash;in fact, it is as near an approach
to chaos as is possible in an imperfect world. But there is reason to
doubt if it would <i>continue</i> to be thus "molekular-ungeordnet." For
the distribution of velocities is according to <i>any law</i> consistent
with the above-mentioned conditions and some such laws would lead to
results hostile to the Second Law, and then we may safely say such laws
of velocity distribution would never occur in Nature and would therefore
belong to the cases which have been specially excepted.
</p>
<p>
Now there are mechanical systems which possess the entropy property and
it has been truly said that the Second Law and irreversibility do not
depend on any special peculiarity of heat motion, but only on the
statistical property of a system possessing an extraordinary number of
degrees of freedom. In this sense Professor J. W. GIBBS treated
Mechanics statistically and showed that then the properties of
temperature and entropy resulted. This matter has already been touched
upon, but as numerous degrees of freedom is a feature of the "elementary
chaos" under consideration it deserves repetition here and more than a
passing mention.
</p>
<p>
<i>Illustration of Degrees of Freedom</i>. Refer a body's motion to
three axes, <img style="vertical-align: -0.439ex; width: 7.302ex; height: 1.984ex;" src="images/16.svg" alt=" " data-tex="X, Y, Z">. If a body has as general a motion as possible, it
may be resolved into <i>translations</i> parallel to the <img style="vertical-align: -0.439ex; width: 7.302ex; height: 1.984ex;" src="images/16.svg" alt=" " data-tex="X, Y, Z"> axes
and to <i>rotations</i> about these axes. Each of these two sets furnishes
three components of motion or a total of six components; then
we say that the perfectly unconstrained motion of the body has
six degrees of freedom. If a body moves parallel to one of the
co-ordinate planes, we say it has two degrees of freedom. When
<span class="pagenum" id="Page_17">[Pg 17]</span>
we come to consider molecular motion in general and the independence
which characterizes the motion of each of the many molecules we see that
altogether we have here an extraordinary number of degrees of freedom, and
composed of such is the realm of our "elementary chaos."
</p>
<p>
If we go to the other extreme and think of only one atom, we see at once
that we cannot properly speak of its disorder. But the case is different
with a moderate number of atoms, say, a hundred or a thousand. Here we
surely can speak of disorder if the co-ordinates of location and the
velocity components are distributed by haphazard among the atoms. But as
the process as a whole, the sequence of events in the aggregate, may not
with this comparatively small number of atoms take place before a
macroscopic observer in a unique (unambiguous) manner, we cannot say
that we have here reached a true state of "elementary chaos." If we now
ask as to the minimum number of atoms necessary to make the process an
irreversible one, the answer is, as many as are necessary to form
determinate mean values which will define the progress of the state in
the macroscopic sense. Only for these mean values does the Second Law
possess significance; for these, however, it is perfectly exact, just as
exact as the theorem of probability, which says that the mean value of
numerous throws with one cubical die is equal to 3½.
</p>
<p>
We may now properly infer from all these views that the state of
"elementary chaos" (or "molekular ungeordnete" motion) is the necessary
condition for adequate haphazard and makes the application of the Theory
of Probabilities possible.
</p>

<div class="footnote">

<p class="nind">
<a id="Footnote_4_1"></a><a href="#FNanchor_4_1"><span class="label">[4]</span></a>On p. 133 of Wärmestrahlung PLANCK says, "only measurable mean values
are <i>kontrollierbar</i>," and this may help us to get the meaning here.</p></div>

<div class="footnote">

<p class="nind">
<a id="Footnote_5_1"></a><a href="#FNanchor_5_1"><span class="label">[5]</span></a>In BOLTZMANN'S H-Theorem we have a process (consisting of a number of
separately reversible processes) which is irreversible in the aggregate.</p></div>

<div class="footnote">

<p class="nind">
<a id="Footnote_6_1"></a><a href="#FNanchor_6_1"><span class="label">[6]</span></a><i>Nature</i>, Vol. LI, p. 78, Nov. 22, 1894.</p></div>

<p><br><br></p>

<p class="center">
(3) <i>Settled and Unsettled States; Distinction between Final Stage<br>
of Elementary Chaos and its Preceding Stages</i></p>

<p>
The immediate purpose in the next few pages is to establish
the (<i>a</i>) distinction between the successive stages of "elementary
disorder" (chaos) as they develop in their inevitable passage
<span class="pagenum" id="Page_18">[Pg 18]</span>
from "abnormal" conditions to the final and so-called "normal"
condition of thermal equilibrium and, furthermore, (<i>b</i>) to show that
each of these stages is "elementar-ungeordnet" and (<i>c</i>) that in
each one sufficient haphazard prevails to permit the legitimate
application of the Theory of Probabilities.
</p>
<p>
We will first describe the unsettled (abnormal) and settled (normal)
states, respectively. When we consider the general state of a gas "we
need not think of the state of equilibrium, for this is still further
characterized by the condition that its entropy is a maximum. Hence in
the general or unsettled state of the gas an unequal distribution of
density may prevail, any number of arbitrarily different streams (whirls
and eddies) may be present, and we may in particular assume that there
has taken place no sort of equalization between the different velocities
of the molecules. We may assume beforehand, in perfectly arbitrary
fashion, the velocities of the molecules as well as their co-ordinates
of location. But there must exist (in order that we may know the state
in the macroscopic sense), certain mean values of density and velocity,
for it is through these very mean values that the state is characterized
from the macroscopic standpoint." The differences that do exist in the
successive stages of disorder of the unsettled state are mainly due to
the molecular collisions that are constantly taking place and which thus
change the locus and velocity of each molecule.
</p>
<p>
We may now easily describe the settled state as a special case of the
unsettled one. In the settled state there is an equal distribution of
density throughout all the elementary spaces, there are no different
streams (whirls or eddies) present, and an equal partition of energy
exists for all the elementary spaces. For it thermal equilibrium exists,
the entropy is a maximum, and temperature of the state has now a
definite meaning, because temperature is the mean energy of the
molecules for this state of equilibrium. The condition is said to be a
"stationary" or permanent one, for the mean values of the density,
<span class="pagenum" id="Page_19">[Pg 19]</span>
velocity, and temperature of this particular aggregate no longer change,
although molecular collisions are still constantly occurring.
</p>
<p>
Well-known examples of the unsettled state of a system are: The
turbulent state with its different streams, whirls, and eddies, the
state in which the potential and kinetic energy is unequally
distributed; for instance, when one part is at a high pressure and
another part at a lower pressure, when one part is hotter than another
part, and when unmixed gases are present in a communicating system.
</p>
<p>
A more specific feature of the unsettled state may be found
in the accompaniment to BURBURY'S condition <img style="vertical-align: 0; width: 1.697ex; height: 1.62ex;" src="images/10.svg" alt=" " data-tex="A"> (already mentioned
at bottom of <a href="#Page_15">p. 15</a>) where it is intimated that
(at the start and after collision) all directions of the relative velocity
<img style="vertical-align: -0.048ex; width: 1.717ex; height: 1.593ex;" src="images/9.svg" alt=" " data-tex="R"> may not be equally likely.
</p>
<p>
When such differences have all disappeared to the extent that equal
elementary spaces possess their equal shares of the different particles,
velocities, and energies, the system will be a settled one, be in
thermal equilibrium, and will possess a maximum entropy and a definite
temperature. Moreover, BURBURY'S condition <img style="vertical-align: 0; width: 1.697ex; height: 1.62ex;" src="images/10.svg" alt=" " data-tex="A"> is here fully
satisfied.
</p>
<p>
At this point we again call attention to the fact, that in both the
unsettled and settled states of a system all conceivable micro-states
are not equally likely to obtain. On <a href="#Page_19">p. 19</a>
mention was made that the unsettled and the settled state each possessed
a host of conceivable micro-states which agreed with the characteristic
averages of their respective macro-states (the unsettled and the settled
ones), and yet in each set some of these led subsequently to events
which did not accord with experience. Therefore for both the unsettled
and the settled state we must limit the manifold character of their
micro-states by eliminating all those micro-states which lead to results
contrary to experience. This is accomplished by assuming the hypothesis
of "elementary-disorder" (elementar-ungeordnet) to obtain for the
unsettled as well as the settled state. Now so far as the haphazard
<span class="pagenum" id="Page_20">[Pg 20]</span>
character of the remaining motions are concerned, we might stop right here,
for the very nature of this hypothesis insures results in harmony
with experience, i.e., with the undisturbed operation of the laws
of probability.
</p>
<p>
But if we do not stop here, preferring to examine some of the special
features of fortuitous motion, as detailed on pp. <a href="#Page_10">10</a>,
<a href="#Page_13">13</a>, <a href="#Page_14">14</a> and <a href="#Page_17">17</a>,
we still see that by this hypothesis we have not removed the haphazard
character of the remaining motions in either the unsettled or the settled
state. For instance, we have not removed BURBURY'S condition <img style="vertical-align: 0; width: 1.697ex; height: 1.62ex;" src="images/10.svg" alt=" " data-tex="A">. We must
remember, too, that in PLANCK'S briefest statement of "elementary disorder"
(bot. of <a href="#Page_11">p. 11</a>), two important features of haphazard
are emphasized, viz.: the independence and great number of the
constituents. BOLTZMANN in his Gas Theorie of course considers the
special features which underlie the application of the Calculus of
Probabilities; thus he says they are, the great number of molecules and
the length of their paths, which together make the laws of the collision
of a molecule in a gas independent of the place where it collided
before. Neither has the introduction of the hypothesis of "elementary
disorder" done away with these special features. There have simply been
excluded from consideration such pre-computed and prearranged
regularities in the paths and directions of molecules as purposely
interfere with the operation of the laws of probability. We are still
free to consider all the imaginable positions and velocities of the
individual molecules which are compatible with the mean velocity,
density, and temperature properly characteristic of each stage of the
passage from the unsettled to the settled state. For adequate haphazard
we only need the assumption that the molecules fly so irregularly as to
permit the operation of the laws of probabilities. Such a presentation
as this of course calls for complete trust that all the specified
requirements have been adequately met and BOLTZMANN'S eminence as a
mathematical physicist and the endorsement of his peers must be
<i>our</i> guarantee for such confidence and trust.
<span class="pagenum" id="Page_21">[Pg 21]</span>
</p>
<p>
Before closing this discussion of unsettled and settled states we will
insert here two remarks, really at this stage, anticipatory in their
nature. The first is, that under the limitation imposed by our
supplementary hypothesis of "elementary chaos," the very sharpest
definition of any macro-state is the <i>number of its possible
micro-states</i>. This is evidently the number of permutations, possible
with the given locus and velocity elements under the restriction imposed
above. Later on we will find that this number of possible micro-states
is smaller for the unsettled state than for the settled one. This gives
us a clean-cut distinction between the two states contemplated. The
second remark is that the inevitable change in the system as a whole is
always from the less probable to the more probable, is a passage from an
unsettled state of the system to its settled state and this is here
synonymous with the growth of the number of possible micro-states. It is
this difference between the initial and final states which constitutes
the universal <i>driving motive</i> in all natural events.
</p>

</div>

<div class="chapter">
<h2><a id="SECTION_B">SECTION B</a>
<br><br>
THE APPLICATION OF CALCULUS OF PROBABILITIES<br>
IN MOLECULAR PHYSICS.</h2>

<p class="center">
(1) <i>The Probability Concept, its Usefulness in the Past, its Present<br>
Necessity, and its Universality.</i></p>

<p>
An indication of its essential value in this physical discussion
is evidenced by the fact that we have almost unwittingly been forced
to constantly refer to it in all of our preliminaries. But when
this concept is first broached to a student, he feels about it like
the "man in the street"; it is by the latter regarded as a matter
of chance and hence of uncertainty and unreliability; moreover,
the latter knows in a vague way that the subject has to do with
averages, that it is often of a statistical nature, and knows that
statistics in general are widely distrusted. The student is at
<span class="pagenum" id="Page_22">[Pg 22]</span>
first likely to share these views with said man in the street, and
at best feels that its introduction is of remote interest, far fetched,
and tends to hide and dissipate the kernel of the matter. The
student must disabuse himself of these false notions by reflecting
how much there is in Nature that is spontaneous, in other words,
how many events there are in which there is a passage from a
less probable to a more probable condition and that he cannot
afford to despise or ignore a Calculus which measures these
changes as exactly as possible.
</p>
<p>
In this connection BOLTZMANN says: (W. S. B. d. Akad. d.
Wiss., Vol. LXVI, B 1872, p. 275).
</p>
<p>
"The mechanical theory of heat assumes that the molecules of
gases are in no way at rest but possess the liveliest sort of motion,
therefore, even when a body does not change its state, every one
of its molecules is constantly altering its condition of motion and
the different molecules likewise simultaneously exist side by side
in most different conditions. It is solely due to the fact that we
always get the same average values, even when the most irregular
occurrences take place under the same circumstances, that we can
explain why we recognize perfectly definite laws in warm bodies.
For the molecules of the body are so numerous and their motions
so swift that indeed we do not perceive aught but these average
values. We might compare the regularity of these average values
with those furnished by general statistics which, to be sure, are
likewise derived from occurrences which are also conditioned by
the wholly incalculable co-operation of the most manifold external
circumstances. The molecules are as it were like so many individuals
having the most different kinds of motion, and it is only
because the number of those which on the average possess the
same sort of motion is a constant one that the properties of the
gas remain unchanged. The determination of the average values
is the task of the Calculus of Probabilities. The problems of
the mechanical theory of heat are therefore problems in this
calculus. It would, however, be a mistake to think any uncertainty
<span class="pagenum" id="Page_23">[Pg 23]</span>
is attached to the theory of heat because the theorems of probability
are applied. One must not confuse an imperfectly proved proposition
(whose truth is consequently doubtful) with a completely established
theorem of the Calculus of Probabilities; the latter represents,
like the result of every other calculus, a necessary consequence
of certain premises, and if these are correct the result is confirmed
by experience, provided a sufficient number of cases has been
observed, which will always be the case with Heat because of the
enormous number of molecules in a body."
</p>
<p>
To become more specific we will mention some of the problems
to which the Theory of Probabilities has been profitably applied.
In business to life and fire insurance; in engineering to reducing
the inevitable errors of observations by the Method of Least
Squares; and in physics to the determination of Maxwell's Law
of the distribution of velocities. The results thus obtained are
universally trusted and accepted by experts. Why then should
this Calculus not be applicable to the more <i>general</i> natural events?
</p>
<p>
In this connection consider some of its good points: (<i>a</i>) It
eliminates from a problem the accidental elements if the latter
are sufficiently numerous; (<i>b</i>) it deals legitimately with averages;
(<i>c</i>) it involves combination considerations other than averages;
(<i>d</i>) it is available for non-mechanical as well as mechanical
occurrences and thus (<i>e</i>) has a capacity for covering the whole range
of natural events, giving it a character of universality which is
now its most valuable asset.
</p>
<p>
As an example of this we may instance BOLTZMANN'S deservedly famous
H-theorem, which establishes the one-sidedness of <i>all</i> natural
events.<a id="FNanchor_7_1"></a><a href="#Footnote_7_1" class="fnanchor">[7]</a> Concerning it, this master in mathematical
physics says:
</p>
<p>
"It can only be deduced from the laws of probability that, if the
initial state is not especially arranged for a certain purpose, the
<span class="pagenum" id="Page_24">[Pg 24]</span>
probability that <img style="vertical-align: 0; width: 2.009ex; height: 1.545ex;" src="images/11.svg" alt=" " data-tex="H"> decreases is always greater than that it
increases. In this connection we may add that BOLTZMANN looked forward
to a time, "when the fundamental equations for the motion of individual
molecules will prove to be merely approximate formulas, which give
average values which, according to the Theory of Probabilities, result
from the co-operation of very many independently moving individuals
constituting the surrounding medium, for example, in meteorology the
laws will refer only to average values deduced by the Theory of
Probabilities from a long series of observations. These individuals must
of course be so numerous and act so promptly that the correct average
values will obtain in millionths of a second."
</p>
<p>
To further strengthen our faith we may point out that the probability
method has been successfully used to determine unique results from
complicated conditions and has been employed for the general treatment
of problems. In the case before us it has solved the entropy puzzle
which has exercised physicists, as well as engineers, for decades, and
it has thereby emancipated the Second Law from all anthropomorphism,
from all dependence on human experimental skill. When we take the
broadest possible view of its character, this Calculus enables us to
read the present riddle of our universe, namely, why it is in its
present improbable state. We have therefore in this Calculus an engine
for investigation which is of great power and is likely to play a large
part in the future in the ascertainment of physical truth. Of course it
must then be in the hands of masters. It is they and they alone who can
properly and adequately interpret such a physical problem as the one
before us. In scientific work our last court of appeal must be Nature,
and we therefore say: The best justification for the use of the Theory
of Probabilities in our problem is that its results are in such complete
accord with the facts.
</p>
<p>
In dealing with this physical engine of investigation, we must
again call attention to some of the features of haphazard necessary
<span class="pagenum" id="Page_25">[Pg 25]</span>
for its legitimate application. Of course the statement of these
features will vary with the mechanical or non-mechanical character of
the problem to which it is applied. As we are here dealing mainly with
the former, we will limit ourselves to its features: (<i>a</i>) The
elements dealt with must be very numerous, strictly speaking, infinite;
(<i>b</i>) as a phase of (<i>a</i>) we may say also that when we speak
of the probability of a state we express the thought that it can be
realized in many different ways; (<i>c</i>) when we speak of the
relative directions of a pair of molecules all possible directions must
be considered; (<i>d</i>) we must so weight the elements say, in
(<i>a</i>), (<i>b</i>), and (<i>c</i>) that they are equally likely;
(<i>e</i>) every one of the entering elements must possess constituents
of which each individual is independent of every other; for instance,
(<i>f</i>) in a gas the place where a molecule collided must be
independent of the place where it collided before. In our physical
problem all of these features are not always realized; for instance, the
number of particles of gas are only finite instead of being infinite;
again, all relative velocities after collision of a pair of molecules
are not equally likely; BOLTZMANN and BURBURY provide for these
shortcomings by very truly asserting that in <i>actual</i> cases we are
not dealing with isolated systems, that the surrounding walls are not
impervious to external influences, and that the latter come at haphazard
without regard to internal state of the system at the time, thus
renewing and maintaining the desired state of haphazard.
</p>
<p>
<i>Methods</i>. This Calculus works largely by the determination of
<i>averages</i> and its results must be interpreted accordingly.
Moreover, for the present we will take a popular, practical view of
these results and consider a very great improbability as equivalent to
an impossibility. Numerical computations are essential in most uses of
this Calculus, but here they will be entirely omitted.
</p>

<div class="footnote">

<p class="nind">
<a id="Footnote_7_1"></a><a href="#FNanchor_7_1"><span class="label">[7]</span></a>The H-theorem considers a process (consisting of a number of separate,
reversible processes) which is irreversible in the aggregate.</p></div>

<p><span class="pagenum" id="Page_26">[Pg 26]</span></p>

<p><br><br></p>

<p class="center">
(2) <i>What is Meant by the Probability of a State? Example</i></p>

<p>
To come back to the matter in hand we will now show what is here meant
by the probability of any state.
</p>
<p>
When we speak of the probability <img style="vertical-align: -0.05ex; width: 2.371ex; height: 1.595ex;" src="images/17.svg" alt=" " data-tex="W"> of a particular
"elementar-ungeordnete" state, we thereby imply that this state may be
variously realized. For every state (which contains many like
independent constituents) corresponds to a certain "distribution,"
namely, a distribution among the gas molecules of the location
co-ordinates and of the velocity components. But such a distribution is
a permutation problem, is always an assignment of one set of like
elements (co-ordinates, velocity components) to a different set of like
elements (molecules). So long as only a particular state is kept in
view, it is of consequence as to how many elements of the two sets are
thus interchangeably assigned to each other and not at all as to which
individual elements of the one set are assigned to particular individual
elements of the other set.<a id="FNanchor_8_1"></a><a href="#Footnote_8_1" class="fnanchor">[8]</a> Then a particular state may be realized by a great
number of assignments individually differing from one another,
but all equally likely to occur.<a id="FNanchor_9_1"></a><a href="#Footnote_9_1" class="fnanchor">[9]</a> If with PLANCK we call such an
assignment a "complexion,"<a id="FNanchor_10_1"></a><a href="#Footnote_10_1" class="fnanchor">[10]</a> we may now say that in general a
particular state contains a large number of different, but equally
likely, complexions. This number, i.e., <i>the number of the complexions
included in a given state can now be defined as the probability
<img style="vertical-align: -0.05ex; width: 2.371ex; height: 1.595ex;" src="images/17.svg" alt=" " data-tex="W"> of the state</i>.<a id="FNanchor_11_1"></a><a href="#Footnote_11_1" class="fnanchor">[11]</a>

Let us present the matter in still another form. BOLTZMANN
derives the expression for magnitude of the probability by at
<span class="pagenum" id="Page_27">[Pg 27]</span>
once distinguishing between a <i>state</i> of a considered system and
the <i>complexion</i> of the considered system. A state of the system is
determined by the law of locus and velocity distribution, i.e., by a
statement of the number of particles which lie in each elementary
district of space and the number of particles which lie in each
elementary velocity realm, assuming that among themselves these
districts and realms are alike and each such infinitesimal element still
harbors very many particles. Accordingly a particular state of the
system embraces a very large number of complexions. For if any two
particles belonging to different regions swap their co-ordinates and
velocities, we get thereby a new complexion, but still the same state.
Now BOLTZMANN assumes <i>all complexions to be equally probable</i> and
therefore the number of complexions included in a particular state
furnishes at the same time the numerical value for the <i>Probability of
the state</i> in question. Illustration taken from the simultaneous
throwing of two, ordinary, cubical dice. Suppose that the sum is to be 4
for each throw, then this can be realized by the following three
complexions:
</p>
<p class="center">
First cube shows 1, the second cube shows 3;
<br>
First cube shows 2, the second cube shows 2;
<br>
First cube shows 3, the second cube shows 1.
</p>
<p class="nind">
The requirement that the sum on the two cubes shall be 2, however,
involves but one complexion. Under the circumstances therefore the
probability of throwing the sum 4 is three times as great as throwing
the sum 2.
</p>
<p>
In closing this part of our presentation, we may make what is now an
almost obvious remark. The long-lasting difficulty in giving a physical
meaning to entropy and the Second Law is due to the fact of its intimate
dependence on considerations of probability. It is only quite recently
that such considerations have attained the dignity of a great working
principle in the domain of Physics.
</p>

<div class="footnote">

<p class="nind">
<a id="Footnote_8_1"></a><a href="#FNanchor_8_1"><span class="label">[8]</span></a>For an example of such permutations see pp. <a href="#Page_28">28</a> and <a href="#Page_61">61</a>, <a href="#Page_62">62</a>.</p></div>

<div class="footnote">

<p class="nind">
<a id="Footnote_9_1"></a><a href="#FNanchor_9_1"><span class="label">[9]</span></a>LIOUVILLE'S theorem is the criterion for the equal possibility or equal probability
of different state distributions.</p></div>

<div class="footnote">

<p class="nind">
<a id="Footnote_10_1"></a><a href="#FNanchor_10_1"><span class="label">[10]</span></a>A happy term, but one not in vogue among English-speaking physicists.</p></div>

<div class="footnote">

<p class="nind">
<a id="Footnote_11_1"></a><a href="#FNanchor_11_1"><span class="label">[11]</span></a>The identity of entropy with the logarithm of this state of probability <img style="vertical-align: -0.05ex; width: 2.371ex; height: 1.595ex;" src="images/17.svg" alt=" " data-tex="W">
is established by showing that both are equal to the same expression. It
seems an easy step from this derivation to BOLTZMANN'S definition of
entropy as the "measure of the disorder of the motions in a system of mass
points."</p></div>

<p><span class="pagenum" id="Page_28">[Pg 28]</span></p>

</div>

<div class="chapter">
<h2><a id="SECTION_C">SECTION C</a></h2>

<p class="center">
(1) <i>Existence, Definition, Measure, Relations, Properties, and
Scope of Irreversibility and Reversibility.</i>
</p>

<p>
In establishing the existence of irreversibility, we can use
one or both of the two general methods of approaching any physical
problems (see Introduction, pp. <a href="#Page_2">2</a>, <a href="#Page_3">3</a>) we can approach by way
of the atomic theory or by considering the behavior of aggregates
in Nature. Enough has already been said in this presentation
of atomic behavior and arrangements to justify the statement
that irreversibility is not inherent in the elementary procedures
themselves but in their irregular arrangement. The motion
of each atom is by itself reversible, but their combined mean
effect is to produce something irreversible.<a id="FNanchor_12_1"></a><a href="#Footnote_12_1" class="fnanchor">[12]</a>
</p>
<p>
This has been rigorously demonstrated by BOLTZMANN'S H-theorem for
molecular physics, and when sufficiently general co-ordinates are
substituted it is also available for the other domains of natural
events. When we consider the behavior of aggregates we recognize at once
a general, empirical law, which has also been called the <i>one physical
axiom</i>, namely, that all natural processes are essentially
irreversible. When we use this method of approach we confessedly rest
entirely on experience, and then it does not make any logical difference
whether we start with one particular fact or another, whether we start
with a fact itself or its necessary consequence: For instance we may
recognize that the universe is permanently different after a frictional
event from what it was before, or we may start, as PLANCK does, by
putting forward the following proposition:
</p>
<p>
"<i>It is impossible to construct an engine which will work in
<span class="pagenum" id="Page_29">[Pg 29]</span>
a complete cycle</i>,<a id="FNanchor_13_1"></a><a href="#Footnote_13_1" class="fnanchor">[13]</a> <i>and produce no effect except the raising of a weight
and the cooling of a heat reservoir.</i>"<a id="FNanchor_14_1"></a><a href="#Footnote_14_1" class="fnanchor">[14]</a>
</p>
<p>
Now up to this time no natural event has contradicted this theorem or
its corollaries. The proof for it is cumulative, wholly experiential and
therefore exactly like that for the law of conservation of energy.
</p>
<p>
Returning to irreversibility, the matter for immediate discussion, we
premise that it will here clarify and simplify our ideas if we consider
<i>all the participating</i> bodies as parts of the system experiencing
the contemplated process. It is in this sense that we must understand
the statement: Every natural event leaves the universe different from
what it was before. Speaking very generally, we may say that in this
difference lies what we call irreversibility.
</p>
<p>
Now irreversibility is what really does exist, everywhere in Nature, and
our idea of reversibility is only a very convenient and fruitful
fiction; our conception of reversibility must, therefore, ultimately be
derived from that of irreversibility.
</p>
<p>
"A process which can in no way be completely reversed is termed
<i>irreversible</i>, all other processes <i>reversible</i>. That a
process may be irreversible, it is not sufficient that it cannot be
directly reversed. This is the case with many mechanical processes which
are not irreversible (See <a href="#Page_32">p. 32</a>). The full
requirement is, that it be impossible, even with the assistance of all
agents in Nature, to restore everywhere the exact initial state when the
process has once taken place."
</p>
<p>
Examples of irreversible processes, which involve only heat and
mechanical phenomena, may be grouped in four classes:
</p>
<p>
(<i>a</i>) The body whose changes of state are considered is in
contact with bodies whose temperature differs by a finite amount
<span class="pagenum" id="Page_30">[Pg 30]</span>
from its own. There is here flow of heat from the hotter to the
colder body and the process is an irreversible one.
</p>
<p>
(<i>b</i>) The body experiences resistance from friction which develops
heat; it is not possible to effect completely the opposite operation of
restoring the whole system to its initial state.
</p>
<p>
(<i>c</i>) The body expands without at the same time developing an
amount of <i>external</i> energy which is exactly equal to the work of
its own elastic forces. For example, this occurs when the pressure which
a body has to overcome is essentially (i.e., finitely) less than the
body's own internal tension. In such a case it is not possible to bring
the whole system (of which the body is a part) completely back into its
initial state. Illustrations are: steam escaping from a high-pressure
boiler, compressed air flowing into a vacuum tank, and a spring suddenly
released from its state of high tension.
</p>
<p>
(<i>d</i>) Two gases at the same pressure and temperature are separated
by a partition. When this is suddenly removed, the two gases mix or
diffuse. This too is an essentially irreversible process.
</p>
<p>
Outside of chemical phenomena, we may instance still other examples of
irreversible processes: flow of electricity in conductors of finite
resistance, emission of heat and light radiation, and decomposition of
the atoms of radio-active substances.
</p>
<p>
"Numerous reversible processes can at least <i>be imagined</i>,
as, for instance, those consisting throughout of a succession of
states of equilibrium, and therefore directly reversible in all their
parts. Further, all perfectly periodic processes, e.g., an ideal
pendulum or planetary motion, are reversible, for, at the end of
every period the initial state is completely restored. Also, all
mechanical processes with absolutely rigid bodies and incompressible
liquids, as far as friction can be avoided, are reversible.
By the introduction of suitable machines with absolutely unyielding
connecting-rods, frictionless joints, and bearings, inextensible
belts, etc., it is always possible to work the machine in such a
<span class="pagenum" id="Page_31">[Pg 31]</span>
way as to bring the system completely into its initial state without
leaving any change in or out of the machines, for the machines
of themselves do not perform any work."
</p>
<p>
Other examples of such reversible processes are: Free fall in a vacuum,
propagation of light and sound waves without absorption and reflection
and unchecked electrical oscillations. All the latter processes are
either naturally periodic, or they can be made completely reversible by
suitable devices so that no sort of change in Nature remains behind; for
example, the free fall of a body by utilizing the velocity acquired to
bring the body back to its original height, light and sound waves by
suitably reflecting them from perfect mirrors.
</p>

<div class="footnote">

<p class="nind">
<a id="Footnote_12_1"></a><a href="#FNanchor_12_1"><span class="label">[12]</span></a>This would seem to imply the existence of a broader principle, the properties
of systems as a whole are <i>not</i> necessarily found in their parts.</p></div>

<div class="footnote">

<p class="nind">
<a id="Footnote_13_1"></a><a href="#FNanchor_13_1"><span class="label">[13]</span></a>Such an engine if it would work might be called "perpetual motion of the
second kind."</p></div>

<div class="footnote">

<p class="nind">
<a id="Footnote_14_1"></a><a href="#FNanchor_14_1"><span class="label">[14]</span></a>The term perpetual is justified because such an engine would possess the most
esteemed feature of perpetual motion&mdash;power production free of cost.</p></div>

<p><br><br></p>

<p class="center">
(2) <i>Character of Process Decided by the Limiting States</i>
</p>

<p>
"Since the decision as to whether a particular process is irreversible
or reversible depends only on whether the process can in any manner
whatsoever be completely reversed or not, the nature of the initial and
final states, and not the intermediate steps of the process, entirely
settle it. The question is, whether or not it is possible, starting from
the final state, to reach the initial one in any way without any other
change.... The final state of an irreversible process is evidently in
some way discriminate from the initial state, while in reversible
processes the two states are in certain respects equivalent.... To
discriminate between the two states they must be fully characterized.
Besides the chemical constitution of the systems in question, the
physical conditions, viz., the state of aggregation, temperature, and
pressure in both states, must be known, as is necessary for the
application of the First Law."
</p>
<p>
"Let us consider any process whatsoever occurring in Nature.
This conducts all participating bodies from a particular initial
condition <img style="vertical-align: 0; width: 1.697ex; height: 1.62ex;" src="images/10.svg" alt=" " data-tex="A"> to a certain final condition <img style="vertical-align: 0; width: 1.717ex; height: 1.545ex;" src="images/18.svg" alt=" " data-tex="B">. The process is
either reversible or irreversible, any third possibility being
<span class="pagenum" id="Page_32">[Pg 32]</span>
excluded. But whether it is reversible or irreversible depends solely
and only on the constitution of the two states <img style="vertical-align: 0; width: 1.697ex; height: 1.62ex;" src="images/10.svg" alt=" " data-tex="A"> and <img style="vertical-align: 0; width: 1.717ex; height: 1.545ex;" src="images/18.svg" alt=" " data-tex="B">, not upon
the other features of the course; after state <img style="vertical-align: 0; width: 1.717ex; height: 1.545ex;" src="images/18.svg" alt=" " data-tex="B"> has been attained,
we must here simply answer the question whether the complete return to
<img style="vertical-align: 0; width: 1.697ex; height: 1.62ex;" src="images/10.svg" alt=" " data-tex="A"> can or cannot be effected in any manner whatsoever. Now if such
complete return from <img style="vertical-align: 0; width: 1.717ex; height: 1.545ex;" src="images/18.svg" alt=" " data-tex="B"> to <img style="vertical-align: 0; width: 1.697ex; height: 1.62ex;" src="images/10.svg" alt=" " data-tex="A"> is not possible then evidently state
<img style="vertical-align: 0; width: 1.717ex; height: 1.545ex;" src="images/18.svg" alt=" " data-tex="B"> in Nature is somehow distinguished from state <img style="vertical-align: 0; width: 1.697ex; height: 1.62ex;" src="images/10.svg" alt=" " data-tex="A">. Nature may be
said to prefer state <img style="vertical-align: 0; width: 1.717ex; height: 1.545ex;" src="images/18.svg" alt=" " data-tex="B"> to state <img style="vertical-align: 0; width: 1.697ex; height: 1.62ex;" src="images/10.svg" alt=" " data-tex="A">. Reversible processes are a
limiting case; here Nature manifests no preference and the passage from
the one to other can take place at pleasure, in either direction. [In
the common case of isentropic expansion from <img style="vertical-align: 0; width: 1.697ex; height: 1.62ex;" src="images/10.svg" alt=" " data-tex="A"> to <img style="vertical-align: 0; width: 1.717ex; height: 1.545ex;" src="images/18.svg" alt=" " data-tex="B">, there is no
exchange of heat with the outside; external work is performed at the
expense of the inner energy of the expanding body. When state <img style="vertical-align: 0; width: 1.717ex; height: 1.545ex;" src="images/18.svg" alt=" " data-tex="B"> is
attained we can effect a complete return to <img style="vertical-align: 0; width: 1.697ex; height: 1.62ex;" src="images/10.svg" alt=" " data-tex="A"> by compressing
isentropically, thus consuming the external work performed on the trip
from <img style="vertical-align: 0; width: 1.697ex; height: 1.62ex;" src="images/10.svg" alt=" " data-tex="A"> to <img style="vertical-align: 0; width: 1.717ex; height: 1.545ex;" src="images/18.svg" alt=" " data-tex="B"> and restoring the internal energy of the body.]
</p>
<p>
"Now it becomes a question of finding a physical magnitude whose amount
will serve as a general measure of Nature's preference for a state. This
must be a magnitude which is directly determined by the state of the
contemplated system, without knowing anything of the past history of the
system, just as is the case when we deal with the state's energy,
volume, etc. This magnitude would possess the property of growing in all
irreversible processes, while in all reversible processes it would
remain unchanged. The amount of its change in a process would furnish a
general measure for the irreversibility of the process."
</p>
<p>
"Now R. CLAUSIUS really found such a magnitude and called it entropy.
Every bodily system possesses in every state a particular entropy, and
this entropy designates the preference of nature for the state in
question; in all the processes which occur in the system, entropy can
only grow, never diminish. If we wish to consider a process in which
<span class="pagenum" id="Page_33">[Pg 33]</span>
said system is subject to influences from without, we must regard the
bodies exerting such influences as incorporated with the original system
and then the statement will hold in the above given form."
</p>
<p>
From what has gone before it is evident that the following commonly
drawn conclusions are correct:
</p>
<p>
An irreversible process is a passage from a less probable to a more
probable state of the system.
</p>
<p>
An irreversible process is a passage from a less stable to a more stable
state of the system.
</p>
<p>
An irreversible process is essentially a spontaneous one, inasmuch
as once started it will proceed without the help of any external
agency.
</p>
<p>
We have in a general way reached the conclusion that entropy is both the
criterion and the measure of irreversibility. But now let us become more
specific and go more into certain details, namely, the common features
in all irreversibility. The property of irreversibility is not inherent
in the elementary occurrences themselves, but only in their irregular
arrangement. Irreversibility depends only on the statistical property of
a system possessing many degrees of freedom, and is therefore
essentially based on mean values; in this connection we may repeat an
earlier statement, the individual motions of atoms are in themselves
reversible, but their result in the aggregate is not.
</p>

<p><br><br></p>

<p class="center">
(3) <i>All the Irreversible Processes Stand or Fall Together</i>
</p>

<p>
This is proved with the help of the theorem (<a href="#Page_30">p. 30</a>)
which denies the possibility of perpetual motion of the second
kind.<a id="FNanchor_15_1"></a><a href="#Footnote_15_1" class="fnanchor">[15]</a> The
argument is this: take any case in any one of the four classes of
irreversible processes given on <a href="#Page_31">p. 31</a>. Now if this
<span class="pagenum" id="Page_34">[Pg 34]</span>
selected case is in reality reversible, i.e., suppose a method were
discovered of <i>completely</i> reversing this process and thus leave no
other change whatsoever, then combining the direct course of the process
with this latter reversed process, they would together constitute a
cyclical process, which would effect nothing but the production of work
and the absorption of an equivalent amount of heat. But this would be
perpetual motion of the second kind, which to be sure is denied by the
empirical theorem on <a href="#Page_30">p. 30</a>. But for the sake of
the argument we may just now waive said impossibility; then we would
have an engine which, co-operating with any second (so-called),
irreversible process, would completely restore the initial state of the
whole system without leaving any other change whatsoever. Then under our
definition on <a href="#Page_30">p. 30</a> this second process ceases to
be irreversible. The same result will obtain for any third, fourth, etc.
So that the above proposition is established. "All the irreversible
processes stand or fall together." If any one of them is reversible all
are reversible.<a id="FNanchor_16_1"></a><a href="#Footnote_16_1" class="fnanchor">[16]</a>
</p>

<div class="footnote">

<p class="nind">
<a id="Footnote_15_1"></a><a href="#FNanchor_15_1"><span class="label">[15]</span></a>At this stage we appreciate that any irreversible process
is a passage from a state <img style="vertical-align: 0; width: 1.697ex; height: 1.62ex;" src="images/10.svg" alt=" " data-tex="A"> of low entropy to a state <img style="vertical-align: 0; width: 1.717ex; height: 1.545ex;" src="images/18.svg" alt=" " data-tex="B"> of high
entropy. We may simplify our proof by considering the return passage
from <img style="vertical-align: 0; width: 1.717ex; height: 1.545ex;" src="images/18.svg" alt=" " data-tex="B"> to <img style="vertical-align: 0; width: 1.697ex; height: 1.62ex;" src="images/10.svg" alt=" " data-tex="A"> to in part occur isothermally and in part
isentropically; then external agencies must produce work and absorb an
equivalent amount of heat.</p></div>

<div class="footnote">

<p class="nind">
<a id="Footnote_16_1"></a><a href="#FNanchor_16_1"><span class="label">[16]</span></a>With the help of the preceding footnote this argument can be followed through
in detail for each of the cases enumerated on <a href="#Page_31">p. 31</a>;
only the complicated case of diffusion presents any difficulty.</p></div>

<p><br><br></p>

<p class="center">
(4) <i>Convenience of the Fiction, the Reversible Processes</i>
</p>

<p>
A reversible process we have declared to be only an <i>ideal case</i>, a
convenient and fruitful fiction which we can <i>imagine</i> by eliminating
from an irreversible process one or more of its inevitable
accompaniments like friction or heat conduction. But reversible
(as well as irreversible) processes have common features. "They
resemble each other more than they do any one irreversible process.
This is evident from an examination of the differential equations
which control them; the differential with respect to time is always
of an even order, because the essential sign of time can be reversed.
Then too they (in whatever domain of physics they may lie)
have the common property that the Principle of Least Action
<span class="pagenum" id="Page_35">[Pg 35]</span>
can represent all of them completely and uniquely determines
the sequence of their events." They are useful for theoretical
demonstration and for the study of conditions of equilibrium.
</p>
<p>
There is a certain, limited, incomplete sense in which we say that we
can change from one state of equilibrium to another in a reversible
manner. For example, we can, considering only the one converting (or
intermediate) body, effect said change by a successive use of isentropic
and isothermal change. But this ignores all but one of the participating
bodies and this is not permissible if we strictly adhere to the true
definition of <i>complete</i> reversible action.
</p>
<p>
We must remember too that no other universal measure of irreversibility
exists than entropy. "Dissipation" of energy has been put forward as
such a measure, but we know already of two irreversible cases where
there is no change of energy, namely, diffusion and expansion of a gas
into a vacuum. [Unavailable, distributed, scattered energy are terms
which could be used here, free from all objection.]
</p>
<p>
But of course, the full equivalent of entropy can be substituted as a
<i>universal</i> measure of irreversibility. On <a href="#Page_27">p.
27</a> we have pointed out that the <i>number of complexions</i>
included in a given state can be defined as the <i>probability W of the
state</i>, then in a footnote, attention is called to the identity of
entropy with the logarithm of this state of probability = logarithm of
the number of complexions of the state. This makes entropy a function of
the number of complexions, so that one may in this sense be regarded as
the equivalent of the other. We may now properly speak of the number of
complexions of a state as the universal measure of its irreversibility.
The physical meaning of irreversibility becomes apparent when put in
this form. The greater the number of complexions included in a state the
more disordered is its elementary condition and the more difficult (more
impossible, so to speak), is it to directly so influence the
constituents of the whole that they will reverse the sequence of the
mean values the aggregate tends of itself to assume. An illustration
<span class="pagenum" id="Page_36">[Pg 36]</span>
will help to make this clear; the irreversible case in which work
(i.e., friction) is converted into heat. "For example, the direct
reversal of a frictional process is impossible because this would
presuppose the existence of an elementary order among adjacent, mutually
interacting molecules. For then it must predominantly be the case that
the collisions of each pair of molecules must bear a certain
distinguishable character inasmuch as the velocities of two colliding
molecules must always depend in a determinate manner on the place where
they meet. Only thereby can it be attained that there will result from
the collisions predominantly like directed velocities."
</p>
<p>
The outcome of the whole study of irreversibility results in the briefly
stated law: "<i>There exists in Nature a quantity which changes always
in the same sense in all natural processes</i>."
</p>
<p>
This boldly asserts the essential one-sidedness of Nature. The
proposition stated in this general form may be correct or incorrect; but
whichever it may be it will remain so independently of human
experimental skill.
</p>

</div>

<div class="chapter">
<h2><a id="SECTION_D">SECTION D</a></h2>

<p class="center">
(1) <i>The Gradual Development of the Idea that Entropy Depends
on Probability</i>
</p>

<p>
Entropy is difficult to conceive, in that, as it does not directly
affect the senses, there is nothing physical to represent it; it cannot
be felt like temperature. It has no analogue in the whole of Physics;
Zeuner's heat weight will perhaps serve as such for reversible states,
but is inadequate for irreversible ones. This is not surprising when we
consider the outcome, namely, that it depends on probability
considerations.
</p>
<p>
CLAUSIUS coined the term Entropy from the Greek, from a word
meaning transformation; with him the transformation value
was equal to the difference between the entropy of the final and
initial states. As there is a general expression for entropy, we
<span class="pagenum" id="Page_37">[Pg 37]</span>
can readily write the equivalent of any transformation between
two particular states.
</p>
<p>
Strictly speaking, however, entropy by itself depends only on the state
in question, not on any change it may experience, nor on its past
history before reaching the state contemplated. Of course, this was
appreciated by such a master mind as CLAUSIUS, and, indeed, he defined
the entropy as the algebraic sum of the transformations necessary to
bring a body into its existing state. Moreover, as the formula for it
was in terms of other more or less sensible thermodynamic quantities,
its relation to these was at first more readily grasped, could be
represented diagrammatically, and had to do duty for the true, but still
unknown, physical idea of entropy itself. It was early understood, too,
that growth of entropy was closely connected with the degradation or
waste of energy; that it was identical with the Second Law. The
frequently given, but not always valid, relation,
<span class="align-center"><img style="vertical-align: -0.439ex; width: 10.212ex; height: 2.034ex;" src="images/19.svg" alt=" " data-tex="
dQ = T dS
"></span><a id="FNanchor_17_1"></a><a href="#Footnote_17_1" class="fnanchor">[17]</a>
led to entropy being called a factor of energy. But all these were
<i>change</i> relations and did not go to the root of the difficulty, as
to what constituted the physical nature of <i>unchanged</i> entropy.
</p>
<p>
Quite early, too, there was a realization of the fact that entropy had
somehow a statistical character, that it had to do with <i>mean</i>
values only. This was well brought out by the long known, and much
quoted, "demon" experiment suggested by Maxwell, in which a being of
superhuman power separated, without doing any work, the colder and
hotter particles of a gas, thus effecting an apparent violation of the
Second Law. This, to be sure, was getting close to the crux of the whole
matter, but still lacked much to give entropy a precise physical
meaning. Nevertheless, we see here a notable approach to the fundamental
<span class="pagenum" id="Page_38">[Pg 38]</span>
requirement that entropy must be tied down to the condition of "elementary
chaos" (elementare-unordnung).
</p>
<p>
We have already dwelt somewhat fully on this hypothesis of "elementary
chaos."
</p>
<p>
"It follows from this presentation that the concepts of entropy and
temperature in their essence are tied to the condition of "elementare
Unordnung." Thus a purely periodic absolute plane wave possesses neither
entropy nor temperature because it contains nothing whatever in the way
of uncheckable, non-measurable magnitudes, and therefore cannot be
"elementar-ungeordnet," just as little as can be the case with the
motion of a single rigid atom. When there is [an irregular co-operation
of many partial oscillations of different periods, which independently
of each other propagate themselves in the different directions of space,
or] an irregular, confused, whirring intermingling of many atoms, then
(and not till then) is there furnished the preliminary condition for the
validity of the hypothesis of "elementare Unordnung and consequently for
the existence of entropy and of temperature."
</p>
<p>
"Now what mechanical or electro-dynamic magnitude represents the entropy
of a state? Evidently this magnitude depends in some way on the
"Probability" of the state. For because "elementare Unordnung" and the
lack of every individual check (or measurement) is of the essence of
entropy it follows that only combination or probability considerations
can furnish the necessary foothold for the computation of this
magnitude. Even the hypothesis of "elementare Unordnung" by itself is
essentially a proposition in Probability, for, out of a vast number of
equally possible cases, it selects a definite number and declares they
do not exist in Nature."
</p>
<p>
Now since the idea of entropy, and likewise the content of Second Law,
is a universal one, and since, moreover, the theorems of probability
possess no less universal significance, we may conjecture (surmise) that
the connection between Entropy and Probability will be a very
<span class="pagenum" id="Page_39">[Pg 39]</span>
close one. We therefore place at the head (forefront) of our further
presentation the following proposition: "<i>The Entropy of a physical
system in a definite condition depends solely on the probability of this
state</i>." The permissibility and fruitfulness of this proposition will
become manifest later in different cases. A general and rigorous proof
of this proposition will not be attempted at this place. Indeed, such an
attempt would have no sense here because without a numerical statement
of the probability of a state it could not be tested numerically.
</p>

<div class="footnote">

<p class="nind">
<a id="Footnote_17_1"></a><a href="#FNanchor_17_1"><span class="label">[17]</span></a>This relation is not a valid one, unless the external work performed by a gas
during its change is equal to <img style="vertical-align: -0.439ex; width: 4.054ex; height: 2.009ex;" src="images/20.svg" alt=" " data-tex="p dV">.</p></div>

<p><br><br></p>

<p class="center">
(2) <i>Planck's Formula for the Relation between Entropy and the
Number of Complexions</i>
</p>

<p>
Now we have already seen, from the permutation considerations presented
on <a href="#Page_27">p. 27</a>, that the Theory of Probabilities leads
very directly to the theorem, "<i>The number of complexions included in
a given state constitutes the probability W of that state</i>." The next
step (omitted here) is to identify the thermodynamically found
expression for entropy of any state with the logarithm of its number of
complexions.
</p>
<p>
PLANCK'S formula for entropy <img style="vertical-align: -0.05ex; width: 1.459ex; height: 1.645ex;" src="images/2.svg" alt=" " data-tex="S"> is:
<span class="align-center"><img style="vertical-align: -0.566ex; width: 56.832ex; height: 2.565ex;" src="images/21.svg" alt=" " data-tex="
S = 1.35\, log_{e} \text{(number of complexions)}\, 10^{-16}
+ \text{constant}\, K;
"></span>
here <img style="vertical-align: 0; width: 2.011ex; height: 1.545ex;" src="images/22.svg" alt=" " data-tex="K"> is an arbitrary constant without physical significance and
can be omitted at pleasure; the numerical value in the first term of the
second member is the quotient of energy (expressed in ergs) divided by
temperature (<img style="vertical-align: -0.05ex; width: 2.851ex; height: 1.667ex;" src="images/23.svg" alt=" " data-tex="°C">). This certainly gives a physical definiteness and
precision to entropy which leaves nothing to be desired.
</p>
<p>
PLANCK, in reproducing from probability consideration the dependence of
entropy <img style="vertical-align: -0.05ex; width: 1.459ex; height: 1.645ex;" src="images/2.svg" alt=" " data-tex="S"> on probability <img style="vertical-align: -0.05ex; width: 2.371ex; height: 1.595ex;" src="images/17.svg" alt=" " data-tex="W">, finds the relation
<span class="align-center"><img style="vertical-align: -0.464ex; width: 23.462ex; height: 2.059ex;" src="images/24.svg" alt=" " data-tex="
S = k\,\, log W + \text{constant},
"></span>
when the dimensions of <img style="vertical-align: -0.05ex; width: 1.459ex; height: 1.645ex;" src="images/2.svg" alt=" " data-tex="S"> evidently depend on those of constant <img style="vertical-align: -0.025ex; width: 1.179ex; height: 1.595ex;" src="images/3.svg" alt=" " data-tex="k">.
<span class="pagenum" id="Page_40">[Pg 40]</span>
</p>
<p>
Here <img style="vertical-align: -0.05ex; width: 1.459ex; height: 1.645ex;" src="images/2.svg" alt=" " data-tex="S"> is BOLTZMANN'S value&mdash;<img style="vertical-align: 0; width: 2.009ex; height: 1.545ex;" src="images/11.svg" alt=" " data-tex="H">, which always changes in one
direction only; <img style="vertical-align: -0.025ex; width: 1.179ex; height: 1.595ex;" src="images/3.svg" alt=" " data-tex="k"> is the universal integration constant which is the
same for a terrestrial as for a cosmical system, and when it is known
for one, it is known for the other; when <img style="vertical-align: -0.025ex; width: 1.179ex; height: 1.595ex;" src="images/3.svg" alt=" " data-tex="k"> is known for radiant
phenomena it is also known and is the same for molecular motions.
</p>
<p>
There are some general statements which indicate more or less rigorously
some of the properties or features of the entropy of a state.
</p>
<p class="hanging2">
(<i>a</i>) Entropy is a universal measure of the "disorder" in the
mass points of a system.
</p>
<p class="hanging2">
(<i>b</i>) Entropy is a universal measure of the irreversibility of a
state and is its criterion as well.
</p>
<p class="hanging2">
(<i>c</i>) Entropy is a universal measure of nature's preference for
the state.
</p>
<p class="hanging2">
(<i>d</i>) Entropy is a universal measure of the spontaneity with
which a state acts when it is free to change.
</p>
<p class="hanging2">
(<i>e</i>) Entropy of a system can only <i>grow</i>.
</p>
<p class="hanging2">
(<i>f</i>) Entropy asserts the essential one-sidedness of Nature.
</p>
<p class="hanging2">
(<i>g</i>) There exists in Nature a magnitude which always changes
in the same sense.
</p>
<p>
(<i>e</i>), (<i>f</i>), and (<i>g</i>) imply <i>change</i> and therefore,
strictly speaking, should not be mentioned here but postponed to a later
section.
</p>

</div>

<div class="chapter">
<h2><a id="SECTION_E">SECTION E</a></h2>

<p class="center">
EQUIVALENTS OF CHANGE OF ENTROPY IN MORE OR LESS
GENERAL PHYSICAL TERMS
</p>

<p>
Here we are really considering the Second Law, for change of entropy is
the kernel of this law, in fact is identical with it. It will be
profitable, however, to view this law in all its many physical aspects.
To be sure, in times past it has been accounted a reproach to the Second
Law that it should be stated in so many different forms,<a id="FNanchor_18_1"></a><a href="#Footnote_18_1" class="fnanchor">[18]</a>
<span class="pagenum" id="Page_41">[Pg 41]</span>
but now that we know precisely that it stands for the growth in the
number of complexions we can more easily trace the connection between
any of these rather vague statements and the present precise definition.
As we have in the main reserved physical interpretations to a later
section we need here only bear in mind certain general principles of
comparison:
</p>
<p>
Any complete summary of the premises necessary for establishing the
inevitable growth of the number of complexions of a system is a valid
statement of the second law.
</p>
<p>
Any general corollary from said growth is a valid statement of the
second law.
</p>
<p>
When instituting any comparison we must keep in mind also the two
principal points of view of regarding any physical problem, namely, the
view of it in the aggregate and that which sees it in its constituent
parts.
</p>
<p>
While we cannot here sharply separate these two points of view, we have
on the whole sought to present first those statements which are based on
experience and next those based on the atomic theory.
</p>
<p class="hanging2">
(1) Growth of entropy is a passage from more to less available energy.
By available is here meant energy which we can direct into any required
channel. With the growth in the number of complexions we can readily see
there is greater inability, on the part of the molecules, for that
concerted and co-operative action which is necessary for the putting
forth of the energy of a system.
</p>
<p class="hanging2">
(2) Growth of entropy is a passage from a concentrated to a distributed
condition of energy. Energy originally concentrated variously in the
system is finally scattered uniformly in said system. In this aggregate
<span class="pagenum" id="Page_42">[Pg 42]</span>
aspect it is a passage from variety to uniformity.
</p>
<p class="hanging2">
(3) Net growth of entropy in all bodies participating in an occurrence
means that the system as a whole has experienced an irreversible change
of state. This change is of course in harmony with the first law of
energy, but this growth gives additional information as it indicates the
<i>direction</i> in which a natural process occurs.
</p>
<p class="hanging2">
(4) Growth of entropy is from less probable to more probable states.
</p>
<p class="hanging2">
Growth of entropy is passage to a state more greatly preferred by nature.
</p>
<p class="hanging2">
Growth of entropy is what obtains whenever a natural process occurs
"spontaneously."
</p>
<p class="hanging2">
Growth of entropy is a passage from
<img style="vertical-align: -2.148ex; width: 13.17ex; height: 5.428ex;" src="images/25.svg" alt=" " data-tex="\left\{\begin{aligned}\text{unsettled} \\
\text{less stable}\end{aligned}\right\}"> to more
<img style="vertical-align: -2.148ex; width: 9.941ex; height: 5.428ex;" src="images/26.svg" alt=" " data-tex="\left\{\begin{aligned}\text{settled} \\
\text{stable}\end{aligned}\right\}"> conditions.
</p>
<p>
All these statements are conspicuously based on the theory of
probabilities.
</p>
<p class="hanging2">
(5) Growth of entropy is a passage from a somewhat regulated to a less
regulated state. It represents, in a certain sense, Nature's escape from
thralldom.
</p>
<p class="hanging2">
Growth of entropy is a passage from a somewhat ordered to a less ordered
molecular arrangement.
</p>
<p class="hanging2">
Growth of entropy is an increase in the disorder of a system of mass
points.
</p>
<p class="hanging2">
Growth of entropy corresponds to an increase in the number of molecular
complexions.
</p>
<p class="hanging2">
(6) Finally we give a mathematical concept which covers the whole domain
of physics: "Any function whose time variation always has the same sign
until a certain state is reached and is then zero, may be called an
entropy function."
</p>

<div class="footnote">

<p class="nind">
<a id="Footnote_18_1"></a><a href="#FNanchor_18_1"><span class="label">[18]</span></a>This need cause no surprise,
for it is only very recently that the conviction is gaining ground that
the Second Law has no independent significance, but that its full
content will only be grasped when its roots are sought in the Theorems
of the Calculus of Probabilities.</p></div>

<p><span class="pagenum" id="Page_43">[Pg 43]</span></p>

</div>

<div class="chapter">
<h2><a id="SECTION_F">SECTION F</a></h2>

<p class="center">
MORE PRECISE AND SPECIFIC STATEMENTS OF THE SECOND LAW
</p>

<p>
We have here classified these statements in the same way as that
followed in the preceding section, when grouping the <i>general</i>
equivalents of the <i>Second Law</i> under the head of <i>change of
entropy</i>. In making comparisons we must, here as there, bear in mind
the following three helpful propositions:
</p>
<p>
(<i>a</i>) The summary of <i>all</i> the necessary prerequisites (or
conditions) for determining entropy may be regarded as a complete and
valid statement of the second law.
</p>
<p>
(<i>b</i>) Any general consequence of any one correct statement of the
second law may be regarded as itself a valid and complete statement of
the second law.
</p>
<p>
(<i>c</i>) All cases of irreversibility stand or fall together; if any
one of them can be completely reversed all can be so reversed.
</p>
<p>
In the preceding section we have already given the most precise physical
statement of the Second Law, namely, when all the participating bodies
of the system are considered, every natural event is marked by an
increase in the number of complexions of the system. We have numbered
the following statements of the second law, for convenience of
reference:
</p>
<p>
(1) J. W. GIBBS. "The impossibility of an uncompensated decrease in
entropy seems to be reduced to an improbability." This of course
considers <i>all</i> the participating bodies of the system.
</p>
<p>
(2) All changes in nature involve a net growth in entropy;
when such a change is measured in reversible ways, the growth
is indicated by the summation: <img style="vertical-align: -1.552ex; width: 9.867ex; height: 4.674ex;" src="images/27.svg" alt=" " data-tex="\int \dfrac{dQ}{T} \geqq 0">, when the
<img style="vertical-align: -2.148ex; width: 9.059ex; height: 5.428ex;" src="images/28.svg" alt=" " data-tex="\left\{\begin{aligned}\text{upper} \\
\text{lower}\end{aligned}\right\}">
sign refers to processes which on the whole are completely
<img style="vertical-align: -2.148ex; width: 14.299ex; height: 5.428ex;" src="images/29.svg" alt=" " data-tex="\left\{\begin{aligned}\text{irreversible} \\
\text{reversible}\end{aligned}\right\}">.
Of course it is now thoroughly understood that the latter case is a purely
<span class="pagenum" id="Page_44">[Pg 44]</span>
ideal one, which is really never realized in nature and is only a
convenient and fruitful fiction in theoretical demonstrations.
</p>
<p>
(3) M. PLANCK. "It is not possible to construct a periodically
functioning motor which effects nothing more than the lifting of a load
and the cooling of a heat reservoir."
</p>
<p>
The proof of this is purely experimental and cumulative and in this
respect is exactly like that for the First Law, the Conservation of
Energy, and has exactly the same sort of validity.
</p>
<p>
(4) Perpetual motion of an isolated system, such as a mechanism with
friction, is impossible and not even approximately realizable.
</p>
<p>
This refers to perpetual motion of the second class, a clear
illustration of which is given on p. 8 of Goodenough's Notes on
Thermodynamics: "A mechanism with friction is inclosed in a case through
which no energy passes. Let the mechanism be started in motion. Because
of friction work is converted into heat which remains in the system,
since no energy passes through the case. Suppose that the heat thus
produced could be completely transformed into work; then this work would
be used again to overcome friction and the heat thus produced would be
again transformed into work. We should then have perpetual motion in a
mechanism with friction without the addition of energy from an external
source." This can be shown to be equivalent but not identical with the
"perpetual motion of the second kind," touched upon in
<a href="#Page_30">p. 30</a>; the latter does confessedly draw on external
energy and furnishes a surplus of power for use, say, in technical service.
</p>
<p>
Nominally, such a machine is a case of perpetual motion, but
not in the usually accepted sense, for it furnishes no <i>surplus</i> of
power; it is the getting of something for nothing, of getting cost-free
power, which has always been the attractive feature of so-called
perpetual motion. Still this machine is as much at variance
with experience as PLANCK'S perpetually working motor of the
<span class="pagenum" id="Page_45">[Pg 45]</span>
second kind. The former may be readily reduced to the latter,
for it is easy to conceive of such legitimate modification of the
former as will make it only a special case of the latter.
</p>
<p>
(5) The following statements are by distinguished physicists and had
better here be considered as confined to events occurring in closed
cycles.
</p>
<p>
CLAUSIUS. It is impossible for a self-acting machine unaided by any
external agency to convey heat from one body to another of higher
temperature.
</p>
<p>
CLERK MAXWELL. It is impossible by the unaided action of natural
processes to transform any part of the heat of a body into mechanical
work, except by allowing heat to pass from that body to another of lower
temperature.
</p>
<p>
THOMSON. It is impossible by means of inanimate material agency to
derive mechanical effect from any portion of matter by cooling it below
the temperature of the coldest of surrounding objects.
</p>
<p>
(6) The efficiency of a perfect engine is independent of the working fluid.
</p>
<p>
(7) Waste of energy once incurred cannot be diminished in the universe,
or in any part of it which neither takes in nor gives out energy.
</p>
<p>
We understand here by waste that residual part of heat of which none
can be elevated back into work.
</p>
<p>
The measure of such waste = <img style="vertical-align: -0.566ex; width: 11.584ex; height: 2.262ex;" src="images/30.svg" alt=" " data-tex="T_{0}(S_{2}-S_{1})">, when <img style="vertical-align: -0.375ex; width: 2.309ex; height: 1.906ex;" src="images/31.svg" alt=" " data-tex="T_{0}"> = lowest
temperature and <img style="vertical-align: -0.339ex; width: 7.515ex; height: 1.934ex;" src="images/32.svg" alt=" " data-tex="S_{2} - S_{1}"> = change of entropy in a process. This
brings out emphatically that the Second Law is not a law of conservation,
it is a law of waste, a law of wasted opportunities for utilizing
technically available energy.
</p>
<p>
(8) The second law and irreversibility do not depend on any special
peculiarity of heat motion, but only on the statistical property of a
system possessing an extraordinary number of degrees of freedom.
</p>
<p>
(9) M. PLANCK. The second law, in its objective physical
<span class="pagenum" id="Page_46">[Pg 46]</span>
form (freed from all anthropomorphism) refers to certain <i>mean</i>
values which are formed from a great number of like "chaotic" elements.
</p>
<p>
(10) When all the participating bodies of the system are considered,
every natural event is marked by an increase in the number of
complexions of the system. We repeat, this is the most precise physical
statement of the second law and covers the whole domain of science.
</p>
<p>
We will not comment further on these statements at this time, leaving
such discussion of their relations to the section on physical
interpretations.
<span class="pagenum" id="Page_47">[Pg 47]</span>
</p>

</div>

<div class="chapter">
<h2><a id="PART_II"></a>PART II
<br><br>
ANALYTICAL EXPRESSIONS FOR A FEW PRIMARY RELATIONS
</h2>

<p>
At the beginning of this presentation we disclaimed any purpose of
giving a rigorous proof for any of the many formulas with which this
subject bristles. We propose only to give in some cases an outline of
the main steps of the demonstration and merely for the purpose of
getting a clearer physical insight into certain states and relations.
Pre-eminent in importance is the state of thermal equilibrium (see pp.
<a href="#Page_19">19</a>, <a href="#Page_52">52</a>,
<a href="#Page_53">53</a>) and we will therefore consider first its main
characteristic:
</p>

</div>

<div class="chapter">
<h2><a id="SECTION_A2"></a>SECTION A
<br><br>
MAXWELL'S LAW OF DISTRIBUTION OF MOLECULAR VELOCITIES</h2>

<p>
Without giving a full proof of the law we will give the main steps which
lead to its analytical statement, in so doing following the presentation
given by HANS LORENZ on pp. 526-529 of his "Technische Wärmelehre," and
will then point out its main features and consequences.
</p>
<p>
We suppose the gas to contain in a unit of volume <img style="vertical-align: -0.025ex; width: 1.357ex; height: 1.025ex;" src="images/33.svg" alt=" " data-tex="n"> molecules each
possessing a different velocity and direction. Let there be a system of
three co-ordinate axes, <img style="vertical-align: -0.489ex; width: 5.949ex; height: 2.081ex;" src="images/34.svg" alt=" " data-tex="\xi,\, \eta,\, \zeta">. A fraction <img style="vertical-align: -0.566ex; width: 6.163ex; height: 2.262ex;" src="images/35.svg" alt=" " data-tex="f(\xi) d\xi">
of the total number of molecules will possess a velocity in the <img style="vertical-align: -0.464ex; width: 0.991ex; height: 2.057ex;" src="images/36.svg" alt=" " data-tex="\xi">
direction, whose values lie between <img style="vertical-align: -0.464ex; width: 11.318ex; height: 2.057ex;" src="images/37.svg" alt=" " data-tex="\xi\, \text{and}\, \xi + d\xi">. The
number of molecules which at the same time possess velocities in the
<img style="vertical-align: -0.489ex; width: 1.124ex; height: 1.489ex;" src="images/38.svg" alt=" " data-tex="\eta"> direction, lying between <img style="vertical-align: -0.489ex; width: 11.718ex; height: 2.059ex;" src="images/39.svg" alt=" " data-tex="\eta\, \text{and}\, \eta + d\eta">,
will be <img style="vertical-align: -0.566ex; width: 13.95ex; height: 2.262ex;" src="images/40.svg" alt=" " data-tex="n f(\xi) d\xi f(\eta) d\eta">, since no preference can be given
to either the <img style="vertical-align: -0.464ex; width: 0.991ex; height: 2.057ex;" src="images/36.svg" alt=" " data-tex="\xi"> or <img style="vertical-align: -0.489ex; width: 1.124ex; height: 1.489ex;" src="images/38.svg" alt=" " data-tex="\eta"> direction. Similarly and finally the
<span class="pagenum" id="Page_48">[Pg 48]</span>
number of molecules whose velocity co-ordinates concurrently lie between
<img style="vertical-align: -0.489ex; width: 5.949ex; height: 2.081ex;" src="images/34.svg" alt=" " data-tex="\xi,\, \eta,\, \zeta">, and <img style="vertical-align: -0.489ex; width: 20.956ex; height: 2.081ex;" src="images/41.svg" alt=" " data-tex="\xi + d\xi,\, \eta + d\eta,\, \zeta + d\zeta">
will be represented by the product
<span class="align-center"><img style="vertical-align: -0.566ex; width: 20.891ex; height: 2.262ex;" src="images/42.svg" alt=" " data-tex="
n f(\xi) d\xi f(\eta) d\eta f(\zeta) d\zeta,
"></span>
where the only thing known about function <img style="vertical-align: -0.464ex; width: 1.244ex; height: 2.059ex;" src="images/43.svg" alt=" " data-tex="f"> is that the sum of
the fractions <img style="vertical-align: -0.566ex; width: 6.163ex; height: 2.262ex;" src="images/44.svg" alt=" " data-tex="f(\xi)d\xi"> extended over all the values of <img style="vertical-align: -0.464ex; width: 0.991ex; height: 2.057ex;" src="images/36.svg" alt=" " data-tex="\xi">
must = unity, so that
<span class="align-center"><img style="vertical-align: -2.159ex; width: 53.61ex; height: 5.553ex;" src="images/45.svg" alt=" " data-tex="
\int_{-\infty}^{+\infty} f(\xi) d\xi
= \int_{-\infty}^{+\infty} f(\eta) d\eta
= \int_{-\infty}^{+\infty} f(\zeta) d\zeta = 1
\qquad\text{(1)}
"></span>
</p>
<p>
Now if we suppose all of the velocities of the <img style="vertical-align: -0.025ex; width: 1.357ex; height: 1.025ex;" src="images/33.svg" alt=" " data-tex="n"> molecules
to be laid off as vectors from a pole <img style="vertical-align: -0.05ex; width: 1.726ex; height: 1.643ex;" src="images/46.svg" alt=" " data-tex="O">, the three directions
<img style="vertical-align: -0.489ex; width: 5.949ex; height: 2.081ex;" src="images/34.svg" alt=" " data-tex="\xi,\, \eta,\, \zeta"> will constitute about <img style="vertical-align: -0.05ex; width: 1.726ex; height: 1.643ex;" src="images/46.svg" alt=" " data-tex="O"> a perfectly arbitrary
system of co-ordinates in which <img style="vertical-align: -0.566ex; width: 18.931ex; height: 2.262ex;" src="images/47.svg" alt=" " data-tex="(d\xi)\, (d\eta)\; (d\zeta) = dV">
designates a volume element<a id="FNanchor_19_1"></a><a href="#Footnote_19_1" class="fnanchor">[19]</a>
and the velocity <img style="vertical-align: -0.489ex; width: 1.17ex; height: 1.489ex;" src="images/48.svg" alt=" " data-tex="\rho"> of a molecule is given by
<span class="align-center"><img style="vertical-align: -0.566ex; width: 24.383ex; height: 2.565ex;" src="images/49.svg" alt=" " data-tex="
\rho^2 = \xi^2 + \eta^2 + \zeta^2
\qquad\text{(2)}
"></span>
<span class="pagenum" id="Page_49">[Pg 49]</span>
</p>
<p>
Now if we put through the origin <img style="vertical-align: -0.05ex; width: 1.726ex; height: 1.643ex;" src="images/46.svg" alt=" " data-tex="O"> another system of co-ordinates of
which one axis coincides with any arbitrarily chosen velocity <img style="vertical-align: -0.489ex; width: 1.17ex; height: 1.489ex;" src="images/48.svg" alt=" " data-tex="\rho">,
then in this axis the above-found product will be
<img style="vertical-align: -0.566ex; width: 19.367ex; height: 2.515ex;" src="images/50.svg" alt=" " data-tex="n^{f}(\rho)f(O) \cdot f(O)dV"> because the two other co-ordinates
(outside of <img style="vertical-align: -0.489ex; width: 1.17ex; height: 1.489ex;" src="images/48.svg" alt=" " data-tex="\rho">) of the volume element <img style="vertical-align: -0.05ex; width: 2.916ex; height: 1.62ex;" src="images/51.svg" alt=" " data-tex="dV"> will equal zero and no
preference can be given to any direction. Then it can be shown that the
form of the function is given by


<span class="align-center"><img style="vertical-align: -0.566ex; width: 22.488ex; height: 3.592ex;" src="images/52.svg" alt=" " data-tex="
f(\xi)^{2} = Ce^{-\frac{\xi^{2}}{c^{2}}},
\qquad\text{(3)}
"></span>
where <img style="vertical-align: -0.05ex; width: 1.719ex; height: 1.645ex;" src="images/53.svg" alt=" " data-tex="C">, <img style="vertical-align: -0.025ex; width: 0.98ex; height: 1.025ex;" src="images/54.svg" alt=" " data-tex="c"> are integration constants which stand in a certain
relation to each other, namely,
<span class="align-center"><img style="vertical-align: -2.308ex; width: 18.353ex; height: 5.344ex;" src="images/55.svg" alt=" " data-tex="
C = \frac{1}{c{\sqrt \pi}},
\qquad\text{(4)}
"></span>
</p>
<p>
Further mathematical manipulation eliminates the different velocity
directions and gives
<span class="align-center"><img style="vertical-align: -2.308ex; width: 50.581ex; height: 5.369ex;" src="images/56.svg" alt=" " data-tex="
dn = \frac{4n}{c^{3}\sqrt \pi} e^{-(\frac{\rho}{c})^{2}} \rho^{2}d\rho
= 4nC(\frac{\rho}{c})^{2} e^{-(\frac{\rho}{c})^{2}} d\rho,
\qquad\text{(5)}
"></span>
for the number of molecules possessing absolute velocities between
<img style="vertical-align: -0.489ex; width: 1.17ex; height: 1.489ex;" src="images/48.svg" alt=" " data-tex="\rho"> and <img style="vertical-align: -0.489ex; width: 6.282ex; height: 2.059ex;" src="images/57.svg" alt=" " data-tex="\rho + d\rho">.
</p>
<p>
This expression (5) is called MAXWELL'S Law of Distribution;
it is identical with that found for the probable distribution of
error in a great number of observations and is graphically shown
by the following figure, with maximum number of molecules for
velocity <img style="vertical-align: -0.025ex; width: 0.98ex; height: 1.025ex;" src="images/54.svg" alt=" " data-tex="c">. The constant <img style="vertical-align: -0.025ex; width: 0.98ex; height: 1.025ex;" src="images/54.svg" alt=" " data-tex="c"> is therefore a velocity from which
<span class="pagenum" id="Page_50">[Pg 50]</span>
most of the molecules differ but little. The development shows
that this self-same distribution exists for every straight line that
can be drawn in the volume under consideration.
</p>

<div class="figcenter" style="width: 400px;">
<img src="images/figure01.jpg" width="400" alt="fig01">
</div>

<p>
BOLTZMANN, in his Gas Theorie, has shown that for such a state the
"number of complexions" is a maximum, that is, the entropy is then a
maximum.
</p>
<p>
From the preceding expression (5) follows that the kinetic energy
<img style="vertical-align: -0.05ex; width: 1.735ex; height: 1.595ex;" src="images/58.svg" alt=" " data-tex="U"> of the system is
<span class="align-center"><img style="vertical-align: -2.308ex; width: 49.581ex; height: 5.801ex;" src="images/59.svg" alt=" " data-tex="
U = \frac{2nm }{c^{3} \sqrt{\pi}} \int_{-\infty}^{+\infty} e^{-(\frac{\rho}{c})^{2}} \rho^{4}d\rho
= n \left(\frac{m[w^{2}]}{2}\right)
\qquad\text{(6)}
"></span>
where <img style="vertical-align: -0.566ex; width: 3.866ex; height: 2.452ex;" src="images/60.svg" alt=" " data-tex="[w^{2}]"> is the <i>mean square of the velocity</i>. Integration
gives
<span class="align-center"><img style="vertical-align: -0.781ex; width: 19.068ex; height: 2.78ex;" src="images/61.svg" alt=" " data-tex="
[w^{2}] = \tfrac{3}{2} c^{2}.
\qquad\text{(7)}
"></span>
</p>
<p>
It is known that the <i>measure of temperature</i> is the mean kinetic
energy of the individual molecule and not simply the mean square
of its velocity, and we possess here therefore a perfectly precise
<span class="pagenum" id="Page_51">[Pg 51]</span>
definition of temperature. We see also from (7) that temperature
in a particular gas is directly proportional to either
<img style="vertical-align: -0.025ex; width: 1.967ex; height: 1.912ex;" src="images/62.svg" alt=" " data-tex="c^{2}"> or <img style="vertical-align: -0.566ex; width: 3.866ex; height: 2.452ex;" src="images/60.svg" alt=" " data-tex="[w^{2}]">.
</p>
<p>
MAXWELL further shows without any assumption as to the nature of the
molecules, or the forces acting between them, that the derived law of
distribution is valid for any gas mixture, but that is it modified when
the gas is exposed to the action of external forces.
</p>
<p>
BOLTZMANN found (Wien. Akad. Sitzber. LXXII B, 1875, p. 443) for
monatomic gases that in spite of the effect of external forces
(<i>a</i>) the velocity of any molecule is equally likely to have any
direction whatever, (<i>b</i>) the velocity distribution in any element
of space is exactly like that in a gas of equal density and temperature
upon which no external forces act, the effect of the external forces
consisting only in varying the density from place to place as in
hydrodynamics.
</p>
<p>
BOLTZMANN says this "normal" state is permanent (stationary) for given
external conditions because magnitude <img style="vertical-align: 0; width: 2.009ex; height: 1.545ex;" src="images/11.svg" alt=" " data-tex="H"> does not vary; such a normal
state has many configurations, but all agree in having same number of
complexions.
</p>
<p>
Also, "MAXWELL'S Velocity Distribution is not a state which assigns to
each molecule a particular place (locus) and a particular velocity,
which are reached say by the locus and velocity of each molecule
asymptotically approaching said assigned locus and velocity. With a
<i>finite</i> number of molecules MAXWELL'S state will never be exactly
but only approximately realized. MAXWELL'S velocity is not a singular
one which is confronted by an immense number of non-Maxwellian velocity
distributions. On the contrary, among the immense number of possible
velocity distributions by far the greater number possess the
characteristics of the MAXWELL velocity distribution."
</p>
<p>
MAX PLANCK (Festschrift, p. 113) lucidly dwells on thermal equilibrium,
entropy and temperature, as follows:
</p>
<p>
"The mechanical significance of the temperature idea is
most closely connected with the mechanical significance of entropy,
<span class="pagenum" id="Page_52">[Pg 52]</span>
for the two are connected by <img style="vertical-align: -0.439ex; width: 10.212ex; height: 2.034ex;" src="images/63.svg" alt=" " data-tex="T dS = dQ">. By answering one of
these questions we at the same time settle the other."
</p>
<p>
In the earlier days interest was naturally centered in the directly
measurable magnitude temperature and entropy appeared as a more
complicated idea which was to be derived from the former. Nowadays this
relation is rather reversed and the prime question is to first explain
entropy mechanically and this will then define temperature. The reason
for this change of attitude is that in all such explanatory efforts to
present Thermodynamics mechanically and give temperature a
<i>complete</i> mechanical definition it is necessary to come back to
the peculiarities of "thermal equilibrium." But the full significance of
this equilibrium conception is only to be reached from the standpoint of
irreversibility. For thermal equilibrium can only be defined as the
final state toward which all irreversible processes strive. In this way
the question as to temperature leads necessarily to the nature of
irreversibility and this in turn is solely founded on the existence of
the entropy function. This magnitude is therefore the primary, general
conception which is significant for all kinds of states and changes of
state, while temperature emerges from this with the help of the special
condition of thermal equilibrium, in which condition the entropy attains
its maximum.
</p>

<div class="footnote">

<p class="nind">
<a id="Footnote_19_1"></a><a href="#FNanchor_19_1"><span class="label">[19]</span></a>In MAXWELL'S distribution
the molecules are assumed to be uniformly scattered throughout the unit
volume; it is the velocities only that are variously distributed in the
different elementary regions. To realize the haphazard character
(necessary in Calculus of Probabilities) of the motions of the
molecules, we must bear in mind that each of the molecules in the unit
volume has a different velocity and direction; here no direction has
preference over another, i.e., one direction of a molecule is as likely
as another. Here at first we write expression for the number of
molecules whose velocities parallel to the co-ordinate axes are
respectively confined between the velocity limits:
<span class="align-center"><img style="vertical-align: -3.507ex; width: 16.116ex; height: 8.145ex;" src="images/64.svg" alt=" " data-tex="
\begin{aligned}
\xi\quad \text{and}\quad \xi + d\xi,\\
\eta\quad \text{and}\quad \eta + d\eta,\\
\zeta\quad \text{and}\quad \zeta + d\zeta.
\end{aligned}
"></span>
To find the number of molecules thus limited the procedure given above
is essentially as follows: Expressed as a <i>fraction</i>
<img style="vertical-align: -0.566ex; width: 6.163ex; height: 2.262ex;" src="images/44.svg" alt=" " data-tex="f(\xi)d\xi"> = probability of velocities parallel to
<img style="vertical-align: -0.464ex; width: 0.991ex; height: 2.057ex;" src="images/36.svg" alt=" " data-tex="\xi"> axis having values between <img style="vertical-align: -0.464ex; width: 11.318ex; height: 2.057ex;" src="images/37.svg" alt=" " data-tex="\xi\, \text{and}\, \xi + d\xi"> and
expressed as a number <img style="vertical-align: -0.566ex; width: 7.52ex; height: 2.262ex;" src="images/65.svg" alt=" " data-tex="nf(\xi)d\xi"> = number of molecules having such
velocities between the assigned limits; similarly,
<img style="vertical-align: -0.566ex; width: 6.43ex; height: 2.262ex;" src="images/66.svg" alt=" " data-tex="f(\eta)d\eta"> = probability of velocities parallel to <img style="vertical-align: -0.489ex; width: 1.124ex; height: 1.489ex;" src="images/38.svg" alt=" " data-tex="\eta">
axis having velocities between <img style="vertical-align: -0.489ex; width: 11.718ex; height: 2.059ex;" src="images/67.svg" alt=" " data-tex="\eta\, \text{and}\, \eta+d\eta">. As
these are two independent sets of velocities, the probability of their
concurrence is the product <img style="vertical-align: -0.566ex; width: 14.227ex; height: 2.262ex;" src="images/68.svg" alt=" " data-tex="f(\xi)d\xi \cdot f(\eta)d\eta"> and the number
of molecules thus concurring is equal to <img style="vertical-align: -0.566ex; width: 15.451ex; height: 2.262ex;" src="images/69.svg" alt=" " data-tex="nf(\xi)d\xi \cdot f(\eta)d\xi">.
Similarly, the number of molecules concurrently possessing velocities
parallel to each of the three axes is
<span class="align-center"><img style="vertical-align: -0.566ex; width: 24.16ex; height: 2.262ex;" src="images/70.svg" alt=" " data-tex="
nf(\xi)d\xi \cdot f(\eta)d\eta \cdot f(\zeta)d\zeta.
"></span>
The problem is simpler in this Maxwellian case than in the more general
case of any state of the body in which there is an unequal distribution in
space of the molecules.</p></div>

</div>

<div class="chapter">
<h2><a id="SECTION_B2"></a>SECTION B
<br><br>
SIMPLE ANALYTICAL EXPRESSION FOR DEPENDENCE OF ENTROPY
ON PROBABILITY</h2>

<p>
Here also we will dispense with a full proof and content ourselves with
the main steps which lead to the desired expression. We will follow
PLANCK'S elegant presentation on pp. 136-148 of his Wärmestrahlung. On
<a href="#Page_22">p. 22</a> we have dwelt on the usefulness and the
necessity for the probability idea in general physics and in this
particular case. We can start, therefore, with PLANCK'S theorem:
<span class="pagenum" id="Page_53">[Pg 53]</span>
</p>
<p>
"The entropy of a physical system in a particular state depends solely
on the probability of this state."
</p>
<p>
No rigorous proof is here attempted, nor any numerical computations; for
present purposes it will suffice to fix in a general way the kind of
dependence of entropy on probability.
</p>
<p>
Let <img style="vertical-align: -0.05ex; width: 1.459ex; height: 1.645ex;" src="images/2.svg" alt=" " data-tex="S"> designate the entropy and <img style="vertical-align: -0.05ex; width: 2.371ex; height: 1.595ex;" src="images/17.svg" alt=" " data-tex="W"> the probability of a physical
system in a particular state, then the above theorem enunciates that
<span class="align-center"><img style="vertical-align: -0.566ex; width: 18.274ex; height: 2.262ex;" src="images/71.svg" alt=" " data-tex="
S = f(W),
\qquad\text{(8)}
"></span>
where <img style="vertical-align: -0.566ex; width: 5.376ex; height: 2.262ex;" src="images/72.svg" alt=" " data-tex="f(W)"> signifies a universal function of the argument <img style="vertical-align: -0.05ex; width: 2.371ex; height: 1.595ex;" src="images/17.svg" alt=" " data-tex="W">.
Now, however <img style="vertical-align: -0.05ex; width: 2.371ex; height: 1.595ex;" src="images/17.svg" alt=" " data-tex="W"> may be defined we can certainly infer from the
Calculus of Probabilities that the probability of a system, composed of
two entirely independent systems, is equal to the product of the
separate probabilities of the individual systems. For example, if we
take for the first system any terrestrial body whatever and for the
second system any hollow space on Sirius, which is traversed by
radiations, then the probability <img style="vertical-align: -0.05ex; width: 2.371ex; height: 1.595ex;" src="images/17.svg" alt=" " data-tex="W">, that simultaneously the
terrestrial body will be in a particular state 1 and said radiation in a
particular state 2, will be given by
<span class="align-center"><img style="vertical-align: -0.566ex; width: 20.057ex; height: 2.262ex;" src="images/73.svg" alt=" " data-tex="
W = W_{1}W_{2},
\qquad\text{(9)}
"></span>
where <img style="vertical-align: -0.339ex; width: 3.123ex; height: 1.885ex;" src="images/74.svg" alt=" " data-tex="W_{1}">, <img style="vertical-align: -0.339ex; width: 3.123ex; height: 1.885ex;" src="images/75.svg" alt=" " data-tex="W_{2}"> respectively represent the separate
probabilities of said two states. Now let <img style="vertical-align: -0.05ex; width: 2.59ex; height: 1.645ex;" src="images/76.svg" alt=" " data-tex="S{1}">, <img style="vertical-align: -0.339ex; width: 2.375ex; height: 1.934ex;" src="images/77.svg" alt=" " data-tex="S_{2}">
respectively represent the entropies of the separate systems
corresponding to said states 1 and 2, then according to Eq. 8, we have
<span class="align-center"><img style="vertical-align: -0.566ex; width: 26.937ex; height: 2.262ex;" src="images/78.svg" alt=" " data-tex="
S_{1} = f(W_{1}),\quad S_{2} = f(W_{2}).
"></span>
But, according to the Second Law of Thermodynamics, the total entropy
of two independent systems is <img style="vertical-align: -0.339ex; width: 11.991ex; height: 1.934ex;" src="images/79.svg" alt=" " data-tex="S = S_{1} + S_{2}">, and consequently
according to (8) and (9),
<span class="align-center"><img style="vertical-align: -0.566ex; width: 27.919ex; height: 2.262ex;" src="images/80.svg" alt=" " data-tex="
f(W_{1}W_{2}) = f(W_{1}) + f(W_{2}).
"></span>
<span class="pagenum" id="Page_54">[Pg 54]</span>
</p>
<p>
From this functional equation <img style="vertical-align: -0.464ex; width: 1.244ex; height: 2.059ex;" src="images/43.svg" alt=" " data-tex="f"> may be determined. After successive
differentiation there is obtained a differential equation of the second
order and its general integral is
<span class="align-center"><img style="vertical-align: -2.036ex; width: 38.174ex; height: 5.204ex;" src="images/81.svg" alt=" " data-tex="
\begin{aligned}
f(W) = k\, log W &+ \text{constant}\\
\text{or} \qquad  S = k\, log W &+ \text{constant},
\end{aligned}
\qquad\text{(10)}
"></span>
which determines the general dependence of entropy on probability. The
universal integration constant <img style="vertical-align: -0.025ex; width: 1.179ex; height: 1.595ex;" src="images/3.svg" alt=" " data-tex="k"> is the same for a terrestrial
system as for a cosmical system, and when its numerical value is known
for either system it will be known for the other; indeed, this constant
<img style="vertical-align: -0.025ex; width: 1.179ex; height: 1.595ex;" src="images/3.svg" alt=" " data-tex="k"> is the same for physically unlike systems, as above, where
concurrence between a molecular and a radiating system was assumed. The
last, additive, constant has no physical significance because entropy
has an arbitrary additive constant and therefore this constant in (10)
may be omitted at pleasure.
</p>
<p>
Relation (10) contains a general method of computing the entropy <img style="vertical-align: -0.05ex; width: 1.459ex; height: 1.645ex;" src="images/2.svg" alt=" " data-tex="S">
from probability considerations. But the relation becomes of practical
value only when the magnitude <img style="vertical-align: -0.05ex; width: 2.371ex; height: 1.595ex;" src="images/17.svg" alt=" " data-tex="W"> of the probability of a system for a
certain state can be given numerically. The most general and precise
definition of this magnitude is an important physical problem and first
of all demands closer insight into the details of what constitutes the
"state" of a physical system. [This has been adequately done in the
earlier part of this presentation. Later on pp. <a href="#Page_27">27</a>,
<a href="#Page_28">28</a>, permutation considerations led us to define
the probability W of a state as the number of complexions included in the
given state.]
<span class="pagenum" id="Page_55">[Pg 55]</span>
</p>

</div>

<div class="chapter">
<h2><a id="SECTION_C2"></a>SECTION C
<br><br>
DETERMINATION OF A PRECISE, NUMERICAL, EXPRESSION FOR THE
ENTROPY OF ANY PHYSICAL CONFIGURATION</h2>

<p>
PLANCK modestly says that everything essential in the determination of
this expression has been done by L. BOLTZMANN, in his wide range of
physical investigation. PLANCK'S discussion, however, is so compact and
lucid that it is best suited for our purpose. Keeping this purpose in
mind we will here also abbreviate by dispensing with parts of PLANCK'S
fuller proof and content ourselves with the main steps which lead to the
desired expression. These main steps are as follows:
</p>
<p class="hanging2">
(<i>a</i>) Determination of the general expression for the
<img style="vertical-align: -2.148ex; width: 24.57ex; height: 5.428ex;" src="images/82.svg" alt=" " data-tex="\left\{\begin{aligned} &\text{Probability or} \\ &\text{No. of complexions}\, W \end{aligned} \right\}">
of a given physical configuration of a known macroscopic state;
</p>
<p class="hanging2">
(<i>b</i>) Determination of the general expression for the Entropy <img style="vertical-align: -0.05ex; width: 1.459ex; height: 1.645ex;" src="images/2.svg" alt=" " data-tex="S">
of a given physical configuration of a known macroscopic state;
</p>
<p class="hanging2">
(<i>c</i>) Special case of (<i>b</i>) namely, expression for the Entropy
<img style="vertical-align: -0.05ex; width: 1.459ex; height: 1.645ex;" src="images/2.svg" alt=" " data-tex="S"> of the state of thermal equilibrium of a monatomic gas;
</p>
<p class="hanging2">
(<i>d</i>) Confirmation, by equating this value of <img style="vertical-align: -0.05ex; width: 1.459ex; height: 1.645ex;" src="images/2.svg" alt=" " data-tex="S"> with that found
thermodynamically and then deriving well-known results.
</p>
<p class="hanging2">
(<i>e</i>) PLANCK'S conversion of the expressions of (<i>b</i>) and
(<i>c</i>) into more precise ones by finding numerical values of <img style="vertical-align: -0.025ex; width: 1.179ex; height: 1.595ex;" src="images/3.svg" alt=" " data-tex="k">
in C. G. S. units; in F. P. S. units;
</p>
<p class="hanging2">
(<i>f</i>) Determination of the dimensions of the universal constant <img style="vertical-align: -0.025ex; width: 1.179ex; height: 1.595ex;" src="images/3.svg" alt=" " data-tex="k">
and therefore also of entropy in general.
<span class="pagenum" id="Page_56">[Pg 56]</span>
</p>

<p><br><br></p>

<p class="center">
<i>Step a</i>
<br>
<i>Determination of the Number of Complexions of a given Physical
Configuration of a Known Macrostate</i></p>

<p>
We will, for simplicity's sake, consider here an ideal gas in a given
macro-state and consisting of N-like, monatomic, molecules. By
generalizing the meaning of our co-ordinates, the following presentation
could be made equally applicable to the more general case of Physics
contemplated under this heading.
</p>
<p>
Of course we must here have clearly in mind what is meant by the state
of a gas. For this we may refer to <a href="#Page_10">p. 10</a> (lines
13 to 24) and to <a href="#Page_19">p. 19</a> (lines 8 to 24). The
conditions there imposed are all fulfilled if we suppose the state given
in such a way that we know: (1) The number of molecules in any
macroscopically small space (volume element); and (2) the number of
molecules which lie in a certain macroscopically small velocity region
(soon to be more fully described). To have the Calculus of Probabilities
applicable, each of the tiny regions contemplated under (1) and (2) must
still contain a large number of molecules and their motions must besides
have all the features of haphazard detailed on pp. <a href="#Page_25">25</a>,
<a href="#Page_26">26</a>; all this is necessary in order that the
contemplated motions may possess all the
characteristics of "elementary chaos."
</p>
<p>
Before proceeding further on our main line, we will define more fully
what is meant by the two elementary regions in which lie respectively
the molecules and their velocity ends. After this has been done we will,
for convenience, combine these two regions into a fictitious elementary
region, say, a six-dimensional one.
</p>
<p>
First there is the volume element <img style="vertical-align: -0.464ex; width: 16.187ex; height: 2.034ex;" src="images/83.svg" alt=" " data-tex="dx \cdot dy \cdot dz = dV">, in which
any molecule having co-ordinates lying between
<img style="vertical-align: -3.507ex; width: 16.25ex; height: 8.145ex;" src="images/84.svg" alt=" " data-tex="\left\{\begin{aligned} x\, \text{and}\, x+dx \\
y\, \text{and}\, y+dy \\ z\, \text{and}\, z+dz \end{aligned}\right\}">
is located; this element can be conceived as a parallelopipedon
<span class="pagenum" id="Page_57">[Pg 57]</span>
whose edges are parallel to the co-ordinate axes; this is the simplest
of the elementary regions here to be considered. To conceive of the
elementary region <img style="vertical-align: -0.489ex; width: 9.979ex; height: 2.081ex;" src="images/85.svg" alt=" " data-tex="d\xi \cdot d\eta \cdot d\zeta"> containing the velocity
ends of the molecules, let us suppose any origin <img style="vertical-align: -0.05ex; width: 1.726ex; height: 1.643ex;" src="images/46.svg" alt=" " data-tex="O"> for velocities in
a unit volume and from this as a pole lay off as vectors the molecular
velocities lying between the limits,
<span class="align-center"><img style="vertical-align: -3.507ex; width: 24.664ex; height: 8.145ex;" src="images/86.svg" alt=" " data-tex="
\begin{aligned}
\xi\quad \text{and}\quad \xi + d\xi,\\
\eta\quad \text{and}\quad \eta + d\eta, \\
\zeta\quad \text{and}\quad \zeta + d\zeta,
\end{aligned}
\qquad\text{(11)}
"></span>
where <img style="vertical-align: -0.489ex; width: 5.193ex; height: 2.081ex;" src="images/87.svg" alt=" " data-tex="\xi, \eta, \zeta"> are the components of said velocities parallel
to the respective co-ordinate axes. Then, under the velocity limitations
imposed, the end of the velocity of each such molecule will lie in the
elementary parallelopiped <img style="vertical-align: -0.489ex; width: 9.979ex; height: 2.081ex;" src="images/85.svg" alt=" " data-tex="d\xi \cdot d\eta \cdot d\zeta">, one vertex of
this parallelopiped having of course the co-ordinates <img style="vertical-align: -0.489ex; width: 5.193ex; height: 2.081ex;" src="images/87.svg" alt=" " data-tex="\xi, \eta, \zeta">.
This parallelopiped can be regarded as a constructed volume within which
the velocity end must lie. We have therefore here two elementary volumes
<img style="vertical-align: -0.464ex; width: 10.253ex; height: 2.034ex;" src="images/88.svg" alt=" " data-tex="dx \cdot dy \cdot dz"> and <img style="vertical-align: -0.489ex; width: 9.979ex; height: 2.081ex;" src="images/85.svg" alt=" " data-tex="d\xi \cdot d\eta \cdot d\zeta">, which are
<i>independent</i> of each other. Now remembering that the probability
of any properly endowed molecule being found in one of these volumes is
in each case equal to the number of molecules belonging or corresponding
to the volume considered. Assuming, for the moment, an equal
distribution of molecules and velocities throughout the whole volume, we
may say that the number of molecules occurring, in each of the said
elementary volumes, is proportional to their respective sizes; this is
here equivalent to saying that the probability of any molecule thus
occurring in said elementary volumes is proportional to their respective
sizes. Having stated the probability of each contemplated occurrence, we
can now say the probability of these two events concurring is equal to
the product of the probabilities of said two separate occurrences.
Moreover, as the probability of each occurrence is proportional to the
<span class="pagenum" id="Page_58">[Pg 58]</span>
size of its own elementary volume, the product of said probabilities
will likewise be proportional to the product
<span class="align-center"><img style="vertical-align: -0.566ex; width: 34.723ex; height: 2.262ex;" src="images/89.svg" alt=" " data-tex="
dx \cdot dy \cdot dz \cdot d\xi \cdot d\eta \cdot d\zeta = \sigma
\qquad\text{(12)}
"></span>
of the two elementary volumes. Here <img style="vertical-align: -0.025ex; width: 1.292ex; height: 1ex;" src="images/90.svg" alt=" " data-tex="\sigma"> can be regarded as a sort
of fictitious volume or region, constructed, say, in a six-dimensional
space.<a id="FNanchor_20_1"></a><a href="#Footnote_20_1" class="fnanchor">[20]</a>
</p>
<p>
The extent of such an elementary region is very minute in comparison
with the total space under consideration, but still it must be conceived
as sufficiently large to embrace many molecules, otherwise its state
would not be one of "elementary chaos." On account of the equivalence
here of probability and number of concurring molecules, we may for the
present say that the number of the latter is proportional to the
magnitude of this elementary region <img style="vertical-align: -0.025ex; width: 1.292ex; height: 1ex;" src="images/90.svg" alt=" " data-tex="\sigma">. But before we proceed
further this last statement must be subjected to a correction, for we
temporarily assumed above that there was an equal distribution of
molecules and velocities throughout the whole volume. Now at the start,
in defining the contemplated state, it was distinctly announced that
there was an unequal distribution of such elementary conditions, the law
of their distribution being given by the known number of molecules in
each elementary volume <img style="vertical-align: -0.05ex; width: 2.916ex; height: 1.62ex;" src="images/51.svg" alt=" " data-tex="dV"> and in each constructed elementary
velocity volume <img style="vertical-align: -0.489ex; width: 9.979ex; height: 2.081ex;" src="images/85.svg" alt=" " data-tex="d\xi \cdot d\eta \cdot d\zeta">. This correction is
effected by the introduction of a finite proportionality factor,
<span class="align-center"><img style="vertical-align: -0.566ex; width: 24.224ex; height: 2.262ex;" src="images/91.svg" alt=" " data-tex="
f(x,y,z,\xi,\eta,\zeta),
\qquad\text{(13)}
"></span>
which can be any given function of the location and velocity
co-ordinates, so long as it fulfills the one condition (put in
abbreviated form),
<span class="align-center"><img style="vertical-align: -0.566ex; width: 20.384ex; height: 2.262ex;" src="images/92.svg" alt=" " data-tex="
\Sigma f \cdot \sigma = N,
\qquad\text{(14)}
"></span>
where <img style="vertical-align: -0.466ex; width: 43.709ex; height: 2.061ex;" src="images/93.svg" alt=" " data-tex="N = \text{the total number of molecules of the gas}">.
</p>
<span class="pagenum" id="Page_59">[Pg 59]</span>
<p>
Strictly speaking, the expression <img style="vertical-align: -0.025ex; width: 1.292ex; height: 1ex;" src="images/90.svg" alt=" " data-tex="\sigma"> for the fictitious
elementary region <img style="vertical-align: -0.025ex; width: 1.292ex; height: 1ex;" src="images/90.svg" alt=" " data-tex="\sigma">, formed by the product of <img style="vertical-align: -0.05ex; width: 2.916ex; height: 1.62ex;" src="images/51.svg" alt=" " data-tex="dV"> and the
constructed-volume element <img style="vertical-align: -0.489ex; width: 9.979ex; height: 2.081ex;" src="images/85.svg" alt=" " data-tex="d\xi \cdot d\eta \cdot d\zeta">, should be
replaced by the expression <img style="vertical-align: -0.025ex; width: 4.266ex; height: 1.91ex;" src="images/94.svg" alt=" " data-tex="m^{3}\sigma">, where <img style="vertical-align: -0.025ex; width: 1.986ex; height: 1.025ex;" src="images/4.svg" alt=" " data-tex="m"> is the mass of a
molecule. The reason for this substitution is found in the fact
that the magnitude of the constructed-volume element
<img style="vertical-align: -0.489ex; width: 9.979ex; height: 2.081ex;" src="images/85.svg" alt=" " data-tex="d\xi \cdot d\eta \cdot d\zeta"> varies with <i>time</i> due
to the variation of velocities effected by molecular collisions. Now
this variation of magnitude is not permissible with the probability
considerations which here obtain. For the probability of a state which
necessarily follows from another state must be like that of the latter.
As the momenta after collision are the same as before collision, we have
now in the momenta, co-ordinates which do not vary with time like their
constituent velocities. Therefore if we substitute in (12) for the
velocities <img style="vertical-align: -0.489ex; width: 5.193ex; height: 2.081ex;" src="images/87.svg" alt=" " data-tex="\xi, \eta, \zeta"> their corresponding momenta, the
variation with time of the constructed-volume will cease and the
objection cited will no longer be a valid one.
</p>
<p>
Now let us take up the determination of the <i>number of complexions</i>
<img style="vertical-align: -0.05ex; width: 2.371ex; height: 1.595ex;" src="images/17.svg" alt=" " data-tex="W"> in the given state. For this purpose think of this
whole state as represented by the sum total of all these equal elementary
regions <img style="vertical-align: -0.025ex; width: 4.266ex; height: 1.91ex;" src="images/94.svg" alt=" " data-tex="m^{3}\sigma">; for convenience of reference let us call
this whole state the "state-region." The probability that a particular
molecule will belong to a particular elementary region
is equally great for all the elementary regions. Let <img style="vertical-align: 0; width: 1.699ex; height: 1.545ex;" src="images/95.svg" alt=" " data-tex="P"> represent
the number of these equal elementary regions. Now we will
proceed with the help of a parallel case. Let us think of as many
dice <img style="vertical-align: 0; width: 2.009ex; height: 1.545ex;" src="images/96.svg" alt=" " data-tex="N"> as there are molecules and let each die be provided with
<img style="vertical-align: 0; width: 1.699ex; height: 1.545ex;" src="images/95.svg" alt=" " data-tex="P"> faces. On each of these faces we will write in their order the
digits 1, 2, 3, ... <img style="vertical-align: 0; width: 1.699ex; height: 1.545ex;" src="images/95.svg" alt=" " data-tex="P">, so that each of the <img style="vertical-align: 0; width: 1.699ex; height: 1.545ex;" src="images/95.svg" alt=" " data-tex="P"> faces will designate
a particular elementary region. Then each throw of the<img style="vertical-align: 0; width: 2.009ex; height: 1.545ex;" src="images/96.svg" alt=" " data-tex="N">
dice will result in representing a particular state of the gas, because
the number of dice which show uppermost a particular digit
will give the number of molecules belonging to the elementary
region represented by said digit. In this parallel case each die is
equally likely to show up any one of the digits 1 to <img style="vertical-align: 0; width: 1.699ex; height: 1.545ex;" src="images/95.svg" alt=" " data-tex="P">, corresponding
<span class="pagenum" id="Page_60">[Pg 60]</span>
to the circumstance that each individual molecule is equally likely to
belong to any one of the elementary regions. The desired probability
<img style="vertical-align: -0.05ex; width: 2.371ex; height: 1.595ex;" src="images/17.svg" alt=" " data-tex="W"> of the given state of the molecules corresponds therefore to the
number of different kinds of throws (complexions), by which the given
distribution <img style="vertical-align: -0.464ex; width: 1.244ex; height: 2.059ex;" src="images/43.svg" alt=" " data-tex="f"> can be realized. For example, if we take <img style="vertical-align: -0.186ex; width: 7.289ex; height: 1.731ex;" src="images/97.svg" alt=" " data-tex="N = 10">
molecules (dice) and <img style="vertical-align: -0.186ex; width: 5.847ex; height: 1.731ex;" src="images/98.svg" alt=" " data-tex="P = 6"> elementary regions (dice faces), and
assume that the state is so given that it is represented by:
<span class="align-center"><img style="vertical-align: -8.484ex; width: 33.335ex; height: 18.1ex;" src="images/99.svg" alt=" " data-tex="
\begin{array}{l}
3\,  \text{molecules in elementary region}\,\, 1\\
4\,  \text{molecules in elementary region}\,\, 2\\
0\,  \text{molecules in elementary region}\,\, 3\\
1\,  \text{molecules in elementary region}\,\, 4\\
0\,  \text{molecules in elementary region}\,\, 5\\
2\,  \text{molecules in elementary region}\,\, 6
\end{array}
"></span>
Then this state can be realized by one throw, in which the 10 dice show
the following digits:
<span class="align-center"><img style="vertical-align: -3.733ex; width: 67.857ex; height: 8.597ex;" src="images/100.svg" alt=" " data-tex="
\begin{array}{l}
1st\quad  2d\quad  3d\quad  4th\quad  5th\quad  6th\quad  7th\quad  8th\quad  9th\quad  10th\quad  die\\
\text{the}\\
\,2\qquad  6\qquad  2\qquad  1\qquad  1\qquad  2\qquad  6\qquad  2\qquad  1\qquad  4\quad  digit
\end{array}
\qquad\text{(15)}
"></span>
</p>
<p>
Under each of the 10 dice stands the digit shown uppermost in the throw.
In fact, we see
<span class="align-center"><img style="vertical-align: -8.484ex; width: 17.351ex; height: 18.1ex;" src="images/101.svg" alt=" " data-tex="
\begin{array}{l}
3\,  \text{dice with digit}\,\, 1\\
4\,  \quad″ \quad″ \quad″\quad\, 2\\
0\,  \quad″ \quad″ \quad″\quad\, 3\\
1\,  \quad″ \quad″ \quad″\quad\, 4\\
0\,  \quad″ \quad″ \quad″\quad\, 5\\
2\,  \quad″ \quad″ \quad″\quad\, 6
\end{array}
"></span>
</p>
<p>
In like manner the same state can be realized by many other
such complexions. The desired number of all possible complexions
can be found by considering the digit row designated above
by (15). For, since the number of molecules (dice) is given,
<span class="pagenum" id="Page_61">[Pg 61]</span>
the digit row will have a particular number of places (<img style="vertical-align: -0.186ex; width: 7.289ex; height: 1.731ex;" src="images/102.svg" alt=" " data-tex="10 = N">).
Besides, since the number of molecules belonging to each elementary
space is given, each digit will occur equally often in the row
in all permissible complexions. Moreover every change in digit
arrangement effects a new complexion. The number of the
possible complexions or the probability <img style="vertical-align: -0.05ex; width: 2.371ex; height: 1.595ex;" src="images/17.svg" alt=" " data-tex="W"> of the given state is
therefore equal, under the conditions specified, to the possible
"permutations with repetition."<a id="FNanchor_21_1"></a><a href="#Footnote_21_1" class="fnanchor">[21]</a> In the simple example chosen
we have for such permutation, according to a known formula,
<span class="align-center"><img style="vertical-align: -1.602ex; width: 23.754ex; height: 4.751ex;" src="images/103.svg" alt=" " data-tex="
\frac{10!}{3!\, 4!\, 0!\, 1!\, 0!\, 2!} = 12,600.
"></span>
Consequently in the general case, we have
<span class="align-center"><img style="vertical-align: -2.172ex; width: 23.188ex; height: 5.321ex;" src="images/104.svg" alt=" " data-tex="
W = \frac{N!}{\Pi(f \cdot \sigma)!}
\qquad\text{(16)}
"></span>
where the sign <img style="vertical-align: 0; width: 1.697ex; height: 1.538ex;" src="images/105.svg" alt=" " data-tex="\Pi"> signifies the product extended over all the <img style="vertical-align: 0; width: 1.699ex; height: 1.545ex;" src="images/95.svg" alt=" " data-tex="P">
elementary regions.
</p>
<p>
Result contained in (16) is equally true for any other physical system,
say, one involving radiant energy. For the conditions and the variables
are similar to those of the molecular system just employed. The chosen
model, the dice system, which served as an easily conceived parallel
case, would be equally serviceable in dealing with the elements of
radiation.
</p>

<div class="footnote">

<p class="nind">
<a id="Footnote_20_1"></a><a href="#FNanchor_20_1"><span class="label">[20]</span></a>Such a fictitious space does not occur in the proof of MAXWELL'S distribution,
because there conditions are simpler. See footnote to <a href="#Page_49">p. 49</a>.</p></div>

<div class="footnote">

<p class="nind">
<a id="Footnote_21_1"></a><a href="#FNanchor_21_1"><span class="label">[21]</span></a>Compare with pp. <a href="#Page_27">27</a> and <a href="#Page_28">28</a> where this permutation process is discussed somewhat
fully.</p></div>

<p><span class="pagenum" id="Page_62">[Pg 62]</span></p>

<p><br><br></p>

<p class="center">
<i>Step b</i>
<br>
<i>Determination of a General Expression for the Entropy S of any
given Natural State</i></p>

<p>
This step is an easy one. We have in Eq. (10) the relation expressing
the universal dependence of entropy <img style="vertical-align: -0.05ex; width: 1.459ex; height: 1.645ex;" src="images/2.svg" alt=" " data-tex="S"> on probability <img style="vertical-align: -0.05ex; width: 2.371ex; height: 1.595ex;" src="images/17.svg" alt=" " data-tex="W">.
Substituting and writing out the logarithm of the quotient given in
(16), we have
<span class="align-center"><img style="vertical-align: -0.566ex; width: 35.813ex; height: 2.262ex;" src="images/106.svg" alt=" " data-tex="
S = k\, log N! - k\, \Sigma\, log (f \cdot \sigma)!
\qquad\text{(17)}
"></span>
</p>
<p>
The summation <img style="vertical-align: 0; width: 1.633ex; height: 1.545ex;" src="images/107.svg" alt=" " data-tex="\Sigma"> must be extended over all the elementary
regions <img style="vertical-align: -0.025ex; width: 1.292ex; height: 1ex;" src="images/90.svg" alt=" " data-tex="\sigma">. With the help of STIRLING'S formula, and remembering
that both <img style="vertical-align: -0.025ex; width: 1.292ex; height: 1ex;" src="images/90.svg" alt=" " data-tex="\sigma"> and <img style="vertical-align: -0.464ex; width: 10.83ex; height: 2.059ex;" src="images/108.svg" alt=" " data-tex="N = \Sigma f \cdot \sigma"> are constant for
all changes of state, the above expression (17) is reduced to the form
<span class="align-center"><img style="vertical-align: -0.566ex; width: 45.841ex; height: 2.262ex;" src="images/109.svg" alt=" " data-tex="
\text{Entropy}\, S = \text{constant} - k\, \Sigma f \cdot log f \cdot \sigma
\qquad\text{(18)}
"></span>
This magnitude <img style="vertical-align: -0.05ex; width: 1.459ex; height: 1.645ex;" src="images/2.svg" alt=" " data-tex="S"> is numerically the same as <img style="vertical-align: 0; width: 2.009ex; height: 1.545ex;" src="images/11.svg" alt=" " data-tex="H"> for which
BOLTZMANN proved that it changed in a one-sided way in all changes of
state. We must bear in mind too that function <img style="vertical-align: -0.464ex; width: 1.244ex; height: 2.059ex;" src="images/43.svg" alt=" " data-tex="f"> represents, <i>for
every state of the gas</i>, the given space and velocity distribution of
the gas molecules. The permanent, stationary, state of the gas known as
thermal equilibrium is only a special case of the general case (18),
this special case being widely known as MAXWELL'S Law of Distribution of
Velocities.
</p>

<p><br><br></p>

<p class="center">
<i>Step c</i>
<br>
<i>Special Case of (b), Namely, Determination of Entropy S for the
Thermal Equilibrium of a Monatomic Gas</i></p>

<p>
This case PLANCK derives very easily from the general case
represented by (18). As the desired result has already been
found in another way in pp. <a href="#Page_48">48</a>-<a href="#Page_53">53</a> when dealing with MAXWELL'S
<span class="pagenum" id="Page_63">[Pg 63]</span>
Law (of distribution of molecular velocities), we will not repeat
PLANCK'S derivation of the law from (18). It will suffice here to
give the results: The law of distribution is given by function
<span class="align-center"><img style="vertical-align: -2.148ex; width: 66.05ex; height: 6.193ex;" src="images/110.svg" alt=" " data-tex="
f = \alpha e^{-\beta(\xi^{2} + \eta^{2} + \zeta^{2})}\quad
\alpha = \frac{N}{V}\left(\frac{3mN}{ 4\pi U}\right)^{\frac{3}{2}}\quad \text{and}\quad \beta = \frac{3mN}{4U},
\qquad\text{(19)}
"></span>
where <img style="vertical-align: -0.025ex; width: 1.448ex; height: 1.025ex;" src="images/111.svg" alt=" " data-tex="\alpha"> and <img style="vertical-align: -0.439ex; width: 1.281ex; height: 2.034ex;" src="images/112.svg" alt=" " data-tex="\beta"> are constants and <img style="vertical-align: -0.05ex; width: 1.735ex; height: 1.595ex;" src="images/58.svg" alt=" " data-tex="U"> the total energy.
As this expression for function <img style="vertical-align: -0.464ex; width: 1.244ex; height: 2.059ex;" src="images/43.svg" alt=" " data-tex="f"> is free from all location
co-ordinates <img style="vertical-align: -0.464ex; width: 5.467ex; height: 1.464ex;" src="images/113.svg" alt=" " data-tex="x, y, z">, we see that this state of thermal equilibrium
is independent of these space co-ordinates and conclude that in this
state the molecules are uniformly distributed in space; only the
velocities are variously distributed, all of which accords with the
earlier presentation. Substituting the results of (19) in the general
equation (18) there results for the entropy S of the state of
equilibrium of a monatomic gas,
<span class="align-center"><img style="vertical-align: -0.781ex; width: 42.909ex; height: 2.736ex;" src="images/114.svg" alt=" " data-tex="
S = \text{constant} + kN(\tfrac{3}{2} log U + log V)
\qquad\text{(20)}
"></span>
To make Eq. (20) practically serviceable we need to know the constants
<img style="vertical-align: -0.025ex; width: 1.179ex; height: 1.595ex;" src="images/3.svg" alt=" " data-tex="k"> and <img style="vertical-align: 0; width: 2.009ex; height: 1.545ex;" src="images/96.svg" alt=" " data-tex="N"> and they will be found later on.
</p>

<p><br><br></p>

<p class="center">
<i>Step d</i>
<br>
<i>Confirmation, by Equating this Probability Value of S with that
found Thermodynamically and Securing well-known Results</i></p>

<p>
We know from Thermodynamics that the change of entropy is defined in
a perfectly general way (for physical changes)<a id="FNanchor_22_1"></a><a href="#Footnote_22_1" class="fnanchor">[22]</a> by
<span class="align-center"><img style="vertical-align: -1.552ex; width: 24.928ex; height: 4.652ex;" src="images/115.svg" alt=" " data-tex="
dS = \frac{dU + pdV}{T}
\qquad\text{(21)}
"></span>
</p>
<p>
Deriving the partial differential coefficients and making use of (20),
there follows:
<span class="align-center"><img style="vertical-align: -1.602ex; width: 28.165ex; height: 4.701ex;" src="images/116.svg" alt=" " data-tex="
p = \frac{kNT}{V} = \frac{RnT}{V},
\qquad\text{(22)}
"></span>
<span class="pagenum" id="Page_64">[Pg 64]</span>
where <img style="vertical-align: -0.025ex; width: 1.357ex; height: 1.025ex;" src="images/33.svg" alt=" " data-tex="n"> = number of gram-molecules (referred to <img style="vertical-align: -0.464ex; width: 9.073ex; height: 2.057ex;" src="images/117.svg" alt=" " data-tex="O_{2} = 32g">) and
<img style="vertical-align: -1.11ex; width: 18.294ex; height: 3.081ex;" src="images/118.svg" alt=" " data-tex="R = 8.315 (10^{7}) \frac{\text{erg}}{\text{deg}}"> = absolute gas
constant [1545 in F.P.S. system]. Here the first of Eqs. (22), represents
the combined laws of BOYLE, GAY-LUSSAC, and AVOGADRO. We get besides from
the equating of (20) and (21), the additional relations,
<span class="align-center"><img style="vertical-align: -1.552ex; width: 48.521ex; height: 4.627ex;" src="images/119.svg" alt=" " data-tex="
k = \frac{Rn}{N} = \omega R,\quad U = \frac{3}{2} kNT = Anc_{2}T,
\qquad\text{(23)}
"></span>
where mechanical equivalent
<img style="vertical-align: -0.798ex; width: 16.88ex; height: 2.753ex;" src="images/120.svg" alt=" " data-tex="A = 4.19(10^{5}) \frac{\text{erg}}{\text{cal}}"> C.G.S. system.
</p>
<p>
From this follows
<span class="align-center"><img style="vertical-align: -1.909ex; width: 44.084ex; height: 4.945ex;" src="images/121.svg" alt=" " data-tex="
c_{v} = 3.0,\quad  c_{p} = 5,\quad \text{and}\quad
\frac{c_{p}}{c_{v}} = \frac{5}{3},
\qquad\text{(24)}
"></span>
as is known for monatomic gases.
</p>
<p>
Furthermore, we find for the mean kinetic energy <img style="vertical-align: 0; width: 1.541ex; height: 1.545ex;" src="images/122.svg" alt=" " data-tex="L"> of a molecule
<span class="align-center"><img style="vertical-align: -1.552ex; width: 25.031ex; height: 4.627ex;" src="images/123.svg" alt=" " data-tex="
L = \frac{U}{N} = \frac{3}{2}kT.
\qquad\text{(25)}
"></span>
</p>
<p>
We also have
<span class="align-center"><img style="vertical-align: -1.577ex; width: 74.115ex; height: 4.701ex;" src="images/124.svg" alt=" " data-tex="
\omega = \frac{n}{N} = \frac{w}{m} = \frac{\text{wt. of a molecule}}{\text{molecular wt. of a molecule}} = \text{const. for all gases}.
\qquad\text{(26)}
"></span>
</p>
<p>
With the help of the specific heats and the characteristic equation of
the gas, the whole thermodynamic behavior of the gas is disclosed. All
this has resulted from the identification of the mechanical and
thermodynamic expressions for entropy and is an indication of the
fruitfulness of the method pursued. PLANCK also shows that this method
leads to the finding of results heretofore unknown.
</p>

<div class="footnote">

<p class="nind">
<a id="Footnote_22_1"></a><a href="#FNanchor_22_1"><span class="label">[22]</span></a>This differential equation is valid
only for changes of temperature and volume of the body but not for its
changes of mass and of chemical composition, for in defining entropy
nothing was said of these latter changes.</p></div>

<p><span class="pagenum" id="Page_65">[Pg 65]</span></p>

<p><br><br></p>

<p class="center">
<i>Step e</i>
<br>
<i>Conversion of the General Expressions in (b) and (c) into more
Precise ones by Finding and Inserting the Numerical Value of
the Universal Constant k; Some of the Results</i></p>

<p>
From the consideration of certain phenomena of radiation PLANCK found
<span class="align-center"><img style="vertical-align: -5.138ex; width: 66.234ex; height: 11.408ex;" src="images/125.svg" alt=" " data-tex="
\begin{aligned}
k &= \frac{\alpha \beta^{3} b^{3}}{48\pi \alpha}
= 1.346(10^{-16}) \frac{\text{erg}}{\text{deg}}\,
\text{in absolute C. G. S. system};\\
k &= 5.5(10^{-24}) \frac{\text{ft.-lbs.}}{\text{deg (F.)}}\, \text{in F. P. S. system},
\end{aligned}
\qquad\text{(27)}
"></span>
where <img style="vertical-align: -0.023ex; width: 1.197ex; height: 1.02ex;" src="images/12.svg" alt=" " data-tex="a"> and <img style="vertical-align: -0.025ex; width: 0.971ex; height: 1.595ex;" src="images/126.svg" alt=" " data-tex="b"> are constants found by experiment while <img style="vertical-align: -0.025ex; width: 1.448ex; height: 1.025ex;" src="images/111.svg" alt=" " data-tex="\alpha">
and <img style="vertical-align: -0.439ex; width: 1.281ex; height: 2.034ex;" src="images/112.svg" alt=" " data-tex="\beta"> are exactly known values, mathematically derived. The
present accuracy of (27) therefore rests on the accuracy of the
experiments from which <img style="vertical-align: -0.023ex; width: 1.197ex; height: 1.02ex;" src="images/12.svg" alt=" " data-tex="a"> and <img style="vertical-align: -0.025ex; width: 0.971ex; height: 1.595ex;" src="images/126.svg" alt=" " data-tex="b"> were found. In discussing Eq.
(10) it was pointed out that <img style="vertical-align: -0.025ex; width: 1.179ex; height: 1.595ex;" src="images/3.svg" alt=" " data-tex="k"> was a universal constant, applicable
to all physical systems and consequently may be used for the molecular
configurations mainly considered in this presentation. But before
introducing numerical value of <img style="vertical-align: -0.025ex; width: 1.179ex; height: 1.595ex;" src="images/3.svg" alt=" " data-tex="k"> in the general expressions
contained under headings <img style="vertical-align: -0.025ex; width: 0.971ex; height: 1.595ex;" src="images/126.svg" alt=" " data-tex="b"> and <img style="vertical-align: -0.025ex; width: 0.98ex; height: 1.025ex;" src="images/54.svg" alt=" " data-tex="c">, we will add other numerical
values of interest.
</p>
<p>
PLANCK gives <img style="vertical-align: -0.566ex; width: 9.833ex; height: 2.52ex;" src="images/127.svg" alt=" " data-tex="2.76(10^{19})"> = number of molecules in 1 ccm. of an ideal
gas at freezing-point (<img style="vertical-align: -0.05ex; width: 2.262ex; height: 1.667ex;" src="images/128.svg" alt=" " data-tex="0°"> C.) and atmospheric pressure; he also gives
for the ratio <img style="vertical-align: -1.577ex; width: 16.986ex; height: 4.652ex;" src="images/129.svg" alt=" " data-tex="\dfrac{N}{n} = 6.175(10^{23})"> = number of molecules
per <img style="vertical-align: -0.025ex; width: 1.986ex; height: 1.025ex;" src="images/4.svg" alt=" " data-tex="m"> grams; the corresponding numbers in English units are,
approximately, <img style="vertical-align: -0.566ex; width: 9.204ex; height: 2.52ex;" src="images/130.svg" alt=" " data-tex="782(10^{23})"> = number of molecules in one cubic foot
of an ideal gas and <img style="vertical-align: -1.577ex; width: 16.85ex; height: 5.138ex;" src="images/131.svg" alt=" " data-tex="\dfrac{N}{n} = \dfrac{2.80(10^{26})}{m}"> = number of
molecules in one pound of an ideal gas. Assuming air to be an ideal gas
and its "apparent" molecular weight about 28.88, the number of molecules
<span class="pagenum" id="Page_66">[Pg 66]</span>
in one pound of air would be <img style="vertical-align: -0.816ex; width: 23.3ex; height: 2.773ex;" src="images/132.svg" alt=" " data-tex="\frac{2.80}{28.88}(10^{26}) = 0.97(10^{25})">.
</p>
<p>
Substituting the numerical value of universal constant <img style="vertical-align: -0.025ex; width: 1.179ex; height: 1.595ex;" src="images/3.svg" alt=" " data-tex="k"> in
Eq. (10) we get Eq. (28).
</p>
<p class="nind">
For C. G. S. system, Entropy of any natural state, Eq. (28) is
<span class="align-center"><img style="vertical-align: -0.566ex; width: 65.56ex; height: 2.565ex;" src="images/133.svg" alt=" " data-tex="
S = 1.346(10^{-16})\, log_{e} (W) = 1.346(10^{-16})\, log_{e}\, (\text{No. of complexions}).
"></span>
</p>
<p>
For F. P. S. system, Entropy of any natural state, Eq. (28) is
<span class="align-center"><img style="vertical-align: -0.566ex; width: 63.675ex; height: 2.583ex;" src="images/134.svg" alt=" " data-tex="
S = 5.50(10^{-24})\, log_{e} (W) = 5.50(10^{-24})\, log_{e}\, (\text{No. of complexions}.)
"></span>
</p>
<p>
To each of these may be added an arbitrary constant. In Eq. (20) we may
substitute directly the equivalent of the product <img style="vertical-align: -0.025ex; width: 3.188ex; height: 1.595ex;" src="images/135.svg" alt=" " data-tex="kN"> found from Eq.
(22), and then get for the entropy <img style="vertical-align: -0.05ex; width: 1.459ex; height: 1.645ex;" src="images/2.svg" alt=" " data-tex="S"> of a monatomic gas in the state
of thermal equilibrium,
<span class="align-center"><img style="vertical-align: -1.552ex; width: 44.601ex; height: 4.627ex;" src="images/136.svg" alt=" " data-tex="
S = \text{constant} + \frac{pV}{T}(\tfrac{3}{2} log U + log V).
\qquad\text{(29)}
"></span>
</p>
<p>
When the volume <img style="vertical-align: -0.05ex; width: 1.74ex; height: 1.595ex;" src="images/8.svg" alt=" " data-tex="V"> is known we can now readily find <img style="vertical-align: 0; width: 2.009ex; height: 1.545ex;" src="images/96.svg" alt=" " data-tex="N"> and then
<img style="vertical-align: -0.025ex; width: 3.188ex; height: 1.595ex;" src="images/135.svg" alt=" " data-tex="kN"> numerically, and place this number as a coefficient in Eq. (20).
</p>

<p><br><br></p>

<p class="center">
<i>Step f</i>
<br>
<i>Determination of the Dimensions of k or of the Entropy S</i></p>

<p>
It is at once evident from an inspection of the perfectly general
Eq. (10) that the dimensions of Entropy <img style="vertical-align: -0.05ex; width: 1.459ex; height: 1.645ex;" src="images/2.svg" alt=" " data-tex="S"> depend solely on those
of the universal constant <img style="vertical-align: -0.025ex; width: 1.179ex; height: 1.595ex;" src="images/3.svg" alt=" " data-tex="k">. The relation given in Eq. (21) shows
at once that dimensions of <img style="vertical-align: -0.05ex; width: 2.636ex; height: 1.645ex;" src="images/137.svg" alt=" " data-tex="dS"> depend upon the quotient found
by dividing energy by temperature and the relations given in
Eqs. (22) and (25) that the dimensions of constant <img style="vertical-align: -0.025ex; width: 1.179ex; height: 1.595ex;" src="images/3.svg" alt=" " data-tex="k"> also depend
on this same quotient. The dimensions of Entropy <img style="vertical-align: -0.05ex; width: 1.459ex; height: 1.645ex;" src="images/2.svg" alt=" " data-tex="S"> and of constant
<img style="vertical-align: -0.025ex; width: 1.179ex; height: 1.595ex;" src="images/3.svg" alt=" " data-tex="k"> are therefore identical and this might suffice to show that
here neither <img style="vertical-align: -0.05ex; width: 1.459ex; height: 1.645ex;" src="images/2.svg" alt=" " data-tex="S"> nor <img style="vertical-align: -0.025ex; width: 1.179ex; height: 1.595ex;" src="images/3.svg" alt=" " data-tex="k"> is to be regarded as a mere ratio or abstract
<span class="pagenum" id="Page_67">[Pg 67]</span>
number. A word further in this connection may, however, be
helpful. In reversible processes we have the well-known relation
<img style="vertical-align: -0.439ex; width: 10.212ex; height: 2.034ex;" src="images/138.svg" alt=" " data-tex="dQ = TdS">. To simplify matters, let us suppose heat <img style="vertical-align: -0.439ex; width: 2.966ex; height: 2.032ex;" src="images/139.svg" alt=" " data-tex="dQ"> supplied
while volume is kept constant, then <img style="vertical-align: -0.439ex; width: 17.941ex; height: 2.034ex;" src="images/140.svg" alt=" " data-tex="dQ = c_{v}dT = TdS"> or
<span class="align-center"><img style="vertical-align: -1.552ex; width: 20.914ex; height: 4.652ex;" src="images/141.svg" alt=" " data-tex="
dS = \frac{dT}{T} c_{v}.
\qquad\text{(30)}
"></span>
Here Entropy <img style="vertical-align: -0.05ex; width: 1.459ex; height: 1.645ex;" src="images/2.svg" alt=" " data-tex="S"> has the same dimensions as <img style="vertical-align: -0.357ex; width: 1.943ex; height: 1.357ex;" src="images/142.svg" alt=" " data-tex="c_{v}">; now in the
relation <img style="vertical-align: -0.439ex; width: 10.696ex; height: 2.032ex;" src="images/143.svg" alt=" " data-tex="dQ = c_{v}dT"> if we regard <img style="vertical-align: -0.357ex; width: 1.943ex; height: 1.357ex;" src="images/142.svg" alt=" " data-tex="c_{v}"> as an abstract number
then, in order that the equation shall be homogeneous the factor
(<img style="vertical-align: -0.023ex; width: 2.769ex; height: 1.593ex;" src="images/144.svg" alt=" " data-tex="dT">) must represent heat energy like <img style="vertical-align: -0.439ex; width: 2.966ex; height: 2.032ex;" src="images/139.svg" alt=" " data-tex="dQ">, and this is sometimes
done; in such case (if <img style="vertical-align: 0; width: 1.593ex; height: 1.532ex;" src="images/145.svg" alt=" " data-tex="T"> retains its ordinary meaning) the quotient
<img style="vertical-align: -1.552ex; width: 3.765ex; height: 4.652ex;" src="images/146.svg" alt=" " data-tex="\dfrac{dT}{T}"> in Eq. (30) is no longer a mere ratio or abstract
number, but a quotient of the dimensions of energy divided by
temperature. On the other hand, if <img style="vertical-align: -1.575ex; width: 8.922ex; height: 4.697ex;" src="images/147.svg" alt=" " data-tex="c_{v} = \dfrac{dQ}{dT}"> be
regarded as of the dimensions of the quotient of energy divided by
temperature, then we may consider <img style="vertical-align: -1.552ex; width: 3.765ex; height: 4.652ex;" src="images/146.svg" alt=" " data-tex="\dfrac{dT}{T}"> in (30) as an
abstract number or ratio and <img style="vertical-align: -0.05ex; width: 2.636ex; height: 1.645ex;" src="images/137.svg" alt=" " data-tex="dS"> of the same dimensions as <img style="vertical-align: -0.357ex; width: 2.581ex; height: 1.952ex;" src="images/148.svg" alt=" " data-tex="C_{v}">.
When an absolute system of units is employed, which possesses as one of
its features the expression of temperature in units of energy, then
<img style="vertical-align: -0.025ex; width: 1.179ex; height: 1.595ex;" src="images/3.svg" alt=" " data-tex="k">, <img style="vertical-align: -0.05ex; width: 1.459ex; height: 1.645ex;" src="images/2.svg" alt=" " data-tex="S"> and <img style="vertical-align: -0.357ex; width: 1.943ex; height: 1.357ex;" src="images/142.svg" alt=" " data-tex="c_{v}"> will all be mere ratios or abstract numbers.<a id="FNanchor_23_1"></a><a href="#Footnote_23_1" class="fnanchor">[23]</a>
</p>

<div class="footnote">

<p class="nind">
<a id="Footnote_23_1"></a><a href="#FNanchor_23_1"><span class="label">[23]</span></a>See C. V. BURTON'S article in Philosophical Transactions, Vol. 23-24, 1887.</p></div>

<p><span class="pagenum" id="Page_68">[Pg 68]</span></p>

</div>

<div class="chapter">
<h2><a id="PART_III"></a>PART III
<br><br>
PHYSICAL INTERPRETATIONS</h2>

<h2><a id="SECTION_A3"></a>SECTION A
<br><br>
OF THE SIMPLE REVERSIBLE OPERATIONS IN THERMODYNAMICS</h2>

<p class="center">
<i>Change under Constant Volume</i></p>

<p>
We found above that the entropy of a state was precisely defined in a
physical way by the number of complexions of that state. Now let us see
what happens to this number of complexions when an ideal gas experiences
some of the simpler changes, of a reversible (non-cyclical) character.
We will begin with the case in which the volume of the gas remains
constant while its temperature rises, the final state of the gas having
a higher temperature than its initial state.
</p>
<p>
We see from Eq. (7), <a href="#Page_51">p. 51</a>, that <img style="vertical-align: -0.025ex; width: 0.98ex; height: 1.025ex;" src="images/54.svg" alt=" " data-tex="c"> grows and
from Eq. (4), <a href="#Page_50">p. 50</a>, that <img style="vertical-align: -0.05ex; width: 1.719ex; height: 1.645ex;" src="images/53.svg" alt=" " data-tex="C"> diminishes.
MAXWELL'S Law, given by Eq. (5), <a href="#Page_50">p. 50</a>, shows for
a given velocity <img style="vertical-align: -1.577ex; width: 2.165ex; height: 4.106ex;" src="images/149.svg" alt=" " data-tex="\dfrac{\rho}{c}"> that the number <img style="vertical-align: -2.041ex; width: 3.529ex; height: 5.14ex;" src="images/150.svg" alt=" " data-tex="\dfrac{dn}{d\rho}">
of molecules possessing the given velocity is <i>less</i> in the final
state than it was in the initial state, and as the total number <img style="vertical-align: -0.025ex; width: 1.357ex; height: 1.025ex;" src="images/33.svg" alt=" " data-tex="n"> of
molecules in the gas is unchanged, there will be a greater variety of
velocities in the final state. This makes the number of possible
permutations greater in the final state, thus increasing the number of
complexions; consequently, as entropy varies with the logarithm of the
number of complexions, we see that the entropy of the final state is
greater than in the initial state and this agrees with experience.
<span class="pagenum" id="Page_69">[Pg 69]</span>
</p>

<p><br><br></p>

<p class="center">
<i>Isobaric Change</i></p>

<p>
Next we interpret how the number of complexions are affected by isobaric
change during a reversible process, again assuming that the temperature
in the final state is greater than in the initial one. Here the steps
and the conclusion are exactly the same as in the preceding case. In
both cases just the opposite result is reached when there is a
<i>fall</i> in temperature.
</p>
<p>
As the <img style="vertical-align: -0.439ex; width: 2.235ex; height: 1.441ex;" src="images/151.svg" alt=" " data-tex="pv"> diagram contains the co-ordinates <img style="vertical-align: -0.439ex; width: 3.241ex; height: 1.441ex;" src="images/152.svg" alt=" " data-tex="p, v">, and represents
mainly the mechanical changes in the body under consideration, we can,
by suitable combination, similarly interpret any other reversible change
of state represented in this <img style="vertical-align: -0.439ex; width: 2.235ex; height: 1.441ex;" src="images/151.svg" alt=" " data-tex="pv"> diagram.
</p>

<p><br><br></p>

<p class="center">
<i>Isothermal Change</i></p>

<p>
However, because of its general importance and because of its bearing on
the temperature-entropy diagram, we will here also tell, in the same
physical terms, what happens when our ideal gas undergoes isothermal
change with increase of volume. As the temperature in the
final state is equal to that in the initial one, the quantity
<img style="vertical-align: -1.552ex; width: 10.977ex; height: 4.586ex;" src="images/153.svg" alt=" " data-tex="[w^{2}] = \dfrac{3}{2} c^{2}"> does not change and therefore <img style="vertical-align: -0.05ex; width: 1.719ex; height: 1.645ex;" src="images/53.svg" alt=" " data-tex="C">
does not change nor (see Eq. (5), <a href="#Page_50">p. 50</a>) does the
number <img style="vertical-align: -1.577ex; width: 3.529ex; height: 4.676ex;" src="images/154.svg" alt=" " data-tex="\dfrac{d\rho}{dn}"> of molecules possessing the velocity
<img style="vertical-align: -1.577ex; width: 2.165ex; height: 4.106ex;" src="images/149.svg" alt=" " data-tex="\dfrac{\rho}{c}"> change. The variety of velocities in the final state is
therefore the same as in the initial state and does not at all contribute
to that necessary increase in the number of complexions (configurations)
for which we are looking.
</p>
<p>
The <i>direction</i> of the velocity of a molecule would be another
variety element, but as the final volume evidently possesses as
many velocity directions as the initial volume, this element or
co-ordinate will not contribute to increased complexity in the
final state. But, as the volume has increased, the final state
will contain more unit volumes (and these can be taken as small
<span class="pagenum" id="Page_70">[Pg 70]</span>
as we please) than the initial state. As it is here equally likely
that a particular molecule will be found in any one of these unit
volumes, it is evident that the increase of volume will add increased
variety to the location or configuration of the molecules and by
indulging in the swapping of places inherent in the production of
complexions, we see that said increment of volume will make
the number of complexions in the final state greater than in the
initial state, which in turn means that the entropy in the final
state is also greater. This accords with experience, but it can
also be seen from the formula
<span class="align-center"><img style="vertical-align: -0.781ex; width: 51.563ex; height: 2.736ex;" src="images/155.svg" alt=" " data-tex="
\text{Entropy} S = \text{constant} +
kN[log \tfrac{3}{2} U + log V],
\qquad\text{(31)}
"></span>
derived by PLANCK (p. 63 of Vorl. ü. Theor. Physik), from probability
considerations, for the state of thermal equilibrium. Here <img style="vertical-align: -0.025ex; width: 1.179ex; height: 1.595ex;" src="images/3.svg" alt=" " data-tex="k"> is the
universal constant (see <a href="#Page_66">p. 66</a>) and the other
terms have the same meaning as before.
</p>

<p><br><br></p>

<p class="center">
<i>Isentropic Change</i></p>

<p>
The last reversible process, to be here physically interpreted, is
isentropic change from the initial state of thermal equilibrium to its
final state. Evidently only the physical elements underlying the
bracketed term in Eq. (31) need to be considered.
</p>
<p>
As we are considering isentropic change (<img style="vertical-align: -0.186ex; width: 6.784ex; height: 1.781ex;" src="images/156.svg" alt=" " data-tex="dS = 0">), it does not
make any difference whether on the one hand we think of this
isentropic change as accompanied by an increase in temperature
and decrease in volume, or on the other hand think of said change
as taking place with decrease of temperature and increase of
volume. Suppose we assume the latter kind of change. Then
from what has preceded we know that increase of volume by
itself would increase the number of complexions of the final
state, also, from what has gone before, we know that the drop
in temperature by itself will lead to decrease in the number of
complexions in the final state. These two influences acting
<span class="pagenum" id="Page_71">[Pg 71]</span>
simultaneously therefore tend to neutralize each other and if
they exist in the proper ratio, derivable from the bracketed quantity
in Eq. (31), they will completely balance each other and produce
no change whatever in the number of complexions while passing
from the initial to the final state of equilibrium, i.e., will produce
no change whatever in the entropy of the gas under consideration.
In isentropic change Nature has no preference for its various
states.
</p>
<p>
The temperature-entropy diagram considers mainly thermal changes, and as
we have considered the influence of both of its co-ordinates in the
number of complexions, we can ascertain by proper combination, for
<i>any</i> reversible change of state, the corresponding character of
the change in the number of complexions. It is evident, too, that in the
diagram any reversible change of state is equivalent, so far as the
change of entropy in the one body is concerned, to an isentropic change
combined with an isothermal change, the latter only affecting the
result, so far as change in number of complexions is concerned.
</p>

</div>

<div class="chapter">
<h2><a id="SECTION_B3"></a>SECTION B
<br><br>
OF THE FUNDAMENTALLY IRREVERSIBLE PROCESSES</h2>

<p>
If we consider only heat and mechanical phenomena and do not include
electrical occurrences, the irreversible processes may be grouped in
four classes:
</p>
<p>
(<i>a</i>) The body whose changes of state are considered is in contact
with one or more bodies whose temperatures differ by a finite amount
from its own. There is here flow of heat from hot to cold and the
process is an irreversible one.
</p>
<p>
(<i>b</i>) When the body experiences friction which develops heat
it is not possible to effect completely the opposite operation.
</p>
<p>
(<i>c</i>) The third group includes those changes of state in which
a body expands without at the same time developing an amount
of external energy which is exactly equal to the work of its elastic
<span class="pagenum" id="Page_72">[Pg 72]</span>
forces. For example this occurs when the pressure which a
body has to overcome is essentially (that is, finitely) less than
the body's own internal tension. In such a case it is not possible
to bring said body back to its initial state by a completely opposite
procedure. Examples of this group are: steam escaping from
a high-pressure boiler, compressed air flowing into a vacuum tank
and a spring suddenly released from its state of high tension.
</p>
<p>
(<i>d</i>) Suppose two gases existing at the same pressure and
temperature are on opposite sides of a partition; when the partition is
quickly removed the two gases will diffuse or mix. These gases will not
<i>unmix</i> of themselves and the diffusion process is an irreversible
one and is somewhat like the process considered under (<i>c</i>).
</p>
<p>
The foregoing facts and propositions have in the main already been
stated in this presentation and it will be profitable to make
comparisons with the definition of irreversible and reversible events
given on <a href="#Page_30">p. 30</a> and with the examples on pp.
<a href="#Page_31">31</a>, <a href="#Page_32">32</a>.
</p>

<p><br><br></p>

<p class="center">
HEAT CONDUCTION
</p>

<p>
The group under head (<i>a</i>) represents the irreversible processes
which perhaps occur most often, namely, the direct passage of
heat, by conduction or radiation, from a hot body to a cold
body, here say from a hot gas to a cold gas. The former loses
in heat energy what the latter gains. As radiation phenomena
have very special features of their own and for the present may
be said to be outside of our selected province, we will confine
our attention to heat conduction alone. Moreover, for our present
purpose, we will suppose said flow or change to take place without
alteration of volume of either the hot or the cold gas. Then will the
hot gas experience a drop in temperature and the cold one a rise
in temperature. We have already treated such isometric changes
and know that the number of complexions is thereby diminished
in the originally hotter body and increased in the originally colder
<span class="pagenum" id="Page_73">[Pg 73]</span>
one. If this increment is greater than the accompanying decrement, then
the final outcome of this direct passage from hot to cold is an increase
in the total number of complexions of the two gases. There will then, by
our precise definition, be a corresponding increase in the total entropy
of the two systems. It is foreign to our present purpose to prove in an
independent, purely mechanical way, that such excess does finally exist
and will here content ourselves with the well-known and simple
thermodynamic expression for this excess,
<span class="align-center"><img style="vertical-align: -2.148ex; width: 28.573ex; height: 5.428ex;" src="images/157.svg" alt=" " data-tex="
Q\left(\frac{1}{T_{2}} - \frac{1}{T_{1}}\right) > 0,
\qquad\text{(32)}
"></span>
where <img style="vertical-align: -0.439ex; width: 1.79ex; height: 2.032ex;" src="images/158.svg" alt=" " data-tex="Q"> is the heat energy thus directly transferred from the
hot to the cold body, <img style="vertical-align: -0.339ex; width: 2.309ex; height: 1.871ex;" src="images/159.svg" alt=" " data-tex="T_{1}"> the absolute temperature of the hot body
and <img style="vertical-align: -0.339ex; width: 2.309ex; height: 1.871ex;" src="images/160.svg" alt=" " data-tex="T_{2}"> that of the cold body.
</p>

<p><br><br></p>

<p class="center">
THE WORK OF FRICTION IS CONVERTED INTO HEAT
</p>

<p>
The group under head (<i>b</i>) contains a class of events which
usually attends, in one form or another, most natural phenomena.
</p>
<p>
We will here consider an interesting (but perhaps too special) case,
namely, the experiment performed by W. THOMSON and JOULE on the flow of
gas through a porous plug. The plug obstructed the uniform,
non-conducting, passage through which the gas was forced without
sensibly changing its velocity of flow: (See LORENZ' Technische
Wärmelehre, p. 275). It can easily be shown (in L., p. 274) that with
an ideally perfect gas,
<span class="align-center"><img style="vertical-align: -0.439ex; width: 47.858ex; height: 2.009ex;" src="images/161.svg" alt=" " data-tex="
\text{Final temperature}\, T_{2} = T_{1} = \text{initial temperature}.
"></span>
As a matter of fact, there was an actual though slight drop in
temperature found to exist with the most perfect gases available.
Evidently the process was a throttling one, reducing the larger initial
pressure to the smaller final one, which reduction was of course
accompanied by a corresponding increase in volume.
<span class="pagenum" id="Page_74">[Pg 74]</span>
</p>
<p>
Assuming that an ideally perfect gas was employed in the experiment, and
that the final state for our consideration is that corresponding to its
attainment of thermal equilibrium, we see that because of the unchanged
temperature there is here no loss of internal energy, for the work
consumed by the friction of the porous plug is <i>all</i> returned to
the gas by the heat developed by said friction. Moreover, the + and -
external work in this experiment also balance. Now although there has
been no loss of energy there has been a growth of entropy corresponding
to the evident increase in the number of complexions. This increase is
exactly equal to that found for reversible isothermal change of state
when accompanied by an increase in volume, and the discussion is
therefore not repeated here.
</p>
<p>
One phase of the above process is the conversion of mechanical work into
heat through the medium of friction.
</p>

<p><br><br></p>

<p class="center">
INCREASE OF VOLUME WITHOUT PERFORMANCE OF EXTERNAL
WORK BY ELASTIC FORCES OF THE GAS
</p>

<p>
This case of an irreversible process comes into group <i>c</i>. We will
consider here JOULE'S well-known experiment with the air tanks, in which
the compressed air, initially stored in the one tank, was allowed to
discharge into the other tank which, at the start, contained only a
vacuum. At the end of the experiment, when thermal equilibrium obtained,
the temperature in the two, now connected, tanks was the same as
originally existed in the compressed-air tank. Here of course it is
assumed that the air exchanged no heat whatever with the outside.
</p>
<p>
As the final state, like the initial state, is in thermal equilibrium,
and possesses the same temperature, we can ascertain the total change in
the number of complexions as we did when discussing isothermal and
reversible changes and because of the accompanying increase in the
volume of the air, find that here as there the number of complexions has
increased and that therefore the entropy of the air has increased in
this case.
<span class="pagenum" id="Page_75">[Pg 75]</span>
</p>
<p>
We might rest satisfied with this conclusion, but additional light will
be shed on entropy significance if we consider more in detail the
intermediate stages of this evidently irreversible process. The rush of
air from the full to the empty tank produces whirls and eddies of a
finite character and it is only when these have subsided, by the
conversion of the visible or sensible kinetic energy of their particles
into heat, that thermal equilibrium obtains. But at each intermediate
stage (while still visibly whirling and eddying) the gas possesses
entropy, even while in the turbulent condition. This is clear from our
present physical definition of entropy, namely, the logarithm of the
number of complexions of the state, for it is evident that even in this
turbulent state it possesses a certain number of complexions, however
difficult mathematically it may actually be to find this number.
Boltzmann found an expression for <i>any</i> condition; PLANCK gave it
the form of Eq. (18), <a href="#Page_63">p. 63</a>,
<span class="align-center"><img style="vertical-align: -0.566ex; width: 49.109ex; height: 2.262ex;" src="images/162.svg" alt=" " data-tex="
\text{Entropy} = S = \text{constant} - k \Sigma f \cdot log f \cdot \sigma,
\qquad\text{(33)}
"></span>
where <img style="vertical-align: -0.566ex; width: 16.405ex; height: 2.52ex;" src="images/163.svg" alt=" " data-tex="k = 1.346(10^{-16})"> (in the C.G.S. system) is a universal
constant, function <img style="vertical-align: -0.464ex; width: 1.244ex; height: 2.059ex;" src="images/43.svg" alt=" " data-tex="f"> is the law of distribution of the particles and
their velocity elements and
<img style="vertical-align: -0.489ex; width: 26.176ex; height: 2.081ex;" src="images/164.svg" alt=" " data-tex="\sigma = dx \cdot dy \cdot dz \cdot d\xi \cdot d\eta \cdot d\zeta">
is a sort of fictitious elementary region in a six-dimensional space.
From its derivation and definition the value given for entropy <img style="vertical-align: -0.05ex; width: 1.459ex; height: 1.645ex;" src="images/2.svg" alt=" " data-tex="S"> in
Eq. (33) depends only on the state of the body at the instant in
question and does not at all depend on its history preceding this
instant.
</p>
<p>
The difference between the value of <img style="vertical-align: -0.05ex; width: 1.459ex; height: 1.645ex;" src="images/2.svg" alt=" " data-tex="S"> for the final state (say, as
given for a gas by Eq. 20) and the value of <img style="vertical-align: -0.05ex; width: 1.459ex; height: 1.645ex;" src="images/2.svg" alt=" " data-tex="S"> as given by Eq. (33)
for the instant, constitutes the driving motive which urges the gas
toward thermal equilibrium. A similar difference or driving motive is
the underlying <i>impelling cause of all</i> natural phenomena.
<span class="pagenum" id="Page_76">[Pg 76]</span>
</p>

<p><br><br></p>

<p class="center">
OF THE DIFFUSION OF GASES
</p>

<p>
This case of an irreversible process comes under group <i>d</i>.
Concerning this phenomenon J. W. GIBBS established the following
proposition:
</p>
<p>
"The entropy of a mixture of gases is the sum of the entropies
which the individual gases would have, if each at the same temperature
occupied a volume equal to the total volume of the mixture."
</p>
<p>
That the total entropy will be larger as a result of the mixing detailed
under <i>d</i>, <a href="#Page_73">p. 73</a>, may be inferred from the
following consideration: When two gases are thus brought together, it is
<i>more probable</i> that in <i>any</i> part of the total space
available for this mixture there will be found <i>both</i> kinds of
molecules than only <i>one</i> kind of these molecules.
</p>
<p>
But this irreversible process can be explained in a more distinctly
physical way. The two gases are originally at the same pressure and
temperature; they mix without other changes occurring in surrounding
bodies; the mixture (when diffusion is completed) is at the same
pressure and temperature as the original gaseous constituents.
Considering each gas by itself, what has happened as the result of
diffusion is that each gas in its final state occupies a larger volume
than in its initial, unmixed, state. The presence of the other gas in
the mixture in no wise changes this fact. Of course this increment in
volume is accompanied by a corresponding decrement in its pressure,
without change in temperature. A sort of isothermal change of state has
taken place in the passage from one condition of thermal equilibrium to
the other. We have already seen that then the number of complexions of
the gas increases and consequently also its entropy. The sum of the
increments of the number of complexions separately experienced by the
two diffusing gases constitutes an increase in the total number of
complexions over and above the total number of complexions existing in
<span class="pagenum" id="Page_77">[Pg 77]</span>
both gases before diffusion. There is of course a corresponding increase in
entropy due to such diffusion.
</p>
<p>
All these irreversible processes are passages from less stable to more
stable conditions, from less probable to more probable states, or
summarizing:
</p>
<p>
There is in Nature a constant tendency to equalize temperature
differences, to convert work into heat, to increase disgregation and to
promote diffusion.
</p>
<p>
This tendency has also been described as the tendency in Nature to pass
from concentrated to distributed conditions of energy.
</p>
<p>
The four irreversible processes just discussed are all spontaneous ones,
i.e., they occur without the help of agencies external to the bodies
directly engaged in the transformations.
</p>
<p>
It is evident that the foregoing statements are really identical,
expressing the same thought in different ways.
</p>

</div>

<div class="chapter">
<h2><a id="SECTION_C3"></a>SECTION C
<br><br>
NEGATIVE CHANGE OF ENTROPY; SOME OF ITS PHYSICAL
FEATURES OR NECESSARY ACCOMPANIMENTS</h2>

<p>
A negative transformation in any part of a system is the diminution of
entropy which it experiences, and this we know means a diminution in the
number of complexions of the part considered. But there are some
features of such negative transformations which, while they do not in
themselves constitute any additional principle, deserve special mention.
</p>
<p>
Before we make such mention, however, we will anticipate a little, and
state the Second Law in forms which will make said features obvious:
</p>
<p>
In an irreversible cycle the sum of the changes of entropies experienced
by all the bodies concerned is greater than zero. When the cycle is
reversible in all of its parts, then said sum of entropy changes is
equal to zero.
</p>
<p>
A corollary from this theorem is that, in a cycle,
<span class="pagenum" id="Page_78">[Pg 78]</span>
</p>
<p>
<i>All</i> the negative transformations present <img style="vertical-align: -0.396ex; width: 1.76ex; height: 2.1ex;" src="images/165.svg" alt=" " data-tex="\leqq"> <i>all</i> the
positive transformations that occur.
</p>
<p>
When there is simply <img style="vertical-align: -2.148ex; width: 17.253ex; height: 5.428ex;" src="images/166.svg" alt=" " data-tex="\left\{\begin{aligned}
\text{an irreversible}\\ \text{a reversible}
\end{aligned}\right\}"> process without the cyclic feature, then <i>the
sum of the entropies of all the bodies participating in any one occurrence
is, at the end of the change of condition</i>
<img style="vertical-align: -2.148ex; width: 15.688ex; height: 5.428ex;" src="images/167.svg" alt=" " data-tex="\left\{\begin{aligned} greater\, than\\equal\qquad to\end{aligned}\right\}">
<i>that at the beginning</i>.
</p>
<p>
From this we see that a negative change of entropy always keeps company
with an equal or greater positive change of entropy.
</p>
<p>
Again, for sake of simplicity, use a gas as an illustration; then we may
say: (1) Every possible negative transformation in a gas is always
accompanied by a net positive transformation in the other and necessary
external agencies. (2) All possible negative transformations in a gas
are reversible ones. We here use the word possible because there is an
impossible class of negative transformations, namely, those which, so
far as order and directness are concerned, are the very opposites of the
so-called spontaneous changes of state.
</p>
<p>
It will suffice here to enumerate these opposites: Without external help
(<i>a</i>) to pass heat from a cold to a hot body, (<i>b</i>) to
decrease the volume of a gas, (<i>c</i>) to convert the heat of friction
directly back into the work which called it forth, (<i>d</i>) to
separate the gaseous constituents of a mixture.
</p>
<p>
By way of contrast we may add, that the so-called spontaneous
(irreversible) processes were all positive transformations which took
place without any change whatever in surrounding bodies.
<span class="pagenum" id="Page_79">[Pg 79]</span>
</p>

</div>

<div class="chapter">
<h2><a id="SECTION_D2"></a>SECTION D
<br><br>
PHYSICAL SIGNIFICANCE OF THE EQUIVALENTS FOR GROWTH OF
ENTROPY GIVEN ON PAGES <a href="#Page_42">42</a>-<a href="#Page_43">43</a></h2>

<p>
According to equivalent (1) growth of entropy is a passage from more to
less available energy. The comment already made on <a href="#Page_42">p. 42</a>
indicates sufficiently that this increase in unavailability is
due to the growth of the ungovernable features of molecular motions as
number of complexions increases.


</p>
<p>
Equivalent (2) states growth of entropy to be a passage from a
concentrated to a distributed condition of energy. In this scattered
state the energy is certainly less controllable and for the same reason
as that given concerning equivalent (1).
</p>
<p>
Equivalent (3) is based on the idea of irreversibility, and we saw on
<a href="#Page_36">p. 36</a> that the growth in the number of complexions
is the measure as well as the criterion of irreversibility. This growth
is therefore a sufficient and necessary feature of this equivalent.


</p>
<p>
The equivalents grouped under (4) are all based on the theory of
probabilities. We have seen on pp. <a href="#Page_36">36</a>,
<a href="#Page_62">62</a>, and elsewhere, that the probability <img style="vertical-align: -0.05ex; width: 2.371ex; height: 1.595ex;" src="images/17.svg" alt=" " data-tex="W"> of a
state is the logarithm of the number of complexions of the state. This
number is therefore a necessary feature of this set of equivalents and
hence constitutes its physical significance.
</p>
<p>
The set of equivalents grouped under (5) are all closely related, their
dependence being more or less indicated by the order in which they are
there stated. The outcome of the series is that growth of entropy
corresponds to an increase in the number of complexions.
</p>
<p>
The mathematical concept stated under (6) covers more than molecular
configurations; it covers configurations whose elements are those of
energy as well, and has been successfully applied by PLANCK in problems
dealing with the energy of radiation. Every such configuration has a
number of complexions.
<span class="pagenum" id="Page_80">[Pg 80]</span>
</p>

</div>

<div class="chapter">
<h2><a id="SECTION_E2"></a>SECTION E
<br><br>
PHYSICAL SIGNIFICANCE OF THE MORE SPECIFIC STATEMENTS
OF THE SECOND LAW GIVEN ON PAGES <a href="#Page_44">44</a>-<a href="#Page_47">47</a></h2>

<p>
In making here the contemplated comparisons and interpretations we must
keep in mind the three helpful propositions given on <a href="#Page_44">p. 44</a>.
</p>
<p>
The conservative statement under (1) is confessedly based on the
Calculus of Probabilities as applied to a mechanical system. We repeat
here therefore what was said about (4) of the preceding series of
equivalents, namely, that the number of complexions of the state is a
necessary feature of this statement of the second law and therefore
constitutes its physical significance.
</p>
<p>
The statement under (2) is a common one. As each of the exact
definitions of the entropy for every natural event has been shown to
depend solely on the number of complexions of a system (all the bodies
participating in the event being considered a part of the system) we
have here likewise in this number an adequate physical explanation of
the second law.
</p>
<p>
Statements (3), (8) and (9) have already been derived and explained in
this presentation (see pp. <a href="#Page_45">45</a>,
<a href="#Page_46">46</a>) as the result of the growth of the number of
complexions in every natural event, when all the bodies participating in
the event are considered.
</p>
<p>
Statement (4) is only a slight variation of (3) and needs no special
comment here.
</p>
<p>
The same may be said of the three forms under (5).
</p>
<p>
The statement in (6) is only a corollary resulting from the use of (3) or
(4) or (5).
</p>
<p>
The statement in (7) of the second law may be objected to
because the underlying definitions are not entirely free from
ambiguity and because it lacks a scientifically general character.
But it expresses compactly a matter of great consequence in
technical circles. Moreover its explanation in our physical terms
<span class="pagenum" id="Page_81">[Pg 81]</span>
is very simple and direct, viz., when waste is incurred there is a
growth in the number of complexions, the complexity of the molecular
motions has been increased. Less of the stored-up energy is available,
less is capable of being directed into certain technical channels.
Evidently the greater the complexity of the molecular motions the less
governable they are by any direct external force or influence we can
bring to bear. This is because we are unable to act directly on the
individual molecules and sway them to our special technical purpose. Our
external forces and agencies can only operate on the <i>aggregates</i>
comprising our system and must obey the one-sided law imposed on all
such aggregates.
<span class="pagenum" id="Page_82">[Pg 82]</span>
</p>

</div>

<div class="chapter">
<h2><a id="PART_IV"></a>PART IV
<br><br>
SUMMARY:
<br><br>
THE CONNECTION BETWEEN PROBABILITY, IRREVERSIBILITY,
ENTROPY AND THE SECOND LAW</h2>

<h2><a id="SECTION_A4">SECTION A</a></h2>

<p class="center">
(1) <i>Prerequisites and Conditions Necessary for the Application
of the Theory of Probabilities</i></p>

<p>
These may be briefly stated to be (<i>a</i>) atomic theory, (<i>b</i>) the
likeness of particles (or elements), (<i>c</i>) very numerous particles,
and (<i>d</i>) "elementary chaos."
</p>
<p>
The first prerequisite is that the body (here a gas) is made up
of small, discrete particles. This atomic theory has long been
the foundation stone of chemistry, and is again coming into
deserved esteem in Physics pure and simple. (See simple and
clear article in Harper's Monthly, June, 1910). But this minute
subdivision must be accompanied by the particles being of the same
kind, or at least belonging to comparatively few groups, each containing
many particles of the same sort. This likeness is necessary;
for only from this likeness results law and order in the whole
from disorder in the parts. If the constituents were of many
different kinds, the results in the aggregate would not be so simple
as we actually find them to be. There is an example of this sort
of complexity in chemistry. We have already intimated that the
particles of each kind must be very numerous, but special emphasis
must be laid on this prerequisite. If we ask how numerous these
<span class="pagenum" id="Page_83">[Pg 83]</span>
elements must be in order that the Theory of Probabilities may
be applicable, the answer is, as many constituents as are necessary
to determine the mean values which define the state in the macroscopic
sense (i.e., in the aggregate condition). An idea of the
extent to which Nature carries this subdivision is furnished by
the fact that one grain (avoirdupois) of air, under standard conditions,
contains,
<span class="align-center"><img style="vertical-align: -0.566ex; width: 18.516ex; height: 2.565ex;" src="images/168.svg" alt=" " data-tex="
14 (10^{20})\, \text{molecules},
"></span>
i.e., millions of billions of particles!
</p>
<p>
The last one of said prerequisites is "elementary chaos," and needs
further elucidation and limitation; we will therefore go into this
feature at greater length.
</p>
<p>
BOLTZMANN has used the term "molekular-ungeordnet" (molecularly
disordered) to designate this chaotic condition of the particles, and
PLANCK has introduced a more general term still, "elementar-ungeordnet"
(elementary disorder or chaos) in order to make the method applicable to
phenomena like radiation, in which the elements are not atoms or
particles but partial oscillations of different periods. The essence of
the matter seems to consist in excluding from consideration all such
regularities in the conditions of the elements as would lead to results
at variance with the well-known laws of physical phenomena, justifying
this exclusion by the assumption that no such elementary regularities
obtain in Nature. This only means that not all of the many molecular
arrangements, which are conceivable from the purely mechanical
standpoint, are actually realized. For instance, in an isolated gaseous
system we could conceive of a succession of elementary states at
variance with the principle of conservation of energy; such a set would
obviously not be realized. This exclusion or limitation leaves room for
various hypotheses as to said elementary disarrangement, but to be
admissible they must all permit of the legitimate application of the
Theory of Probabilities, the best one being ultimately determined by its
<span class="pagenum" id="Page_84">[Pg 84]</span>
agreement in the whole with known facts or laws. Evidently by
prearrangement and precomputation there could be obtained molecular
arrangements which would establish long-continued regularities, which
would furnish mean results in the aggregate, that would be at variance
with the well-known behavior of Nature. All such cases are here
excluded.
</p>
<p>
According to PLANCK the unregulated, confused and whirring intermingling
of very many atoms (in the case of a monatomic gas) is the prerequisite
for the validity of this hypothesis of "elementary chaos."
</p>

<p><br><br></p>

<p class="center">
(2) <i>Differences in the States of "Elementary Chaos"</i></p>

<p>
When we consider the <i>general</i> state of a gas we need not think
of the state of equilibrium, for this is still further characterized
by the condition that its entropy is a maximum.<a id="FNanchor_24_1"></a><a href="#Footnote_24_1" class="fnanchor">[24]</a>
</p>
<p>
Hence in the general or unsettled state of the gas an unequal
distribution of density may prevail, any number of arbitrarily different
streams (whirls and eddies) may be present, and we may in particular
assume that there has taken place no sort of equalization between the
different velocities of the molecules. To conceive of said differences
we may assume beforehand, in perfectly arbitrary fashion, the velocities
of the molecules as well as their co-ordinates of location. But there
must exist (in order that we may know the state in the macroscopic
sense), certain mean values of density and velocity, for it is through
these very mean values that the state is characterized from the
aggregate (macroscopic) standpoint. The differences that do exist in the
successive stages of disorder of the unsettled state are mainly due to
the molecular collisions that are constantly taking place, thus changing
the velocity and locus of each molecule.
</p>
<p>
Let us for sake of brevity speak of the state of permanence
<span class="pagenum" id="Page_85">[Pg 85]</span>
finally attained by this chaotic mass as the <i>normal</i> state, and all
the preceding chaotic states as <i>abnormal</i> states.
</p>

<div class="footnote">

<p class="nind">
<a id="Footnote_24_1"></a><a href="#FNanchor_24_1"><span class="label">[24]</span></a>The rest of the paragraph is a repetition of what was stated at middle of <a href="#Page_19">p. 19</a>.</p></div>

<p><br><br></p>

<p class="center">
(3) <i>Number of complexions, or probability, of a chaotic state.</i></p>

<p>
It was shown, in an earlier portion of this presentation, that each such
chaotic state (abnormal or normal) is characterized by its number of
complexions, which is determined by the Theory of Probabilities. This
number is a variable one for the successive abnormal states and is a
fixed and a maximum one (under given external conditions) for the normal
state. Now BOLTZMANN (by the application of the Theory of Probabilities
to this chaotic state) has shown that the <i>means</i> of these states
vary in one direction only, in such a way that the probable number of
complexions of the successive abnormal states continually grows till it
attains its maximum in the normal and permanent state.
</p>

</div>

<div class="chapter">
<h2><a id="SECTION_B4"></a>SECTION B
<br><br>
IRREVERSIBILITY</h2>

<p>
This <i>one-sidedness</i> of the average action or flux constitutes and
sharply defines what is meant by irreversibility. It does not
imply that the motion of any particular atom cannot be reversed,
but that the order in which these averages (or the number of
complexions) occur cannot be reversed. We have here a process,
consisting of a number of separately reversible processes, which
proves to be irreversible in the <i>aggregate</i>. This is not the only
possible characterization of the property of irreversibility inherent
in all natural events, but is perhaps as general and exact a one
as can be enunciated. Superficially speaking, from the confused
and irregular motions contemplated, it is quite evident that this
succession of whirls and eddies cannot be worked directly backward
to bring about, in reverse order, the finite physical state
which initiated them; for the effecting of such an opposite change
<span class="pagenum" id="Page_86">[Pg 86]</span>
would demand a co-operation and concert of action on the part of
the elementary constituents which is felt to be quite impossible.
It will not be so general and scientific, but perhaps more easily
apprehended, if we put this result in terms of human effort,
namely, "by asserting that any process is irreversible we assert
that by no means within our present or future power can we
reverse it, i.e., we cannot control the individual molecules."
</p>

</div>

<div class="chapter">
<h2><a id="SECTION_C4"></a>SECTION C
<br><br>
ENTROPY</h2>

<p>
We have seen above that the inevitable growth in the number of
complexions is the mark of irreversibility; the number of complexions at
any stage can also in a certain sense be regarded as the measure, index
or determinant of that stage or state of the system of elements under
consideration. Any function of the number of complexions can be regarded
as such measure, index or determinant. Now it has been shown by
BOLTZMANN that the expression found thermodynamically for the quantity
called entropy differs only by a physically insignificant constant from
the logarithm of said number of complexions. But the latter may properly
be regarded as a true measure of the probability of the system being in
the state considered. BOLTZMANN has defined the entropy of a physical
system as the logarithm of the probability of the mechanical condition
of the system and PLANCK has cast it into the numerical form,
<span class="align-center"><img style="vertical-align: -2.339ex; width: 54.192ex; height: 5.81ex;" src="images/169.svg" alt=" " data-tex="
\begin{aligned}
S &= 1.35\, log_{e} \text{('probability')}\, 10^{-16} + \text{constant}\, K\\
  &= 1.35\, log_{e} \text{(number of complexions)}\, 10^{-16} + \text{const.}\, K;
\end{aligned}
"></span>
where <img style="vertical-align: -0.05ex; width: 1.459ex; height: 1.645ex;" src="images/2.svg" alt=" " data-tex="S"> is the entropy of any natural state of the body and <img style="vertical-align: 0; width: 2.011ex; height: 1.545ex;" src="images/22.svg" alt=" " data-tex="K"> is
an arbitrary constant, the numerical value of the first term of the
second member is the quotient of the energy (expressed in ergs)
<span class="pagenum" id="Page_87">[Pg 87]</span>
divided by the temperature (in centigrade degrees). In English
units and the F.P.S. system this numerical value is <img style="vertical-align: -0.566ex; width: 11.077ex; height: 2.538ex;" src="images/170.svg" alt=" " data-tex="5.50 (10^{-24})">.
</p>
<p>
From the whole development we see that entropy <img style="vertical-align: -0.05ex; width: 1.459ex; height: 1.645ex;" src="images/2.svg" alt=" " data-tex="S"> depends
only on the <i>number</i> of complexions; it should not be considered,
as is sometimes done, as of the same dimensions of energy or
anything that may <i>generally</i> be called a factor of energy.
</p>

</div>

<div class="chapter">
<h2><a id="SECTION_D3"></a>SECTION D
<br><br>
THE SECOND LAW</h2>

<p>
It is evident that all these results have for their original basis the
Theory of Probabilities. Consequently, because these conclusions are
thus based, they must be interpreted according to the general method
underlying this Theory. This method essentially is the determination of
average (mean) values and calling them the probable ones. We therefore
conclude that each state is characterized by the mean number of
complexions belonging to that state, that is, by this mean number which
changes always in a one-sided way, ever in the same sense, inasmuch as
it inevitably and invariably grows till the normal, settled condition is
reached.
</p>
<p>
For the sake of clarity we must keep in mind that the motions of the
individual atoms are reversible and that in this sense the irreversible
processes are reduced to reversible ones. But the process as a whole is
not reversible because, by the very act of complete reversal, we would
suspend the general, chaotic character of the elementary motions and
give them to this extent a special, prearranged feature which would be
more or less hostile to the original definition of "elementary chaos."
The irreversibility is not in the elementary events themselves, but
solely in their irregular arrangement. It is this which guarantees the
one-sided change of the mean value characteristic of each one of the
successive states of the process.
</p>
<p>
Now remembering that the kernel of the Second Law is that
all processes in Nature are irreversible, or, that all changes in
<span class="pagenum" id="Page_88">[Pg 88]</span>
Nature vary in one direction only, we can, in the light of what of
has just preceded, repeat the following precise, scientific statement:
</p>
<p>
"The Second Law, in its objective-physical form (freed from all
anthropomorphism) refers to certain mean values which are found from a
great number of like and 'chaotic' elements."
</p>
<p>
If we now go back to what constitutes the kernel of the Second Law, we
will see the relevance and force of PLANCK'S enunciation of this law:
</p>
<p>
"It is not possible to construct a periodically functioning motor which
effects nothing more than the lifting of a load and the cooling of a
heat reservoir."
</p>
<p>
The proof of this is purely experimental and cumulative, and the same
may be said of the earlier statement of this law, "all changes in Nature
vary in one direction only." The character of this proof is, moreover,
exactly like that for the First Law, the Conservation of Energy, and has
the same sort of validity.
</p>
<p>
When we compared and interpreted the current statements of the Second Law
(pp. <a href="#Page_44">44</a>-<a href="#Page_47">47</a>) we enunciated and
made use of three helpful propositions that will now be repeated:
</p>
<p class="hanging2">
(<i>a</i>) All cases of irreversibility stand or fall together; if any one
can be reversed all can be reversed.
</p>
<p class="hanging2">
(<i>b</i>) Any general consequence of any one correct statement of
the Second Law may be regarded as itself a valid and complete
statement of the Second Law.
</p>
<p class="hanging2">
(<i>c</i>) The <i>summary of all</i> the necessary prerequisites (or
conditions) for determining Entropy may be regarded as a complete and
valid statement of the Second Law.
</p>
<p>
In this connection it will also be helpful to remember PLANCK'S
statement: "In order that a process may be truly reversible it will not
suffice to declare that the mediating body is <i>directly</i>
reversible, but that at the end, everywhere in the whole of Nature, the
same state must be restored which existed at the beginning of said
reversible process."
<span class="pagenum" id="Page_89">[Pg 89]</span>
</p>
<p>
As regards the use of helpful proposition (<i>a</i>):
</p>
<p>
We know that PLANCK'S motor statement of the Second Law was grounded on
the well-known irreversible passage of heat from a cold to a hot body.
But to show the mutual interdependence (<i>a</i>) of one irreversible
change on every other, we will instance in illustration the case of a
frictional event, or the conversion of mechanical work into heat.
</p>
<p>
If this frictional occurrence could by any simple or complex apparatus
be made completely reversible so that everywhere, in the whole of
Nature, the same state would be restored which existed at the beginning
of the frictional occurrence, then such an apparatus would be the motor
contemplated in PLANCK'S statement of the Second Law, for this
periodically running apparatus would convert heat into work without any
other change remaining. A similar line of argument, with a similar
result, could be pursued with every other case of irreversibility that
could be adduced. It is evident that, with the help of the above-given
propositions (<i>a</i>), (<i>b</i>), and (<i>c</i>), the Second Law can
be cast into many other valid forms.
</p>
<p>
We close this presentation of the meaning of the Second Law by the
remark that this law has no <i>independent</i> significance, for its
roots go down deep into the Theory of Probabilities. It is therefore
conceivable that it is applicable to some purely human and animate
events as well as to inanimate, natural events; provided, of course,
that the former possess numerous like and uncontrolled constituents
which may be properly characterized as "elementar-ungeordnet," in other
words, provided the variable elements present constitute adequate
haphazard for the Calculus of Probabilities.
<span class="pagenum" id="Page_90">[Pg 90]</span>
</p>

</div>

<div class="chapter">
<h2><a id="PART_V"></a>PART V
<br><br>
REACH AND SCOPE OF SECOND LAW</h2>

<h2><a id="SECTION_A5"></a>SECTION A
<br><br>
ITS EXTENSION TO ALL BODIES</h2>

<p>
CLAUSIUS extended the operation of the Second Law or, what is the same
thing, the scope of entropy, to all bodies. See RÜHLMANN'S "Handbuch d.
mech. Wärmtheorie," Vol. I, pp. 395-405.
</p>
<p>
BOLTZMANN says in this connection: "As regards entropy, solid and liquid
bodies do not differ <i>qualitatively</i> from perfect gases; the
discussion of the entropy of the former, however, presents greater
mathematical difficulties."
</p>
<p>
Certain features of the entropy of solid and liquid bodies have, however,
been derived with the help of ideal gases as temporary auxiliaries. We
consider this argument the simplest and therefore now give an outline of
PLANCK'S presentation of the matter.<a id="FNanchor_25_1"></a><a href="#Footnote_25_1" class="fnanchor">[25]</a>
</p>

<div class="footnote">

<p class="nind">
<a id="Footnote_25_1"></a><a href="#FNanchor_25_1"><span class="label">[25]</span></a>Thermodynamik, 2d Ed., pp. 87-100.</p></div>

<p><br><br></p>

<p class="center">
PLANCK'S PROOF THAT ALSO FOR ANY OTHER BODIES THAN GASES
THERE REALLY EXISTS A FUNCTION WHICH POSSESSES THE CHARACTERISTICS
OF ENTROPY; THE MAIN STEPS ARE NUMBERED
</p>

<p class="hanging">
(1) Expression of entropy for an ideal gas and properties of entropy.
</p>
<p>
<span class="align-center"><img style="vertical-align: -4.499ex; width: 50.28ex; height: 10.129ex;" src="images/171.svg" alt=" " data-tex="
\begin{aligned}
\begin{alignedat}{2}
(2)\, (a)\, S &= M (C_{v} log T + \frac{R}{m} log v + \text{const.});
\qquad \text{(34)}\\
(b)\quad dS &= \frac{dU + pdV}{T} = \frac{d'Q}{T},
\qquad\qquad\quad\qquad \text{(35)}
\end{alignedat}
\end{aligned}
"></span>
<span class="pagenum" id="Page_91">[Pg 91]</span>
where the elastic forces do a <img style="vertical-align: -0.439ex; width: 11.918ex; height: 2.009ex;" src="images/172.svg" alt=" " data-tex="\text{work} = p dV">; strictly speaking,
<img style="vertical-align: -0.439ex; width: 3.594ex; height: 2.156ex;" src="images/173.svg" alt=" " data-tex="d'Q"> is not differential of <img style="vertical-align: -0.439ex; width: 1.79ex; height: 2.032ex;" src="images/158.svg" alt=" " data-tex="Q"> the heat supply.
</p>
<p>
(3) Two gases (1) and (2) thermally connected, are maintained at same
temperature but different pressure and change adiabatically while
experiencing change of volumes; then it can be shown that for this finite
change, <img style="vertical-align: -0.339ex; width: 18.966ex; height: 1.934ex;" src="images/174.svg" alt=" " data-tex="S_{1} + S_{2} = \text{constant}">, that is for the two gases the
sum of the final
<img style="vertical-align: -0.651ex; width: 61.538ex; height: 2.538ex;" src="images/175.svg" alt=" " data-tex="\text{entropies} = S_{1}^1 + S_{2}^1 = S_{1} + S_{2} = \text{sum of their initial entropies}.">
No other change is effected in any other bodies but in these two gases;
here emphasis is laid on preposition <i>in</i>; for the work done may be
the lifting or lowering of a load and such change of location in rigid
bodies involves no change of <i>inner</i> energy. Changes of density in
external bodies can be also avoided by having the two gas tanks located
in a vacuum.
</p>
<p>
(4) A similar proposition can be established for a system of any number
of gases by successively treating the gases in pairs as above. The
theorem then reads: "If the gas system as a whole possesses the same
entropy in two different states then the system can be brought from one
state to the other in a reversible manner without changes remaining in
other bodies."
</p>
<p>
(5) We know that the expansion of an ideal gas without doing external
work and receiving any heat supply is an irreversible process. The
consequence is that the entropy of this gas increases. It follows at
once that "it is impossible to diminish the entropy of an ideal gas
without changes remaining <i>in</i> other bodies."
</p>
<p>
(6) The same result obtains for a system of any number of ideal gases.
Consequently "there exists in the whole of Nature no means (be they of
the mechanical, thermal, chemical or electrical sort) of diminishing the
entropy of a system of ideal gases, without changes remaining <i>in</i>
other bodies."
</p>
<p>
(7) "If a system of ideal gases has changed to another state
(possibly in an entirely unknown way) without changes remaining
in other bodies, then the final entropy can certainly not be smaller,
it can only be greater than or equal to the initial condition. In
<span class="pagenum" id="Page_92">[Pg 92]</span>
the former case this process is an irreversible one, in the latter
case a reversible one.
</p>
<p>
"Equality of entropy in the two states therefore constitutes a
sufficient and at the same time a necessary condition for the complete
reversibility of the passage from one state to the other, provided no
changes are to remain behind in other bodies."
</p>
<p>
(8) "This proposition has a very considerable range of validity; for
there was expressly no limiting assumption made concerning the way in
which the gas system reached its final condition; the proposition is
therefore valid not only for slowly and simply changing processes but
also for any physical and chemical processes provided at the end no
changes remained <i>in</i> any body outside of the gas system. Nor need
we believe that entropy of a gas has significance only for states of
equilibrium, provided we can suppose the gas mass (moving in any way) to
consist of sufficiently small parts each so homogeneous that it
possesses entropy."<a id="FNanchor_26_1"></a><a href="#Footnote_26_1" class="fnanchor">[26]</a>
</p>
<p>
Then the summation must extend over all these gas parts. "The velocity
has no influence on the entropy, just as little as the height of the
heavy gas parts above a particular horizontal plane."
</p>
<p>
(9) "The laws thus far deduced for ideal gases can in the same way be
transferred to any other bodies, the main difference in general being
that the expression for the entropy of any body cannot be written in
finite magnitudes because the equation of condition is not generally
known. But it can always be shown&mdash;and this is the decisive
point&mdash;that for any other body there really exists a function
possessing the characteristic properties of entropy."
</p>
<p>
Now let us assume any physically "homogeneous body, by
which is meant that the smallest visible space parts of the system
are completely alike. Here it does not matter whether or no
the substance is chemically homogeneous, i.e., whether it consists
<span class="pagenum" id="Page_93">[Pg 93]</span>
of entirely like molecules, and consequently it also does not here
matter whether in the course of the prospective changes of state
it experiences chemical transformation.... When the substance
is stationary the whole energy of the system will consist of the
so-called 'inner' energy <img style="vertical-align: -0.05ex; width: 1.735ex; height: 1.595ex;" src="images/58.svg" alt=" " data-tex="U">, which depends only on the mass and
inner constitution of the substance, which constitution is conditioned
by the temperature and density."
</p>
<p>
(10) Let us suppose that with such a homogeneous body there is conducted
a certain reversible or irreversible cycle process which therefore
brings the body exactly back again to its initial condition. Let the
external influences on the body consist in the performance of work and
in heat supply or withdrawal, which heat exchange is to be effected by
any number of suitable heat reservoirs. At the end of the process no
changes remain in the body itself, only the heat reservoirs have altered
their state. Now let us suppose the heat carriers in the reservoirs to
be composed of purely ideal gases, which may be kept at constant volume
or under constant pressure, at any rate only be subject to reversible
changes of volume. According to the last proved proposition, the sum of
the entropies of all the gases cannot have become smaller, for at the
end of the process no changes remain in any other body, not even in the
body which completed the cycle process.
</p>
<p>
(11) Let <img style="vertical-align: -0.439ex; width: 3.594ex; height: 2.156ex;" src="images/173.svg" alt=" " data-tex="d'Q"> be the heat <i>gained</i> by the body from some reservoir
in an element of time and <img style="vertical-align: 0; width: 1.593ex; height: 1.532ex;" src="images/145.svg" alt=" " data-tex="T"> the temperature of the reservoir<a id="FNanchor_27_1"></a><a href="#Footnote_27_1" class="fnanchor">[27]</a>
at the same moment, then the change of entropy experienced by the reservoir
at this instant will be
<span class="align-center"><img style="vertical-align: -1.552ex; width: 6.978ex; height: 4.799ex;" src="images/176.svg" alt=" " data-tex="
- \frac{d'Q}{T},
"></span>
and in the whole course of time all the reservoirs together will experience
the entropy change
<span class="align-center"><img style="vertical-align: -1.552ex; width: 8.612ex; height: 4.799ex;" src="images/177.svg" alt=" " data-tex="
-\Sigma \frac{d'Q}{T},
"></span>
<span class="pagenum" id="Page_94">[Pg 94]</span>
and then we know that there must be satisfied the condition
<span class="align-center"><img style="vertical-align: -1.552ex; width: 37.593ex; height: 4.799ex;" src="images/178.svg" alt=" " data-tex="
-\Sigma \frac{d'Q}{T} \geqq 0\quad
\text{or}\quad
\Sigma \frac{d'Q}{T} \leqq 0
\qquad \text{(36)}
"></span>
which is the form in which CLAUSIUS first enunciated the Second Law.
</p>
<p>
(12) Another condition for the process considered is furnished by the
First Law. For each element of time <img style="vertical-align: -0.439ex; width: 13.985ex; height: 2.156ex;" src="images/179.svg" alt=" " data-tex="d'Q + A = dU">, where <img style="vertical-align: -0.05ex; width: 1.735ex; height: 1.595ex;" src="images/58.svg" alt=" " data-tex="U"> is the
inner energy of the body and <img style="vertical-align: 0; width: 1.697ex; height: 1.62ex;" src="images/10.svg" alt=" " data-tex="A"> the work expended upon it in an
element of time by external means.
</p>
<p>
Now let us consider a more special case in which the external pressure
at each instant is equal to the pressure p of the supposedly stationary
body. Then the external work will be represented by
<span class="align-center"><img style="vertical-align: -0.566ex; width: 20.082ex; height: 2.262ex;" src="images/180.svg" alt=" " data-tex="
A = -p dV,
\qquad \text{(37)}
"></span>
and then it follows that
<span class="align-center"><img style="vertical-align: -0.566ex; width: 25.896ex; height: 2.396ex;" src="images/181.svg" alt=" " data-tex="
d'Q = dU + pdV.
\qquad \text{(38)}
"></span>
</p>
<p>
(13) Furthermore let the temperature of each heat reservoir, at the
instant when it comes into use, be equal to the simultaneous temperature
of the body; then the cycle process becomes a reversible one and the
inequality of the second law becomes an equality,
<span class="align-center"><img style="vertical-align: -1.552ex; width: 19.925ex; height: 4.799ex;" src="images/182.svg" alt=" " data-tex="
\Sigma \frac{d'Q}{T} = 0,
\qquad \text{(39)}
"></span>
and substitution of above value for <img style="vertical-align: -0.439ex; width: 3.594ex; height: 2.156ex;" src="images/173.svg" alt=" " data-tex="d'Q"> gives
<span class="align-center"><img style="vertical-align: -1.552ex; width: 25.686ex; height: 4.652ex;" src="images/183.svg" alt=" " data-tex="
\Sigma \frac{dU + pdV}{T} = 0.
\qquad \text{(40)}
"></span>
</p>
<p>
In this equation there occur only quantities referring to the
state of the body itself and therefore it can be interpreted without
<span class="pagenum" id="Page_95">[Pg 95]</span>
any reference to the heat reservoirs. It contains the following
proposition:
</p>
<p>
"If a homogeneous body by suitable treatment is allowed to pass through
a series of continuous states of equilibrium and thus finally to come
back to its initial condition, the summation of the differential,
<span class="align-center"><img style="vertical-align: -1.552ex; width: 11.356ex; height: 4.652ex;" src="images/184.svg" alt=" " data-tex="
\frac{dU + pdV}{T},
"></span>
for all the changes of state will be equal to zero. From this follows at
once that if the change of state is not allowed to continue to the
restoration of the initial condition (1), but is stopped at any state
(2), the value of the sum
<span class="align-center"><img style="vertical-align: -2.028ex; width: 23.938ex; height: 5.555ex;" src="images/185.svg" alt=" " data-tex="
\int_{1}^{2} \frac{dU + pdV}{T},
\qquad \text{(41)}
"></span>
will depend solely on the final state (2) and on the initial state
(1), and not on the course of the passage from 1 to 2."<a id="FNanchor_28_1"></a><a href="#Footnote_28_1" class="fnanchor">[28]</a>
</p>
<p>
"The last expression is called by CLAUSIUS the entropy of the body in
state 2, referred to state 1 as the zero state. The entropy of a body in
a particular state is, therefore, like energy, completely determined
down to an additive constant depending on the choice of the zero state."
</p>
<p>
(14) "Let us again designate the entropy by <img style="vertical-align: -0.05ex; width: 1.459ex; height: 1.645ex;" src="images/2.svg" alt=" " data-tex="S">, then
<span class="align-center"><img style="vertical-align: -1.948ex; width: 18.345ex; height: 5.048ex;" src="images/186.svg" alt=" " data-tex="
S = \int \frac{dU + pdV}{T},
"></span>
<span class="pagenum" id="Page_96">[Pg 96]</span>
or, what amounts to the same thing, by
<span class="align-center"><img style="vertical-align: -1.552ex; width: 25.934ex; height: 4.652ex;" src="images/187.svg" alt=" " data-tex="
dS = \frac{dU + pdV}{T},
\qquad \text{(42)}
"></span>
which reduced to the unit of mass becomes
<span class="align-center"><img style="vertical-align: -1.552ex; width: 24.452ex; height: 4.652ex;" src="images/188.svg" alt=" " data-tex="
ds = \frac{du + pdv}{T}.
\qquad \text{(43)}
"></span>
</p>
<p>
"This is evidently identical with the value found for an ideal gas. But
it is equally applicable to every body when its energy <img style="vertical-align: -0.186ex; width: 8.424ex; height: 1.731ex;" src="images/189.svg" alt=" " data-tex="U = Mu"> and
volume <img style="vertical-align: -0.186ex; width: 8.232ex; height: 1.731ex;" src="images/190.svg" alt=" " data-tex="V = Mv"> are known as functions, say, of <img style="vertical-align: -0.439ex; width: 1.138ex; height: 1.439ex;" src="images/191.svg" alt=" " data-tex="p"> and <img style="vertical-align: 0; width: 1.593ex; height: 1.532ex;" src="images/145.svg" alt=" " data-tex="T">, for
the expression for entropy can then be directly determined by
integration. But since these functions are not completely known for any
other substance we must in general rest content with the differential
equation. For the present proof, however, and for many applications of
the Second Law it suffices to know that this differential equation
really contains a unique definition of entropy."
</p>
<p>
As with an ideal gas, we can now always speak of the entropy of any
substance as a certain finite magnitude determined by the values of the
temperature and volume at the instant, and can so speak even when the
substance experiences any reversible or irreversible change. Moreover,
the differential equation (43) is applicable to any change of state,
even an irreversible one.
</p>
<p>
In thus applying the idea of entropy there is no conflict with its
derivation. The entropy of a state is measured by a reversible process
which conducts the body from its present state to the zero state, but
this ideal process has nothing to do with the changes of state that the
body has experienced or is going to experience.
</p>
<p>
"On the other hand, we must emphasize that differential
equation (43) for <img style="vertical-align: -0.023ex; width: 2.238ex; height: 1.593ex;" src="images/192.svg" alt=" " data-tex="ds"> is valid only for changes of temperature and
volume and is not so for changes of mass or of chemical composition.
<span class="pagenum" id="Page_97">[Pg 97]</span>
For changes of the latter sort were never considered in defining entropy."
</p>
<p>
(15) "Finally, we may designate the sum of the entropies of several
bodies as the entropy of the system of all the bodies, provided the
system can be subdivided into infinitesimal elements for which uniform
density and temperature can be assumed; but velocity and force of
gravitation do not at all enter into the expression for entropy."
</p>

<div class="footnote">

<p class="nind">
<a id="Footnote_26_1"></a><a href="#FNanchor_26_1"><span class="label">[26]</span></a>If the motion of the gas is so turbulent that temperature and density cannot
be defined, then we must have recourse to BOLTZMANN'S broader definition of
entropy.</p></div>

<div class="footnote">

<p class="nind">
<a id="Footnote_27_1"></a><a href="#FNanchor_27_1"><span class="label">[27]</span></a>It does not here matter what the temperature of the body is at this instant.</p></div>

<div class="footnote">

<p class="nind">
<a id="Footnote_28_1"></a><a href="#FNanchor_28_1"><span class="label">[28]</span></a>This is evident from the fact that the quantities
<img style="vertical-align: -0.05ex; width: 1.735ex; height: 1.595ex;" src="images/58.svg" alt=" " data-tex="U">, <img style="vertical-align: -0.439ex; width: 1.138ex; height: 1.439ex;" src="images/191.svg" alt=" " data-tex="p">, <img style="vertical-align: -0.05ex; width: 1.74ex; height: 1.595ex;" src="images/8.svg" alt=" " data-tex="V">, and <img style="vertical-align: 0; width: 1.593ex; height: 1.532ex;" src="images/145.svg" alt=" " data-tex="T">, under the integral are each a function
of the state only and do not depend on its past history. This falls far
short of being true for turbulent states, for which it is difficult to
get <img style="vertical-align: -0.439ex; width: 1.138ex; height: 1.439ex;" src="images/191.svg" alt=" " data-tex="p"> and <img style="vertical-align: 0; width: 1.593ex; height: 1.532ex;" src="images/145.svg" alt=" " data-tex="T">. PLANCK does not make the preceding statement, but
gives instead a rigorous proof based on cyclical considerations.</p></div>

</div>

<div class="chapter">
<h2><a id="SECTION_B5"></a>SECTION B
<br><br>
GENERAL CONCLUSION AS TO ENTROPY CHANGES</h2>

<p>
Now that the existence and value of entropy have been established for
every state of any body, we can proceed to draw general conclusions in
much the same way as that followed with the ideal gases. The general
result is:
</p>
<p class="fontsize_80">
"IT IS IN NO WAY POSSIBLE TO DIMINISH THE ENTROPY OF A SYSTEM OF BODIES
WITHOUT HAVING CHANGES REMAIN IN OTHER BODIES."
</p>
<p>
If we admit to the system all the bodies participating in the process,
this theorem becomes:
</p>
<p>
"<i>Every physical and chemical process occurring in Nature takes place
in such a way that there is an increase in the sum of the entropies of
the bodies in any way participating in the process.</i>"
</p>
<p>
We will close with BOLTZMANN'S statement:
</p>
<p>
"<i>The driving motive (or impelling cause) in all natural events
is the difference between the existing entropy and its maximum value.</i>"
</p>
<p><span class="pagenum" id="Page_98">[Pg 98]</span></p>
</div>

<div style='display:block; margin-top:4em'>*** END OF THE PROJECT GUTENBERG EBOOK PHYSICAL SIGNIFICANCE OF ENTROPY OR OF THE SECOND LAW ***</div>
<div style='text-align:left'>

<div style='display:block; margin:1em 0'>
Updated editions will replace the previous one&#8212;the old editions will
be renamed.
</div>

<div style='display:block; margin:1em 0'>
Creating the works from print editions not protected by U.S. copyright
law means that no one owns a United States copyright in these works,
so the Foundation (and you!) can copy and distribute it in the United
States without permission and without paying copyright
royalties. Special rules, set forth in the General Terms of Use part
of this license, apply to copying and distributing Project
Gutenberg&#8482; electronic works to protect the PROJECT GUTENBERG&#8482;
concept and trademark. Project Gutenberg is a registered trademark,
and may not be used if you charge for an eBook, except by following
the terms of the trademark license, including paying royalties for use
of the Project Gutenberg trademark. If you do not charge anything for
copies of this eBook, complying with the trademark license is very
easy. You may use this eBook for nearly any purpose such as creation
of derivative works, reports, performances and research. Project
Gutenberg eBooks may be modified and printed and given away&#8212;you may
do practically ANYTHING in the United States with eBooks not protected
by U.S. copyright law. Redistribution is subject to the trademark
license, especially commercial redistribution.
</div>

<div style='margin-top:1em; font-size:1.1em; text-align:center'>START: FULL LICENSE</div>
<div style='text-align:center;font-size:0.9em'>THE FULL PROJECT GUTENBERG LICENSE</div>
<div style='text-align:center;font-size:0.9em'>PLEASE READ THIS BEFORE YOU DISTRIBUTE OR USE THIS WORK</div>

<div style='display:block; margin:1em 0'>
To protect the Project Gutenberg&#8482; mission of promoting the free
distribution of electronic works, by using or distributing this work
(or any other work associated in any way with the phrase &#8220;Project
Gutenberg&#8221;), you agree to comply with all the terms of the Full
Project Gutenberg&#8482; License available with this file or online at
www.gutenberg.org/license.
</div>

<div style='display:block; font-size:1.1em; margin:1em 0; font-weight:bold'>
Section 1. General Terms of Use and Redistributing Project Gutenberg&#8482; electronic works
</div>

<div style='display:block; margin:1em 0'>
1.A. By reading or using any part of this Project Gutenberg&#8482;
electronic work, you indicate that you have read, understand, agree to
and accept all the terms of this license and intellectual property
(trademark/copyright) agreement. If you do not agree to abide by all
the terms of this agreement, you must cease using and return or
destroy all copies of Project Gutenberg&#8482; electronic works in your
possession. If you paid a fee for obtaining a copy of or access to a
Project Gutenberg&#8482; electronic work and you do not agree to be bound
by the terms of this agreement, you may obtain a refund from the person
or entity to whom you paid the fee as set forth in paragraph 1.E.8.
</div>

<div style='display:block; margin:1em 0'>
1.B. &#8220;Project Gutenberg&#8221; is a registered trademark. It may only be
used on or associated in any way with an electronic work by people who
agree to be bound by the terms of this agreement. There are a few
things that you can do with most Project Gutenberg&#8482; electronic works
even without complying with the full terms of this agreement. See
paragraph 1.C below. There are a lot of things you can do with Project
Gutenberg&#8482; electronic works if you follow the terms of this
agreement and help preserve free future access to Project Gutenberg&#8482;
electronic works. See paragraph 1.E below.
</div>

<div style='display:block; margin:1em 0'>
1.C. The Project Gutenberg Literary Archive Foundation (&#8220;the
Foundation&#8221; or PGLAF), owns a compilation copyright in the collection
of Project Gutenberg&#8482; electronic works. Nearly all the individual
works in the collection are in the public domain in the United
States. If an individual work is unprotected by copyright law in the
United States and you are located in the United States, we do not
claim a right to prevent you from copying, distributing, performing,
displaying or creating derivative works based on the work as long as
all references to Project Gutenberg are removed. Of course, we hope
that you will support the Project Gutenberg&#8482; mission of promoting
free access to electronic works by freely sharing Project Gutenberg&#8482;
works in compliance with the terms of this agreement for keeping the
Project Gutenberg&#8482; name associated with the work. You can easily
comply with the terms of this agreement by keeping this work in the
same format with its attached full Project Gutenberg&#8482; License when
you share it without charge with others.
</div>

<div style='display:block; margin:1em 0'>
1.D. The copyright laws of the place where you are located also govern
what you can do with this work. Copyright laws in most countries are
in a constant state of change. If you are outside the United States,
check the laws of your country in addition to the terms of this
agreement before downloading, copying, displaying, performing,
distributing or creating derivative works based on this work or any
other Project Gutenberg&#8482; work. The Foundation makes no
representations concerning the copyright status of any work in any
country other than the United States.
</div>

<div style='display:block; margin:1em 0'>
1.E. Unless you have removed all references to Project Gutenberg:
</div>

<div style='display:block; margin:1em 0'>
1.E.1. The following sentence, with active links to, or other
immediate access to, the full Project Gutenberg&#8482; License must appear
prominently whenever any copy of a Project Gutenberg&#8482; work (any work
on which the phrase &#8220;Project Gutenberg&#8221; appears, or with which the
phrase &#8220;Project Gutenberg&#8221; is associated) is accessed, displayed,
performed, viewed, copied or distributed:
</div>

<blockquote>
  <div style='display:block; margin:1em 0'>
    This eBook is for the use of anyone anywhere in the United States and most
    other parts of the world at no cost and with almost no restrictions
    whatsoever. You may copy it, give it away or re-use it under the terms
    of the Project Gutenberg License included with this eBook or online
    at <a href="https://www.gutenberg.org">www.gutenberg.org</a>. If you
    are not located in the United States, you will have to check the laws
    of the country where you are located before using this eBook.
  </div>
</blockquote>

<div style='display:block; margin:1em 0'>
1.E.2. If an individual Project Gutenberg&#8482; electronic work is
derived from texts not protected by U.S. copyright law (does not
contain a notice indicating that it is posted with permission of the
copyright holder), the work can be copied and distributed to anyone in
the United States without paying any fees or charges. If you are
redistributing or providing access to a work with the phrase &#8220;Project
Gutenberg&#8221; associated with or appearing on the work, you must comply
either with the requirements of paragraphs 1.E.1 through 1.E.7 or
obtain permission for the use of the work and the Project Gutenberg&#8482;
trademark as set forth in paragraphs 1.E.8 or 1.E.9.
</div>

<div style='display:block; margin:1em 0'>
1.E.3. If an individual Project Gutenberg&#8482; electronic work is posted
with the permission of the copyright holder, your use and distribution
must comply with both paragraphs 1.E.1 through 1.E.7 and any
additional terms imposed by the copyright holder. Additional terms
will be linked to the Project Gutenberg&#8482; License for all works
posted with the permission of the copyright holder found at the
beginning of this work.
</div>

<div style='display:block; margin:1em 0'>
1.E.4. Do not unlink or detach or remove the full Project Gutenberg&#8482;
License terms from this work, or any files containing a part of this
work or any other work associated with Project Gutenberg&#8482;.
</div>

<div style='display:block; margin:1em 0'>
1.E.5. Do not copy, display, perform, distribute or redistribute this
electronic work, or any part of this electronic work, without
prominently displaying the sentence set forth in paragraph 1.E.1 with
active links or immediate access to the full terms of the Project
Gutenberg&#8482; License.
</div>

<div style='display:block; margin:1em 0'>
1.E.6. You may convert to and distribute this work in any binary,
compressed, marked up, nonproprietary or proprietary form, including
any word processing or hypertext form. However, if you provide access
to or distribute copies of a Project Gutenberg&#8482; work in a format
other than &#8220;Plain Vanilla ASCII&#8221; or other format used in the official
version posted on the official Project Gutenberg&#8482; website
(www.gutenberg.org), you must, at no additional cost, fee or expense
to the user, provide a copy, a means of exporting a copy, or a means
of obtaining a copy upon request, of the work in its original &#8220;Plain
Vanilla ASCII&#8221; or other form. Any alternate format must include the
full Project Gutenberg&#8482; License as specified in paragraph 1.E.1.
</div>

<div style='display:block; margin:1em 0'>
1.E.7. Do not charge a fee for access to, viewing, displaying,
performing, copying or distributing any Project Gutenberg&#8482; works
unless you comply with paragraph 1.E.8 or 1.E.9.
</div>

<div style='display:block; margin:1em 0'>
1.E.8. You may charge a reasonable fee for copies of or providing
access to or distributing Project Gutenberg&#8482; electronic works
provided that:
</div>

<div style='margin-left:0.7em;'>
    <div style='text-indent:-0.7em'>
        &#8226; You pay a royalty fee of 20% of the gross profits you derive from
        the use of Project Gutenberg&#8482; works calculated using the method
        you already use to calculate your applicable taxes. The fee is owed
        to the owner of the Project Gutenberg&#8482; trademark, but he has
        agreed to donate royalties under this paragraph to the Project
        Gutenberg Literary Archive Foundation. Royalty payments must be paid
        within 60 days following each date on which you prepare (or are
        legally required to prepare) your periodic tax returns. Royalty
        payments should be clearly marked as such and sent to the Project
        Gutenberg Literary Archive Foundation at the address specified in
        Section 4, &#8220;Information about donations to the Project Gutenberg
        Literary Archive Foundation.&#8221;
    </div>

    <div style='text-indent:-0.7em'>
        &#8226; You provide a full refund of any money paid by a user who notifies
        you in writing (or by e-mail) within 30 days of receipt that s/he
        does not agree to the terms of the full Project Gutenberg&#8482;
        License. You must require such a user to return or destroy all
        copies of the works possessed in a physical medium and discontinue
        all use of and all access to other copies of Project Gutenberg&#8482;
        works.
    </div>

    <div style='text-indent:-0.7em'>
        &#8226; You provide, in accordance with paragraph 1.F.3, a full refund of
        any money paid for a work or a replacement copy, if a defect in the
        electronic work is discovered and reported to you within 90 days of
        receipt of the work.
    </div>

    <div style='text-indent:-0.7em'>
        &#8226; You comply with all other terms of this agreement for free
        distribution of Project Gutenberg&#8482; works.
    </div>
</div>

<div style='display:block; margin:1em 0'>
1.E.9. If you wish to charge a fee or distribute a Project
Gutenberg&#8482; electronic work or group of works on different terms than
are set forth in this agreement, you must obtain permission in writing
from the Project Gutenberg Literary Archive Foundation, the manager of
the Project Gutenberg&#8482; trademark. Contact the Foundation as set
forth in Section 3 below.
</div>

<div style='display:block; margin:1em 0'>
1.F.
</div>

<div style='display:block; margin:1em 0'>
1.F.1. Project Gutenberg volunteers and employees expend considerable
effort to identify, do copyright research on, transcribe and proofread
works not protected by U.S. copyright law in creating the Project
Gutenberg&#8482; collection. Despite these efforts, Project Gutenberg&#8482;
electronic works, and the medium on which they may be stored, may
contain &#8220;Defects,&#8221; such as, but not limited to, incomplete, inaccurate
or corrupt data, transcription errors, a copyright or other
intellectual property infringement, a defective or damaged disk or
other medium, a computer virus, or computer codes that damage or
cannot be read by your equipment.
</div>

<div style='display:block; margin:1em 0'>
1.F.2. LIMITED WARRANTY, DISCLAIMER OF DAMAGES - Except for the &#8220;Right
of Replacement or Refund&#8221; described in paragraph 1.F.3, the Project
Gutenberg Literary Archive Foundation, the owner of the Project
Gutenberg&#8482; trademark, and any other party distributing a Project
Gutenberg&#8482; electronic work under this agreement, disclaim all
liability to you for damages, costs and expenses, including legal
fees. YOU AGREE THAT YOU HAVE NO REMEDIES FOR NEGLIGENCE, STRICT
LIABILITY, BREACH OF WARRANTY OR BREACH OF CONTRACT EXCEPT THOSE
PROVIDED IN PARAGRAPH 1.F.3. YOU AGREE THAT THE FOUNDATION, THE
TRADEMARK OWNER, AND ANY DISTRIBUTOR UNDER THIS AGREEMENT WILL NOT BE
LIABLE TO YOU FOR ACTUAL, DIRECT, INDIRECT, CONSEQUENTIAL, PUNITIVE OR
INCIDENTAL DAMAGES EVEN IF YOU GIVE NOTICE OF THE POSSIBILITY OF SUCH
DAMAGE.
</div>

<div style='display:block; margin:1em 0'>
1.F.3. LIMITED RIGHT OF REPLACEMENT OR REFUND - If you discover a
defect in this electronic work within 90 days of receiving it, you can
receive a refund of the money (if any) you paid for it by sending a
written explanation to the person you received the work from. If you
received the work on a physical medium, you must return the medium
with your written explanation. The person or entity that provided you
with the defective work may elect to provide a replacement copy in
lieu of a refund. If you received the work electronically, the person
or entity providing it to you may choose to give you a second
opportunity to receive the work electronically in lieu of a refund. If
the second copy is also defective, you may demand a refund in writing
without further opportunities to fix the problem.
</div>

<div style='display:block; margin:1em 0'>
1.F.4. Except for the limited right of replacement or refund set forth
in paragraph 1.F.3, this work is provided to you &#8216;AS-IS&#8217;, WITH NO
OTHER WARRANTIES OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT
LIMITED TO WARRANTIES OF MERCHANTABILITY OR FITNESS FOR ANY PURPOSE.
</div>

<div style='display:block; margin:1em 0'>
1.F.5. Some states do not allow disclaimers of certain implied
warranties or the exclusion or limitation of certain types of
damages. If any disclaimer or limitation set forth in this agreement
violates the law of the state applicable to this agreement, the
agreement shall be interpreted to make the maximum disclaimer or
limitation permitted by the applicable state law. The invalidity or
unenforceability of any provision of this agreement shall not void the
remaining provisions.
</div>

<div style='display:block; margin:1em 0'>
1.F.6. INDEMNITY - You agree to indemnify and hold the Foundation, the
trademark owner, any agent or employee of the Foundation, anyone
providing copies of Project Gutenberg&#8482; electronic works in
accordance with this agreement, and any volunteers associated with the
production, promotion and distribution of Project Gutenberg&#8482;
electronic works, harmless from all liability, costs and expenses,
including legal fees, that arise directly or indirectly from any of
the following which you do or cause to occur: (a) distribution of this
or any Project Gutenberg&#8482; work, (b) alteration, modification, or
additions or deletions to any Project Gutenberg&#8482; work, and (c) any
Defect you cause.
</div>

<div style='display:block; font-size:1.1em; margin:1em 0; font-weight:bold'>
Section 2. Information about the Mission of Project Gutenberg&#8482;
</div>

<div style='display:block; margin:1em 0'>
Project Gutenberg&#8482; is synonymous with the free distribution of
electronic works in formats readable by the widest variety of
computers including obsolete, old, middle-aged and new computers. It
exists because of the efforts of hundreds of volunteers and donations
from people in all walks of life.
</div>

<div style='display:block; margin:1em 0'>
Volunteers and financial support to provide volunteers with the
assistance they need are critical to reaching Project Gutenberg&#8482;&#8217;s
goals and ensuring that the Project Gutenberg&#8482; collection will
remain freely available for generations to come. In 2001, the Project
Gutenberg Literary Archive Foundation was created to provide a secure
and permanent future for Project Gutenberg&#8482; and future
generations. To learn more about the Project Gutenberg Literary
Archive Foundation and how your efforts and donations can help, see
Sections 3 and 4 and the Foundation information page at www.gutenberg.org.
</div>

<div style='display:block; font-size:1.1em; margin:1em 0; font-weight:bold'>
Section 3. Information about the Project Gutenberg Literary Archive Foundation
</div>

<div style='display:block; margin:1em 0'>
The Project Gutenberg Literary Archive Foundation is a non-profit
501(c)(3) educational corporation organized under the laws of the
state of Mississippi and granted tax exempt status by the Internal
Revenue Service. The Foundation&#8217;s EIN or federal tax identification
number is 64-6221541. Contributions to the Project Gutenberg Literary
Archive Foundation are tax deductible to the full extent permitted by
U.S. federal laws and your state&#8217;s laws.
</div>

<div style='display:block; margin:1em 0'>
The Foundation&#8217;s business office is located at 809 North 1500 West,
Salt Lake City, UT 84116, (801) 596-1887. Email contact links and up
to date contact information can be found at the Foundation&#8217;s website
and official page at www.gutenberg.org/contact
</div>

<div style='display:block; font-size:1.1em; margin:1em 0; font-weight:bold'>
Section 4. Information about Donations to the Project Gutenberg Literary Archive Foundation
</div>

<div style='display:block; margin:1em 0'>
Project Gutenberg&#8482; depends upon and cannot survive without widespread
public support and donations to carry out its mission of
increasing the number of public domain and licensed works that can be
freely distributed in machine-readable form accessible by the widest
array of equipment including outdated equipment. Many small donations
($1 to $5,000) are particularly important to maintaining tax exempt
status with the IRS.
</div>

<div style='display:block; margin:1em 0'>
The Foundation is committed to complying with the laws regulating
charities and charitable donations in all 50 states of the United
States. Compliance requirements are not uniform and it takes a
considerable effort, much paperwork and many fees to meet and keep up
with these requirements. We do not solicit donations in locations
where we have not received written confirmation of compliance. To SEND
DONATIONS or determine the status of compliance for any particular state
visit <a href="https://www.gutenberg.org/donate/">www.gutenberg.org/donate</a>.
</div>

<div style='display:block; margin:1em 0'>
While we cannot and do not solicit contributions from states where we
have not met the solicitation requirements, we know of no prohibition
against accepting unsolicited donations from donors in such states who
approach us with offers to donate.
</div>

<div style='display:block; margin:1em 0'>
International donations are gratefully accepted, but we cannot make
any statements concerning tax treatment of donations received from
outside the United States. U.S. laws alone swamp our small staff.
</div>

<div style='display:block; margin:1em 0'>
Please check the Project Gutenberg web pages for current donation
methods and addresses. Donations are accepted in a number of other
ways including checks, online payments and credit card donations. To
donate, please visit: www.gutenberg.org/donate.
</div>

<div style='display:block; font-size:1.1em; margin:1em 0; font-weight:bold'>
Section 5. General Information About Project Gutenberg&#8482; electronic works
</div>

<div style='display:block; margin:1em 0'>
Professor Michael S. Hart was the originator of the Project
Gutenberg&#8482; concept of a library of electronic works that could be
freely shared with anyone. For forty years, he produced and
distributed Project Gutenberg&#8482; eBooks with only a loose network of
volunteer support.
</div>

<div style='display:block; margin:1em 0'>
Project Gutenberg&#8482; eBooks are often created from several printed
editions, all of which are confirmed as not protected by copyright in
the U.S. unless a copyright notice is included. Thus, we do not
necessarily keep eBooks in compliance with any particular paper
edition.
</div>

<div style='display:block; margin:1em 0'>
Most people start at our website which has the main PG search
facility: <a href="https://www.gutenberg.org">www.gutenberg.org</a>.
</div>

<div style='display:block; margin:1em 0'>
This website includes information about Project Gutenberg&#8482;,
including how to make donations to the Project Gutenberg Literary
Archive Foundation, how to help produce our new eBooks, and how to
subscribe to our email newsletter to hear about new eBooks.
</div>

</div>
</body>

</html>
